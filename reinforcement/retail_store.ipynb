{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0ee0b4",
   "metadata": {
    "id": "de0ee0b4"
   },
   "source": [
    "*Master IASD, PSL - 2023/2024 - O. Cappé, February 2024*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288e304",
   "metadata": {
    "id": "5288e304"
   },
   "source": [
    "# Retail Store Management\n",
    "\n",
    "**This is your INDIVIDUAL homework that needs to be returned by March 4, 2024, at the latest, as a functional completed Python notebook file. Late submissions will be applied a penalty.** Please print your name here and be sure to name your file <code>YourFirstName-YOURLASTNAME-retail_store.ipynb</code> and to send it by email to <olivier.cappe@ens.fr> before the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1fc9e5",
   "metadata": {},
   "source": [
    "We consider, the retail store management model seen in the first course.\n",
    "\n",
    "You own a bike store. During week $t$, the demand is $D_t$ units, which we may assume to be $\\operatorname{Poisson}(d)$ distributed, independently of the past. On Monday morning you may choose to command $A_t$ additional units that are delivered immediately before the shop opens. For each week,\n",
    "\n",
    "- Maintenance Cost: $h$ per unit left in your stock from previous week\n",
    "- Ordering Cost: $c$ per ordered unit\n",
    "- Sales Profit: $f$ per sold unit\n",
    "\n",
    "With the following constraints\n",
    "\n",
    "- Your warehouse has a maximal capacity of $m$ unit (any additional bike gets stolen)\n",
    "- You cannot sell bikes that you don’t have in stock\n",
    "\n",
    "We will consider that $\\mathcal{A}=\\mathcal{S}=\\{0,\\dots,m\\}$ and the MDP evolves according to\n",
    "\n",
    "- $D_t \\sim \\operatorname{Poisson}(d)$\n",
    "- $X_t = -h S_t -c A_t + f \\min(D_t, S_t + A_t, m)$\n",
    "- $S_{t+1} = \\max(0, \\min(S_t+A_t,m)-D_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb47478",
   "metadata": {
    "id": "cfb47478",
    "ExecuteTime": {
     "end_time": "2024-02-09T07:46:39.774186Z",
     "start_time": "2024-02-09T07:46:36.287574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please REFRAIN from importing any additional module\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d41b76",
   "metadata": {},
   "source": [
    "## The Retail Store Environment\n",
    "\n",
    "The <code>RetailStore</code> class defines the environnement, providing the basic functions for interacting with the system (<code>env.reset</code> and <code>env.step</code>) and for computing basic parameter-dependent quantities (reward and transition functions, value function of a policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198d462a",
   "metadata": {
    "id": "198d462a",
    "ExecuteTime": {
     "end_time": "2024-02-09T07:46:39.787987Z",
     "start_time": "2024-02-09T07:46:39.780265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions for the Retail store environment (do NOT modify this code block)\n",
    "\n",
    "class RetailStore:\n",
    "\n",
    "\n",
    "    def __init__(self, m, h, c, f, d):\n",
    "        self.m = m  # Stock capacity\n",
    "        self.h = h  # Maintenance cost per unit\n",
    "        self.c = c  # Buying price per unit\n",
    "        self.f = f  # Selling price per unit\n",
    "        self.d = d  # Weekly average demand\n",
    "\n",
    "\n",
    "def reset(self, state):\n",
    "    \"\"\" Restarts the environment at time 0 in specified state. \"\"\"\n",
    "    self.state = state\n",
    "    self.time = 0\n",
    "\n",
    "\n",
    "def step(self, action):\n",
    "    \"\"\" Given the action, performs one call to the environment and return the reward. \"\"\"\n",
    "    demand = poisson.rvs(self.d)\n",
    "    reward = -self.h * self.state - self.c * action + self.f * min([demand, self.state + action, self.m])\n",
    "    # Update the time and state variables\n",
    "    self.time += 1\n",
    "    self.state = max([min([self.state + action, self.m]) - demand, 0])\n",
    "    return reward\n",
    "\n",
    "\n",
    "def reward_function(self):\n",
    "    \"\"\" Computes the action-depend reward function r(s,a). \"\"\"\n",
    "    r = np.zeros((self.m + 1, self.m + 1))\n",
    "    for s in range(self.m + 1):\n",
    "        for a in range(self.m + 1):\n",
    "            # Note: computing the expectation of the truncated Poisson distribution using the survival function\n",
    "            r[s, a] = -self.h * s - self.c * a + self.f * sum(\n",
    "                poisson.sf(np.linspace(0, min(s + a, self.m) - 1, num=min(s + a, self.m)), self.d))\n",
    "    return r\n",
    "\n",
    "\n",
    "def transition_function(self):\n",
    "    \"\"\" Computes the action-depend transition probabilities p(s,a,s'). \"\"\"\n",
    "    p = np.zeros((self.m + 1, self.m + 1, self.m + 1))\n",
    "    for s in range(self.m + 1):\n",
    "        for a in range(self.m + 1):\n",
    "            for i in range(min(s + a, self.m)):\n",
    "                p[s, a, min(s + a, self.m) - i] = poisson.pmf(i, self.d)\n",
    "            p[s, a, 0] = poisson.sf(min(s + a, self.m) - 1, self.d)\n",
    "    return p\n",
    "\n",
    "\n",
    "def reward_policy(self, pi):\n",
    "    \"\"\" Computes the reward function r_pi(s) associated with a policy. \"\"\"\n",
    "    r = self.reward_function()\n",
    "    r_pi = np.sum(np.multiply(r, pi), axis=1)\n",
    "    return r_pi\n",
    "\n",
    "\n",
    "def transition_policy(self, pi):\n",
    "    \"\"\" Computes the transition probabilities p_pi(s,s') associated with a policy. \"\"\"\n",
    "    p = self.transition_function()\n",
    "    p_pi = np.zeros((self.m + 1, self.m + 1))\n",
    "    for s in range(self.m + 1):\n",
    "        p_pi[s, :] = np.matmul(np.transpose(pi[s, :]), p[s, :, :])\n",
    "    return p_pi\n",
    "\n",
    "\n",
    "def value_policy(self, pi, gamma):\n",
    "    \"\"\" Computes the value function of a policy, with discount gamma (using matrix inversion). \"\"\"\n",
    "    r_pi = self.reward_policy(pi)\n",
    "    p_pi = self.transition_policy(pi)\n",
    "    v_pi = np.linalg.solve(np.eye(self.m + 1) - gamma * p_pi, r_pi)\n",
    "    return v_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c583e",
   "metadata": {
    "id": "600c583e"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78db98",
   "metadata": {},
   "source": [
    "### Q1. Simulation of Fixed Ordering Policies\n",
    "\n",
    "We will consider a small-size model in which $[m, h, c, f, d, \\gamma] = [9, 0.1, 0.5, 1, 4, 0.875]$ **(Comment these choice of parameters).** By simulating trajectories from the model **(discuss what length is necessary)** get an empirical idea of **how well fixed-ordering (i.e. ordering always the same quantity of goods) perform?** Use the <code>env.value_policy</code> to **plot the value functions of the fixed-ordering policies.** **What is your interpretation of the results? Do you think that there are better policies?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3679a5",
   "metadata": {
    "id": "6b3679a5",
    "ExecuteTime": {
     "end_time": "2024-02-09T07:46:44.166922Z",
     "start_time": "2024-02-09T07:46:44.155292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  3.6  -> 3\n",
      "1 :  3.2  -> 1\n",
      "2 :  2.4  -> 0\n",
      "3 :  1.5  -> 0\n",
      "4 :  -0.5  -> 2\n",
      "5 :  3.3  -> 0\n",
      "6 :  -0.5  -> 2\n",
      "7 :  3.3  -> 0\n",
      "8 :  0.5  -> 1\n",
      "9 :  1.4  -> 1\n",
      "10 :  2.4  -> 0\n",
      "11 :  -1.5  -> 3\n",
      "Discounted reward: 12.6\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the retail store environment with the selected parameters \n",
    "[m, h, c, f, d, gamma] = [9, 0.1, 0.5, 1, 4, 0.875]\n",
    "env = RetailStore(m, h, c, f, d)\n",
    "\n",
    "# Just an example of simulating a short trajectory and computing the cumulated reward  \n",
    "env.reset(m)\n",
    "n = 12\n",
    "w = 1\n",
    "v = 0\n",
    "for _ in range(n):\n",
    "    x = env.step(3)\n",
    "    print(env.time - 1, ': ', '{:.1f}'.format(x), ' ->', env.state)\n",
    "    v += w * x\n",
    "    w *= gamma\n",
    "print('Discounted reward:', '{:.1f}'.format(v))\n",
    "\n",
    "# --- Your answer here ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd79a7",
   "metadata": {},
   "source": [
    "--- _Your answer here_ ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09405ebc",
   "metadata": {},
   "source": [
    "### Q2. Computing the Optimal Policy\n",
    "Obtain the optimal policy by **implementing the Policy Iteration algorithm** (use the <code>env.value_policy</code> to compute the value function). **How do you know that it has converged?** **What does the optimal policy do?** **Comment the form of the value function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d7b2fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T07:47:53.637444Z",
     "start_time": "2024-02-09T07:47:53.123987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[12.66641819 13.06641819 13.46641819 13.86641819 14.26641819 14.66641819\n",
      " 15.05591511 15.37589753 15.65065105 15.89190991]\n"
     ]
    }
   ],
   "source": [
    "# --- Your answer here ---\n",
    "def policy_iteration(env, gamma, max_iter=1000, tol=1e-6):\n",
    "    m = env.m\n",
    "    pi = np.zeros((m + 1, m + 1))\n",
    "    for i in range(m + 1):\n",
    "        pi[i, i] = 1\n",
    "    for _ in range(max_iter):\n",
    "        v = env.value_policy(pi, gamma)\n",
    "        q = env.reward_function() + gamma * np.matmul(env.transition_function(), v)\n",
    "        pi_new = np.zeros((m + 1, m + 1))\n",
    "        for s in range(m + 1):\n",
    "            pi_new[s, np.argmax(q[s, :])] = 1\n",
    "        if np.linalg.norm(pi_new - pi) < tol:\n",
    "            break\n",
    "        pi = pi_new\n",
    "    return pi, v\n",
    "\n",
    "\n",
    "pi, v = policy_iteration(env, gamma)\n",
    "print(pi)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302b237",
   "metadata": {},
   "source": [
    "-- _Your answer_ here ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f6b12",
   "metadata": {},
   "source": [
    "### Q3. Q Learning\n",
    "**Implement the asynchronous Q-Learning algorithm using the purely random policy (all actions selected uniformly) as the exploration policy** (note: this requires only about 10 lines of code). **Plot the convergence of the algorithm** both in terms of convergence of the state-action value table $Q_t$ and of convergence of the value function of the associated greedy policy $\\pi_{t+}(s) = \\arg\\max_a Q_t(s,a)$. Use a single trajectory of length $n = 10,000$ and **test different schemes of decrease** of the learning rate (following the course guidelines). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "360d9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e5725",
   "metadata": {},
   "source": [
    "### Q4. Policy Gradient\n",
    "\n",
    "We will now consider using policy gradient from a set of simulated trajectories using the REINFORCE formula to approximate the gradient of the value function. To do so, we consider a log-linear parameterization of the policy and provide the two utility functions below.\n",
    "\n",
    "<code>policy_features</code> Computes a $2(m+1)$--dimensional feature vector $\\phi(s,a)$ corresponding to the state-action pair $(s,a)$.\n",
    "\n",
    "<code>policy_choice</code> Computes the vector $\\pi_\\theta(s,:)$ of action probabilities using the softmax operator:\n",
    "$$\n",
    "    \\pi_\\theta(s,a) = \\frac{\\exp\\left(\\theta^T \\phi(s,a)\\right)}{\\sum_{a'=0}^m \\exp\\left(\\theta^T \\phi(s,a')\\right)}\n",
    "$$\n",
    "You should check from the code that the matrix $(\\theta^T \\phi(s,a))_{0\\leq s,a \\leq n}$ is a weighted sum of the $m+1$ fixed ordering policies and of the $m+1$ threshold policies (which you should have met already...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a5c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy gradient utility functions (do NOT modify this code block)\n",
    "\n",
    "def policy_features(s, a, m):\n",
    "    \"\"\"Returns the feature vector corresponding to state (s,a).\"\"\"\n",
    "    f = np.zeros(2 * (m + 1))\n",
    "    # Index of the fixed ordering policy that is non zero in (s,a)\n",
    "    f[a] = 1\n",
    "    # Indices of the threshold policies that are non zero in (s,a)\n",
    "    if (s + a <= m):\n",
    "        f[(m + 1) + s + a] = 1\n",
    "    if (a == 0):\n",
    "        for i in range(0, s + 1):\n",
    "            f[(m + 1) + i] = 1\n",
    "    return f\n",
    "\n",
    "\n",
    "def policy_choice(s, m, theta):\n",
    "    \"\"\"Returns the vector of action probabilities pi(s,:) corresponding to state s and parameter theta.\"\"\"\n",
    "    pi = np.zeros(m + 1)\n",
    "    # Note : Computation in log to avoid numerical underflows\n",
    "    for a in range(m + 1):\n",
    "        pi[a] = np.dot(policy_features(s, a, m), theta)\n",
    "    pi = pi - np.max(pi)\n",
    "    pi = np.exp(pi) / np.sum(np.exp(pi))\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f30082",
   "metadata": {},
   "source": [
    "**Code a function** <code>policy_gradient</code> that computes the gradient $\\nabla_\\theta \\log\\pi_\\theta(s,a)$ of $\\pi_\\theta(s,a)$. First **write in the text block below the LaTeX formula implemented by the function, explaining how you obtain this formula.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244fbf3e",
   "metadata": {},
   "source": [
    "--- _Your answer here_ ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e5fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your answer here ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e372454",
   "metadata": {},
   "source": [
    "**Implement the policy gradient algorithm** approximating the gradients using the REINFORCE formula\n",
    "$$\n",
    "  \\sum_{t=0}^{n} \\gamma^t \\left(\\sum_{i=0}^{n} \\gamma^i X_{t+i} \\right) \\nabla_\\theta \\log \\pi_\\theta(S_t, A_t)\n",
    "$$\n",
    "computed on trajectories of length 35 started from a random initial state and using 200 iterations of SGD updates. To do so, complete the code template provided below. **Monitor the convergence of the algorithm by plotting the difference between the mean of the optimal value function and the mean of the value functions corresponding to successive values of $\\theta$ (explain why one considers the mean).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61c2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Edit this block ---\n",
    "n = 35\n",
    "nb_iter = 200\n",
    "\n",
    "theta = np.zeros(2 * (m + 1))\n",
    "for i_iter in range(nb_iter):\n",
    "    # Compute the REINFORCE approximation of the gradient of the value function from a run of length n\n",
    "    # of the MDP initialized t a random state and using the policy corresponding to the current value of theta\n",
    "\n",
    "    grad = np.zeros(2 * (m + 1))  # --- Something better than this is needed here! ---\n",
    "\n",
    "    # Do a step of SGD update on theta\n",
    "    theta += 0.1 * np.power(1 + i_iter, -0.6) * grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
