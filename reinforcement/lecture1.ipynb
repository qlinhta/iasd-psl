{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "734ad4637e5bfb56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4799add64d3ce5a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reinforcement Learning (RL) Objective\n",
    "\n",
    "Compared to other learning paradigms, RL is specific in that:\n",
    "- Observations are not available prior to learning but collected sequentially through successive actions (or decisions)\n",
    "- Observations come with associated rewards (or costs) that quantify the relevance of the sequence of actions\n",
    "- The outcome of an action typically depends on the history of prior actions and observations.\n",
    "\n",
    "Episodic vs. continuing tasks:\n",
    "- Episodic tasks terminate after a finite number of steps, e.g. a game of chess or a maze\n",
    "- Continuing tasks continue forever and the agent-environment interaction is ongoing, e.g. a robot navigating in a room"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "776386822935b844"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "MDP ($\\mathcal{S_t}$ is the state at time $t$, $\\mathcal{A_t}$ is the action at time $t$, $\\mathcal{R_t}$ is the reward at time $t$):\n",
    "- Policy: $\\pi(a|s) = \\mathbb{P}[\\mathcal{A_t} = a | \\mathcal{S_t} = s]$ is the probability of taking action $a$ in state $s$\n",
    "- State transition probability: $\\mathcal{P}_{ss'}^a = \\mathbb{P}[\\mathcal{S_{t+1}} = s' | \\mathcal{S_t} = s, \\mathcal{A_t} = a]$ is the probability of transitioning to state $s'$ from state $s$ after taking action $a$\n",
    "- Reward function: $\\mathcal{R}_s^a = \\mathbb{E}[\\mathcal{R_{t+1}} | \\mathcal{S_t} = s, \\mathcal{A_t} = a]$ is the expected reward after taking action $a$ in state $s$\n",
    "- Value function: $v_\\pi(s) = \\mathbb{E}_\\pi[\\mathcal{R_{t+1}} + \\gamma \\mathcal{R_{t+2}} + \\gamma^2 \\mathcal{R_{t+3}} + \\dots | \\mathcal{S_t} = s]$ is the expected return starting from state $s$ and following policy $\\pi$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1568ac50dc0a312"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Recall**:\n",
    "- $\\mathbb{P}[X=x|Y] = \\frac{\\mathbb{P}[X=x, Y]}{\\mathbb{P}[Y]}$ (Bayes' rule), probability of $X$ given $Y$ is equal to the probability of $X$ and $Y$ divided by the probability of $Y$\n",
    "- $\\mathbb{E}_\\pi[X|Y] = \\sum_{x \\in X} x \\mathbb{P}[X=x|Y] = \\phi (Y)$ (conditional expectation), expected value of $X$ given $Y$ is equal to a function of $Y$\n",
    "- $\\mathbb{E}[X|Y] \\stackrel{\\text{def}}{=} \\phi (Y)$ (conditional expectation), expected value of $X$ given $Y$ is equal to a function of $Y$\n",
    "- $\\mathbb{E}[\\mathbb{E}[X|Y]] = \\mathbb{E}[X]$ (Tower property), expected value of the expected value of $Y$ given $X$ is equal to the expected value of $Y$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}[\\mathbb{E}[X|Y]] &= \\sum_{y \\in Y} \\phi(y) \\mathbb{P}[Y=y] \\\\\n",
    "&= \\sum_{x, y} x \\mathbb{P}[X=x, Y=y] \\mathbb{P}[Y=y] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- \\mathbb{1}_A(x) = \\begin{cases} 1 & \\text{ if } x \\in A \\\\ 0 & \\text{ otherwise} \\end{cases} (indicator function), indicator function of $A$ is equal to 1 if $x$ is in $A$ and 0 otherwise\n",
    "- $\\mathbb{E}[\\mathbb{1}_A(X)] = \\mathbb{P}[X \\in A]$ (law of the unconscious statistician), expected value of the indicator function of $A$ is equal to the probability of $X$ being in $A$\n",
    "- $\\mathbb{P}(S_{t+1} = s' | S_t = s) = \\mathbb{E}[\\mathbb{1}_{S_{t+1} = s'} | S_t = s] = \\mathbb{p}(s, s')$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39acc5fa77456b55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Markov Chain**: a Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.\n",
    "$$\n",
    "\\mathbb{P}[\\mathcal{S_{t+1}} = s_{t+1} | \\mathcal{S_t} = s_t, \\mathcal{S_{t-1}} = s_{t-1}, \\dots, \\mathcal{S_0} = s_0] = \\mathbb{P}[\\mathcal{S_{t+1}} = s_{t+1} | \\mathcal{S_t} = s_t] = \\mathcal{p}(s_t, s_{t+1})\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bac9ce5a54e1c9fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "$p(s, s') = \\mathbb{P}[\\mathcal{S_{t+1}} = s' | \\mathcal{S_t} = s]$ so $\\sum_{s' \\in \\mathcal{S}} p(s, s') = 1$.\n",
    "\n",
    "**Transition matrix**:\n",
    "When the state space $\\mathcal{S}$ is finite, say $\\mathcal{S} = \\{s_1, s_2, \\dots, s_n\\}$, the transition matrix $\\mathcal{P}$ is defined as\n",
    "$$\n",
    "\\mathcal{P} = \\begin{bmatrix} p(s_1, s_1) & p(s_1, s_2) & \\dots & p(s_1, s_n) \\\\ p(s_2, s_1) & p(s_2, s_2) & \\dots & p(s_2, s_n) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ p(s_n, s_1) & p(s_n, s_2) & \\dots & p(s_n, s_n) \\end{bmatrix}\n",
    "$$\n",
    "and $\\mathcal{P}_{ij} = p(s_i, s_j)$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24ca9e45251bf201"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Markov Reward Process**: a Markov reward process is a Markov chain with values.\n",
    "A join process $(\\mathcal{S_t}, \\mathcal{X_t})_{t \\geq 0}$ where $\\mathcal{S_t} \\in \\mathcal{S}$ is the state at time $t$ and $\\mathcal{X_t} \\in \\mathbb{R}$ is the reward at time $t$ such that, given the history $\\mathcal{H_t} = (\\mathcal{S_0}, \\mathcal{X_0}, \\dots, \\mathcal{S_t}, \\mathcal{X_t})$,\n",
    "- $X_t$ and $S_t$ are random variables\n",
    "- $X_t$ depends on $S_t$ and $S_{t+1}$ but not on earlier states or rewards"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73b8346609f6f262"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Value function**:\n",
    "For a sequence $w = (w_t)_{t \\geq 0}$ of summable weights, the value function is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "v_w(s) &= \\mathbb{E}\\left[\\sum_{t = 0}^ \\infty w_t \\mathcal{X_t} | \\mathcal{S_0} = s\\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "where $w_t$ is the weight at time $t$ and $\\mathcal{X_t}$ is the reward at time $t$.\n",
    "The value function is the expected discounted sum of rewards starting from state $s$.\n",
    "- For any $t$, $v_w(s_t) = \\mathbb{E}\\left[\\sum_{k = t}^ \\infty w_k \\mathcal{X_k} | \\mathcal{S_t} = s_t\\right]$\n",
    "- $v_w(s) = \\left[\\sum_{i= 0}^ \\infty w_i \\mathcal{P}^i r)_{s}\\right]$ where $r(s) = \\begin{bmatrix} \\mathcal{r}_s^1 \\\\ \\mathcal{r}_s^2 \\\\ \\vdots \\\\ \\mathcal{r}_s^n \\end{bmatrix}$ and $\\mathcal{r}_s^i = \\mathbb{E}[\\mathcal{X_t} | \\mathcal{S_t} = s, \\mathcal{A_t} = i]$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f5f5df3a1a44e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bellman equation**:\n",
    "Particular cases of interest are:\n",
    "- Finite horizon: $w_0 = w_1 = \\dots = w_{n} = 1$ and $w_i = 0$ when $i \\geq n$\n",
    "- Discounted reward: $w_i = \\gamma^i$ for some $\\gamma \\in (0, 1)$, for which, one has the following Bellman equation: $ v_\\gamma(s) = \\mathcal{r}_s + \\gamma \\mathcal{P} v_\\gamma(s)$ and one may also determine $v_\\gamma$ as: $v_\\gamma = (I - \\gamma \\mathcal{P})^{-1} \\mathcal{r}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a16367078830c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dee43ef8d00c6e73"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
