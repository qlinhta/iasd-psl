{"cells":[{"cell_type":"markdown","source":["# <font color='#3b4859'><b>Les fondamentaux de PyTorch</b></font>\n","\n","Bienvenue dans le suite du cours les fondammentaux au Deep Learning. Dans les Notebooks pr√©c√©dents vous avez utilis√© NumPy pour coder vos premiers r√©seaux de neurones mais en pratique, vous utiliserez des frameworks d√©di√©es au Deep Learning.\n","\n","Dans ce Notebook vous allez d√©couvrir comment utiliser PyTorch pour impl√©menter vos algorithmes."],"metadata":{"id":"iDoU8olXi3OW"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ZGtcZbNpMMxR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697280208040,"user_tz":-120,"elapsed":4112,"user":{"displayName":"Hippolyte Mayard","userId":"12682917716835997811"}},"outputId":"0b29ca9f-7577-4476-eda8-b102dd0d9aa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Version de pytorch :  2.0.1+cu118\n"]}],"source":["import time\n","\n","import numpy as np\n","import torch\n","print(\"Version de pytorch : \", torch.__version__)"]},{"cell_type":"code","source":["torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOrdIsVayG1I","executionInfo":{"status":"ok","timestamp":1697280264142,"user_tz":-120,"elapsed":210,"user":{"displayName":"Hippolyte Mayard","userId":"12682917716835997811"}},"outputId":"5693b2b5-c70c-4011-fdb6-5f0989e1864b"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpZ4yZgt_BxL"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"Qj6ZQ96sbccS"},"source":["# <font color='#3b4859'>0. PyTorch vs. NumPy</font>\n","\n","Dans les Notebooks pr√©c√©dents vous avez vu que NumPy permettait d'effectuer des calculs d'alg√®bre lin√©aire.\n","\n","PyTorch va permettre d'acc√©l√©rer ces calculs en utilisant les **tensors** et surtout, les **GPU**."]},{"cell_type":"markdown","metadata":{"id":"48PbKHR4jpI2"},"source":["### <font color='#3b4859'>Param√©trer l'acc√®s du notebook aux GPU</font>\n","\n","Pour ce faire vous devez effectuer la proc√©dure suivante, allez dans : **Modifier -> Param√®tres du Notebook -> Acc√©l√©rateur Mat√©riel**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBmXefMsMNjl"},"outputs":[],"source":["# Effectuons un test rapide pour voir la diff√©rence des temps de calculs\n","# Cr√©ation de deux matrices A et B de dim(5000, 5000)\n","d = 5000\n","\n","A = np.random.rand(d, d).astype(np.float32)\n","B = np.random.rand(d, d).astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pZlGXMXMQhI"},"outputs":[],"source":["# Puis on effectue le produit\n","s = time.time()\n","C = A.dot(B)\n","print(time.time() - s)"]},{"cell_type":"markdown","metadata":{"id":"bQ9prgnIba6N"},"source":["Pour allouer un Tenseur √† un GPU, on utilise la fonction `.cuda()` ou alors `to.(\"cuda\")`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgHiiV7AMSgP"},"outputs":[],"source":["# La m√™me chose avec PyTorch\n","d = 5000\n","\n","A = torch.rand(d, d).cuda()\n","B = torch.rand(d, d).cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uo-pt5xHMe_p"},"outputs":[],"source":["s = time.time()\n","C = torch.mm(A,B)\n","print(time.time() - s)"]},{"cell_type":"markdown","metadata":{"id":"Vc5xoYYQwRJm"},"source":["Vous pouvez constater ici que l'utilisation d'un tensur sur GPU permet d'acc√©l√©rer les calculs."]},{"cell_type":"markdown","metadata":{"id":"w44C7sOM6L0c"},"source":["# <font color='#3b4859'>1. Les tenseurs et leurs op√©rations classiques</font>\n","\n","Les Tenseurs sont des objets similaires √† des NumPy ndarrays. Cependant, les Tenseurs peuvent √™tre utilis√©s sur GPU afin d'acc√©lerer les calculs.\n"]},{"cell_type":"markdown","source":["## <font color='#3b4859'>1.1. Cr√©ation de tenseurs √† partir de donn√©es</font>\n","Les Tenseurs peuvent √™tre cr√©√©s **√† partir d'une liste** en utilisant la fonction [```torch.tensor```](https://pytorch.org/docs/stable/generated/torch.tensor.html)."],"metadata":{"id":"FE9oZL7_jbkU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4WjnSggNiFI"},"outputs":[],"source":["# Exemple en dimension 1\n","data = [1.0, 2.0, 3.0]\n","tensor = torch.tensor(data)\n","print(\"Exemple en dimension 1\")\n","print(tensor)\n","\n","# Example en dimension 2\n","data = [[1., 2., 3.], [4., 5., 6.]]\n","tensor = torch.tensor(data)\n","print(\"\\nExemple en dimension 2\")\n","print(tensor)\n","\n","# Exemple en dimension 3\n","data = [[[1.,2.], [3.,4.]],\n","        [[5.,6.], [7.,8.]]]\n","tensor = torch.tensor(data)\n","print(\"\\nExemple en dimension 3\")\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"RqNDaxwIchKY"},"source":["## <font color='#3b4859'>1.2. Initialiser d'un tenseur vide </font>\n","\n","Il est possible de d'initialiser un tenseur vide en utilisant la fonction [```torch.empty```](https://pytorch.org/docs/stable/generated/torch.empty.html).\n","\n","Une matrice \"vide\", c'est √† dire non initialis√©e, est d√©clar√©e. Elle ne contient pas de valeurs d√©finies et connues avant d'√™tre utilis√©e.\n","Lorsqu'une matrice non initialis√©e est cr√©√©e, les valeurs qui se trouvaient dans la m√©moire allou√©e √† ce moment-l√† apparaissent comme les valeurs initiales.\n","\n","Vous trouverez un exemple d'utilisation de la fonction ```torch.empty``` en ex√©cutant la cellule suivante."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8GOnu9o6tmb"},"outputs":[],"source":["# Initialisation d'une matrice non initialis√©e de taille 2x3\n","x = torch.empty(2, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"GQ0JG7QudIUa"},"source":["## <font color='#3b4859'>1.3. Initialiser un tenseur de fa√ßon al√©atoire </font>\n","\n","On peut initialiser un tenseur de fa√ßon al√©atoire, de diff√©rentes fa√ßons.\n","\n","- La fonction [```torch.rand```](https://pytorch.org/docs/stable/generated/torch.rand.html) retourne un tenseur dont les valeurs sont tir√©es al√©atoirement en suivant une loi uniforme sur $[0,1[$.\n","- La fonction [```torch.randn```](https://pytorch.org/docs/stable/generated/torch.randn.html) retourne un tenseur dont les valeurs sont tir√©es al√©atoirement en suivant une loi normale de moyenne 0 et d'√©cart type 1.\n","\n","Vous trouverez des exemples d'utilisation des fonctions ```torch.rand``` et ```torch.randn``` en ex√©cutant la cellule suivante."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ2GvIugc7eq"},"outputs":[],"source":["# Initialisation d'une matrice al√©atoire de taille 2x3\n","# les valeurs suivent une loi uniforme sur [0,1[\n","x = torch.rand(2, 3)\n","print(x)\n","\n","# Initialisation d'une matrice al√©atoire de taille 2x3\n","# les valeurs suivent une loi normale de moyenne 0 et d'√©cart type 1\n","x = torch.randn(2, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"oiUnw5uxdTEN"},"source":["## <font color='#3b4859'>1.4. Initialiser un tenseur contenant uniquement des 0 ou des 1 </font>\n","\n","- La fonction [```torch.zeros```](https://pytorch.org/docs/stable/generated/torch.zeros.html) retourne un tenseur compos√© uniquement de 0\n","- La fonction [```torch.ones```](https://pytorch.org/docs/stable/generated/torch.ones.html) retourne un tenseur compos√© uniquement de 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M543AqqpdOQy"},"outputs":[],"source":["# Initialisation d'une matrice al√©atoire de taille 2x3\n","# Remplie de valeurs 0\n","x = torch.zeros(2, 3)\n","print(\"Matrice de zeros\")\n","print(x)\n","\n","# Initialisation d'une matrice al√©atoire de taille 2x3\n","# Remplie de valeurs 1\n","x = torch.ones(2, 3)\n","print(\"Matrice de uns\")\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"s0HQKRy2wsDd"},"source":["## <font color='#3b4859'>1.5. Cr√©er un tenseur √† partir d'un tenseur existant </font>\n","\n","Il est possible d'initialiser un tenseur √† partir d'un tenseur existant."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"visdOehpxqd-"},"outputs":[],"source":["x = x.new_ones(2, 3, dtype=torch.double)      # new_* methods take in sizes\n","print(x)\n","\n","x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n","print(x)                                      # result has the same size"]},{"cell_type":"markdown","metadata":{"id":"ehiXz-UuwRJp"},"source":["## <font color='#3b4859'>1.6. Il est important de typer les tenseurs </font>\n","\n","Lorsque vous d√©finissez un tenseur, il est important de lui donner un type, de fa√ßon similaire aux objets array de Numpy.\n","\n","Les types de tenseurs sont les types usuels :\n"," - ```torch.int32``` (√©quivalent √† ```torch.long```)\n"," - ```torch.int64```\n"," - ```torch.float32``` (√©quivalent √† ```torch.float```)\n"," - ```torch.float64```\n"," - ```torch.bool```\n"]},{"cell_type":"markdown","metadata":{"id":"LxHQ39iRwRJp"},"source":["Il existe plusieurs fa√ßon de pr√©ciser le type d'un tenseur :\n","- lors de son initialisation en pr√©cisant en param√®tre  ```dtype = TYPE```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzjM0teMwRJp"},"outputs":[],"source":["data = [[1, 2, 3], [4, 5, 6]]\n","\n","# Initialisation d'un tenseur d'int\n","int_tensor = torch.tensor(data, dtype = torch.int32)\n","print(int_tensor)\n","print(int_tensor.dtype)\n","\n","# Initialisation d'un tenseur de float\n","float_tensor = torch.tensor(data, dtype = torch.float32)\n","print(float_tensor)\n","print(float_tensor.dtype)"]},{"cell_type":"markdown","metadata":{"id":"kk1tbHqPwRJq"},"source":["- en appelant le m√©thode ```.to(TYPE)```ou bien ```.float()``` (fonctionne √©galement avec la m√©thod ```.int()``` ou ```.bool()```)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKLqE358wRJq"},"outputs":[],"source":["int_tensor = torch.tensor(data)\n","\n","# en utilisant .to(TYPE)\n","float_tensor = int_tensor.to(torch.float32)\n","print(float_tensor)\n","print(float_tensor.dtype)\n","\n","# en utilisant .float()\n","float_tensor = int_tensor.float()\n","print(float_tensor)\n","print(float_tensor.dtype)"]},{"cell_type":"markdown","metadata":{"id":"HLZfCcObzBTe"},"source":["## <font color='#3b4859'>1.7. Size et Shape d'un Tenseur </font>\n","\n","Pour conna√Ætre les dimensions d'un tenseur, on peut utiliser les m√©thodes suivantes :\n","- ```.shape```\n","- ```.size()```\n","\n","On obtient un objet `torch.Size`. Il s'agit d'un tuple. Par cons√©quent, il supporte toutes les op√©rations valables sur les tuples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVIu-uZnzEm-"},"outputs":[],"source":["# Example with 1-D data\n","data = [1.0, 2.0, 3.0]\n","tensor = torch.Tensor(data)\n","print(\"Exemple en 1-D\")\n","print(tensor)\n","print(tensor.size())\n","print(tensor.shape)\n","\n","# Example with 2-D data\n","data = [[1., 2., 3.], [4., 5., 6]]\n","tensor = torch.Tensor(data)\n","print(\"\\nExemple en 2-D\")\n","print(tensor)\n","print(tensor.size())\n","print(tensor.shape)\n","\n","# Example with 3-D data\n","data = [[[1.,2.], [3.,4.]],\n","        [[5.,6.], [7.,8.]]]\n","tensor = torch.Tensor(data)\n","print(\"\\nExemple en 3-D \")\n","print(tensor)\n","print(tensor.size())\n","print(tensor.shape)"]},{"cell_type":"markdown","metadata":{"id":"cYai6C6H0kKx"},"source":["## <font color='#3b4859'>1.8. Op√©rations avec les Tenseurs </font>\n","La plupart des op√©rations avec les Tenseurs sont similaires √† celles impl√©ment√©es dans NumPy pour les arrays :\n","\n","- addition\n","- soustraction\n","- multiplication (division) par un scalaire\n","- produit scalaire\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRj42hPO0prW"},"outputs":[],"source":["x = torch.Tensor([ 1., 2., 3. ])\n","y = torch.Tensor([ 4., 5., 6. ])\n","\n","# Op√©rations arithm√©tiques\n","# Addition\n","z1 = x + y\n","print(z1)\n","\n","# Addition\n","z2 = x - y\n","print(z2)\n","\n","# Multiplication (division) par un scalaire\n","z3 = x / 3\n","print(z3)\n","\n","# fonction dot : produit scalaire\n","print(torch.dot(x,y))\n","print(x @ y )"]},{"cell_type":"markdown","metadata":{"id":"W9Y79GmuwRJr"},"source":["## <font color='#3b4859'>1.9. Op√©rations in-place</font>\n","\n","Les op√©rations sur les tenseurs avec un underscore `_` sont appel√©es \"in place\" op√©rations. Elles s'appliquent directement √† la suite du tenseur (exemples: `x.copy_(y)`, `x.t_()`) et changent la valeurs de `x`.\n","\n","Voir la documentation officielle ([PyTorch official documentation](http://pytorch.org/docs/torch.html)) pour voir la liste, non exhaustive, des op√©rations propos√©es par Pytorch.\n","\n","Vous trouverez un exemple d'utilisation de fonction in-place dans la cellule suivante :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjy0Zqik0-wL"},"outputs":[],"source":["# In-place addition\n","\n","x = torch.Tensor([ 1., 2., 3. ])\n","y = torch.Tensor([ 4., 5., 6. ])\n","\n","y.add_(x)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"_IqeYxdg3Bvk"},"source":["## <font color='#3b4859'>1.10. Indexation et reshaping des Tenseurs</font>\n","\n","L'indexation des tenseurs est similaire √† celle d'un Numpy Array.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9REyiT-wRJr"},"outputs":[],"source":["x = torch.Tensor([[1., 2., 3.], [4., 5., 6]])\n","print(x[:, 1]) # selectionne la colonne num√©ro 1\n","print(x[-1, :]) # selectionne la derni√®re ligne"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUwsXsfw26g_"},"outputs":[],"source":["x = torch.randn(4, 4)\n","y = x.view(16)\n","z = x.view(-1, 8)  # l'index -1 permet d'inf√©rer √† partir des autres dimensions d√©finies\n","#ici comme l'autre dimension est 8, la premi√®re dimension sera √©gale √† 16/2=8)\n","print(x.size(), y.size(), z.size())"]},{"cell_type":"markdown","metadata":{"id":"f33ljXvsh5IB"},"source":["Pour s√©lectionner les Tenseurs contenant un objet en 1-D, `.item()` permet de r√©cup√©rer l'objet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clJzuzI93U7k"},"outputs":[],"source":["x = torch.randn(1)\n","print(x)\n","print(x.item())"]},{"cell_type":"markdown","metadata":{"id":"zRbkdZcArlk7"},"source":["## <font color='#3b4859'>1.11. Convertir un Tenseur depuis Numpy ou en Numpy array</font>\n","La conversion d'un Tenseur en NumPy array et vice versa est tr√®s simple.\n","\n","Le Torch Tenseur et la Numpy Array vont **partager le m√™me espace en m√©moire** (si le Tenseur est sur CPU) et tout changement de l'un entra√Æne un changement de l'autre.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlLtucZM3g-N"},"outputs":[],"source":["a = torch.ones(5)\n","print(\"Original a:\", a)\n","\n","b = a.numpy()\n","print(\"Original b:\", b)\n","\n","a.add_(1)\n","print(\"New a:\", a)\n","print(\"New b:\", b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cij37AWq31XG"},"outputs":[],"source":["a = np.ones(5)\n","b = torch.from_numpy(a)\n","np.add(a, 1, out=a)\n","print(a)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"npIpV4SpjQuG"},"source":["## <font color='#3b4859'>1.12. Les Tenseurs CUDA </font>\n","\n","Les Tenseurs peuvent chang√©s facilement de device (CPU ou GPU) en utilisant `.to()`.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvBpFQTulo95"},"outputs":[],"source":["# Essayez d'ex√©cuter cette cellule avec et sans GPU\n","import torch\n","print(\"CUDA available?\", torch.cuda.is_available())\n","\n","# Nous allons utiliser des objets ``torch.device`` pour d√©placer les tensors √†\n","# l'int√©rieur et en dehors des GPU\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")          # un objet CUDA device\n","    x = torch.Tensor([1.0, 2.0, 3.0])\n","    y = torch.ones_like(x, device=device)  # Cr√©√© un tenseur sur GPU\n","    x = x.to(device)                       # ou utilisez just le string ``.to(\"cuda\")``\n","    z = x + y\n","    print(z)\n","    print(z.to(\"cpu\", torch.double))       # ``.to`` peut √©galement changer le dtype"]},{"cell_type":"markdown","source":["## <font color='#3b4859'>1.13. ‚öíÔ∏è **EXERCICE** </font>\n","\n","Ce bref exercice va vous permettre de tester les diff√©rentes notions vues dans la premi√®re partie."],"metadata":{"id":"IVykFZdVzMcJ"}},{"cell_type":"markdown","source":["1Ô∏è‚É£ Cr√©ez un tenseur matriciel ```t```, de dimensions $50 \\times 100$ initialis√© avec des valeurs enti√®res entre 0 et 10."],"metadata":{"id":"n-fTonnmzcYG"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","t = None\n","\n","### FIN DU CODE ###"],"metadata":{"id":"u-YtxnN51VNo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2Ô∏è‚É£ Changez le type du tenseur ```t``` pr√©c√©demment cr√©√© en ```float32```."],"metadata":{"id":"bLZ1uPok13Rl"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","t = None\n","\n","### FIN DU CODE ###"],"metadata":{"id":"rKNJTJfdzc26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3Ô∏è‚É£ Redimensionnez le tenseur ```t``` en ```t2``` matrice de dimension $1000 \\times 5$."],"metadata":{"id":"Xs5TVXL82n1_"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","t2 = None\n","\n","### FIN DU CODE ###"],"metadata":{"id":"j51vLaM53Uhl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4Ô∏è‚É£ Effectuez le calcul suivant : $t3 = t2 . t2^T$\n"],"metadata":{"id":"ndU3YgeaaJZF"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","t3 = None\n","\n","### FIN DU CODE ###"],"metadata":{"id":"lrD-ZBtI46Ms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color='#3b4859'>2. Autograd: Diff√©rentiation automatique</font>"],"metadata":{"id":"AVxOoNJDll2K"}},{"cell_type":"markdown","metadata":{"id":"UqClHzpw6Lg3"},"source":["## <font color='#3b4859'>2.1. D√©finition et rappel</font>\n","\n","Le package autograd permet la diff√©rentiation automatique de toutes les op√©rations effectu√©es sur un tenseur. Le framework cr√©√© un graphe de calcul et le compl√®te au fur et a mesure des op√©rations, ce qui signifie que la backpropagation est d√©finie selon l'ex√©cution du code.\n","\n","``torch.Tensor`` est la classe centrale du package. En ex√©cutant l'attribut ``.requires_grad`` comme ``True``, **toutes le op√©rations sont track√©es dans le Tenseur**.\n","A la fin des calculs, l'appel de ``.backward()`` permet de **r√©cup√©rer le calcul des gradients de fa√ßon automatique**. Les gradients du Tenseurs seront accumul√©s dans l'attribut ``.grad``.\n","\n","Pour **arr√™ter le tracking de l'historique des gradients**, il faut utiliser ``.detach()`` pour d√©tacher le Tenseur de l'historique de calculs, et pour emp√™cher de tracker les calculs futurs.\n","\n","Pour **emp√™cher l'historique de tracking (et l'utilisation de la m√©moire)**, il est aussi possible d'√©crire le code dans une condition ``with torch.no_grad():``. C'est en g√©n√©ral utile lors de l'√©valuation d'un mod√®le car le mod√®le peut avoir des param√®tres entra√Ænables contenant le param√®tre `requires_grad=True` alors que l'on n'a pas besoin d'utiliser les gradients.\n"]},{"cell_type":"markdown","source":["## <font color='#3b4859'>2.2. Application</font>\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"omhgncA5e1oH"}},{"cell_type":"markdown","source":["### <font color='#3b4859'>2.2.1. Exemple 1</font>\n","\n","On s'int√©resse √† l'op√©ration suivante :\n","\n","$Y = \\frac{1}{4}\\sum_i z_i$ avec $z_i = 3(x_i+2)^2$\n","\n","On veut calculer la d√©riv√©e suivante : $\\frac{\\partial Y}{\\partial x_i}$.\n","\n","Analytiquement, le r√©sultat est obtenu avec le calcul suivant : $\\frac{\\partial Y}{\\partial x_i} = \\frac{1}{4}\\frac{\\partial z_i}{\\partial x_i} = \\frac{1}{4}.3.2(x_i+2) = \\frac{3}{2}(x_i+2)$\n","</br>\n","Par cons√©quent :\n","$\\frac{\\partial Y}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$. </br>\n","\n","Dans la cellule suivante, on impl√©mente le calcul qui permet d'obtenir $Y$ :"],"metadata":{"id":"qmvt769YAvt3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3JXX8xr8HWJ"},"outputs":[],"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)\n","z = 3 * (x + 2)**2\n","print(z)\n","Y = z.mean()\n","print(Y)"]},{"cell_type":"markdown","source":["A la fin du calcul, la fonction ``.backward()`` permet de calculer les gradients."],"metadata":{"id":"z6Kb2nfIifUk"}},{"cell_type":"code","source":["Y.backward()"],"metadata":{"id":"0t_q4S2RhT4Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Et ainsi, l'attribut ``.grad`` permet d'acc√©der aux valeurs des gradients :"],"metadata":{"id":"v1W1JzG_jFGj"}},{"cell_type":"code","source":["x.grad"],"metadata":{"id":"eWaLoLZXhXU4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La fonction ``.backward()`` nous a permis de calculer la valeur : $\\frac{\\partial Y}{\\partial x_i}\\bigr\\rvert_{x_i=1}$"],"metadata":{"id":"P7lvfAwkkc2T"}},{"cell_type":"markdown","metadata":{"id":"TbbCf8kM8-hd"},"source":["### <font color='#3b4859'>2.2.2. Exemple 2</font>\n","**üí° Astuce :** ``.requires_grad_( ... )`` permet de changer l'√©tat ``requires_grad`` d'un Tenseur existant. L'input par d√©faut de ``.requires_grad_( ... ) `` est ``False``.\n","\n","Dans cet exemple, nous consid√©rons la fonction suivante :\n","\n","$Y = {[\\frac{3x}{x-1}]}^{2}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrCqBdbE9Avl"},"outputs":[],"source":["x = 2 * torch.ones(2, 2)\n","x = ((x * 3) / (x - 1))\n","print(x.requires_grad)\n","\n","x.requires_grad_(True)\n","\n","print(x.requires_grad)\n","y = (x * x).sum()\n","print(y.grad_fn)"]},{"cell_type":"markdown","source":["A la fin du calcul, l'appel de la fonction ``.backward()`` permet de calculer les gradients √† partir du graphe de calculs."],"metadata":{"id":"sRaHqcgSFEtV"}},{"cell_type":"code","source":["y.backward()"],"metadata":{"id":"sVBRlkW1mvCm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Et ainsi, l'attribut ``.grad`` permet d'acc√©der aux valeurs des gradient :"],"metadata":{"id":"3pxijXJQFOQ0"}},{"cell_type":"code","source":["x.grad"],"metadata":{"id":"_HZcmpWzmwZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On obtient ainsi $\\frac{\\partial Y}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 12$"],"metadata":{"id":"Od_P0JAIF9pU"}},{"cell_type":"markdown","source":["‚öíÔ∏è **EXERCICE :**\n","\n","1Ô∏è‚É£\n","- √âcrivez √† l'aide de tenseurs PyTorch une fonction $g$ qui calcule la **cosine similarity** de deux vecteurs *float* $\\mathbf{x}$ and $\\mathbf{y}$ selon la formule\n","$$g(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x}^T \\mathbf{y}}{|| \\mathbf{x} ||_2 || \\mathbf{y} ||_2 }$$\n","\n","- Vous utiliserez l'**autograd** pour calculer les d√©riv√©es par rapport √† $\\mathbf{x} \\in \\mathbb{R}^3$ et $\\mathbf{y} \\in \\mathbb{R}^3$ pour les valeurs donn√©es.\n","\n","\n","Vous pourrez utiliser `torch.linalg.norm` pour le calcul de la norme 2 : [voir la documentation](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm)\n","\n","2Ô∏è‚É£\n","- Quelle est la valeur attendue pour la cosine similarity de deux vecteurs colin√©aires ? quelle est la valeur attendue des gradients de la fonction cosine par rapport √† chacun des vecteurs en input ?\n","\n","- Calculez $\\nabla_x g(x, y)$ et $\\nabla_y g(x, y)$ avec $\\mathbf{x}$ et $\\mathbf{y}$ d√©finis selon $\\mathbf{x} = \\alpha \\cdot \\mathbf{y}$ avec $\\alpha \\in \\mathbb{R}$.\n","\n","V√©rifiez vos r√©sultats avec PyTorch.\n"],"metadata":{"id":"O9bli1Z4GGK3"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","#1)\n","def g(x, y):\n","  return None\n","\n","x = torch.tensor([0, 1, 2], dtype=torch.float32, requires_grad=True)\n","y = torch.tensor([3, 0.9, 2.2], dtype=torch.float32, requires_grad=True)\n","\n","cosine = g(x, y)\n","print(\"cosine: \", cosine)\n","\n","None\n","print('x.grad:', x.grad)\n","print('y.grad:', y.grad)\n","\n","#2)\n","x = None\n","y = None\n","\n","cosine = g(x, y)\n","print(\"cosine: \", cosine)\n","\n","None\n","print('x.grad:', x.grad)\n","print('y.grad:', y.grad)\n","### FIN DU CODE ###"],"metadata":{"id":"9AkPe4ryGFo_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La cosine similarity de deux vecteurs colin√©aires est √©gale √† $¬±1$ selon que les vecteurs soient orient√©s dans le m√™me sens. Ainsi, le gradient de la cosine similarity sera nul car la valeur de la cosine similarity est maximale (ou minimale)."],"metadata":{"id":"OQHeiUkpMNDo"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"vscode":{"interpreter":{"hash":"b807bed79a7ee0d6e063739364c317c149a792c3c6ff4c65e6a49611843ffbd8"}},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}