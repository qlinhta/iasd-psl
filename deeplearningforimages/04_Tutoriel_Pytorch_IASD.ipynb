{"cells":[{"cell_type":"markdown","source":["# <font color='#3b4859'><b>Les fondamentaux de PyTorch</b></font>\n","\n","Bienvenue dans le suite du cours les fondammentaux au Deep Learning. Dans les Notebooks précédents vous avez utilisé NumPy pour coder vos premiers réseaux de neurones mais en pratique, vous utiliserez des frameworks dédiées au Deep Learning.\n","\n","Dans ce Notebook vous allez découvrir comment utiliser PyTorch pour implémenter vos algorithmes."],"metadata":{"id":"iDoU8olXi3OW"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ZGtcZbNpMMxR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697280208040,"user_tz":-120,"elapsed":4112,"user":{"displayName":"Hippolyte Mayard","userId":"12682917716835997811"}},"outputId":"0b29ca9f-7577-4476-eda8-b102dd0d9aa6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Version de pytorch :  2.0.1+cu118\n"]}],"source":["import time\n","\n","import numpy as np\n","import torch\n","print(\"Version de pytorch : \", torch.__version__)"]},{"cell_type":"code","source":["torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOrdIsVayG1I","executionInfo":{"status":"ok","timestamp":1697280264142,"user_tz":-120,"elapsed":210,"user":{"displayName":"Hippolyte Mayard","userId":"12682917716835997811"}},"outputId":"5693b2b5-c70c-4011-fdb6-5f0989e1864b"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpZ4yZgt_BxL"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"Qj6ZQ96sbccS"},"source":["# <font color='#3b4859'>0. PyTorch vs. NumPy</font>\n","\n","Dans les Notebooks précédents vous avez vu que NumPy permettait d'effectuer des calculs d'algèbre linéaire.\n","\n","PyTorch va permettre d'accélérer ces calculs en utilisant les **tensors** et surtout, les **GPU**."]},{"cell_type":"markdown","metadata":{"id":"48PbKHR4jpI2"},"source":["### <font color='#3b4859'>Paramétrer l'accès du notebook aux GPU</font>\n","\n","Pour ce faire vous devez effectuer la procédure suivante, allez dans : **Modifier -> Paramètres du Notebook -> Accélérateur Matériel**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBmXefMsMNjl"},"outputs":[],"source":["# Effectuons un test rapide pour voir la différence des temps de calculs\n","# Création de deux matrices A et B de dim(5000, 5000)\n","d = 5000\n","\n","A = np.random.rand(d, d).astype(np.float32)\n","B = np.random.rand(d, d).astype(np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pZlGXMXMQhI"},"outputs":[],"source":["# Puis on effectue le produit\n","s = time.time()\n","C = A.dot(B)\n","print(time.time() - s)"]},{"cell_type":"markdown","metadata":{"id":"bQ9prgnIba6N"},"source":["Pour allouer un Tenseur à un GPU, on utilise la fonction `.cuda()` ou alors `to.(\"cuda\")`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgHiiV7AMSgP"},"outputs":[],"source":["# La même chose avec PyTorch\n","d = 5000\n","\n","A = torch.rand(d, d).cuda()\n","B = torch.rand(d, d).cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uo-pt5xHMe_p"},"outputs":[],"source":["s = time.time()\n","C = torch.mm(A,B)\n","print(time.time() - s)"]},{"cell_type":"markdown","metadata":{"id":"Vc5xoYYQwRJm"},"source":["Vous pouvez constater ici que l'utilisation d'un tensur sur GPU permet d'accélérer les calculs."]},{"cell_type":"markdown","metadata":{"id":"w44C7sOM6L0c"},"source":["# <font color='#3b4859'>1. Les tenseurs et leurs opérations classiques</font>\n","\n","Les Tenseurs sont des objets similaires à des NumPy ndarrays. Cependant, les Tenseurs peuvent être utilisés sur GPU afin d'accélerer les calculs.\n"]},{"cell_type":"markdown","source":["## <font color='#3b4859'>1.1. Création de tenseurs à partir de données</font>\n","Les Tenseurs peuvent être créés **à partir d'une liste** en utilisant la fonction [```torch.tensor```](https://pytorch.org/docs/stable/generated/torch.tensor.html)."],"metadata":{"id":"FE9oZL7_jbkU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4WjnSggNiFI"},"outputs":[],"source":["# Exemple en dimension 1\n","data = [1.0, 2.0, 3.0]\n","tensor = torch.tensor(data)\n","print(\"Exemple en dimension 1\")\n","print(tensor)\n","\n","# Example en dimension 2\n","data = [[1., 2., 3.], [4., 5., 6.]]\n","tensor = torch.tensor(data)\n","print(\"\\nExemple en dimension 2\")\n","print(tensor)\n","\n","# Exemple en dimension 3\n","data = [[[1.,2.], [3.,4.]],\n","        [[5.,6.], [7.,8.]]]\n","tensor = torch.tensor(data)\n","print(\"\\nExemple en dimension 3\")\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"RqNDaxwIchKY"},"source":["## <font color='#3b4859'>1.2. Initialiser d'un tenseur vide </font>\n","\n","Il est possible de d'initialiser un tenseur vide en utilisant la fonction [```torch.empty```](https://pytorch.org/docs/stable/generated/torch.empty.html).\n","\n","Une matrice \"vide\", c'est à dire non initialisée, est déclarée. Elle ne contient pas de valeurs définies et connues avant d'être utilisée.\n","Lorsqu'une matrice non initialisée est créée, les valeurs qui se trouvaient dans la mémoire allouée à ce moment-là apparaissent comme les valeurs initiales.\n","\n","Vous trouverez un exemple d'utilisation de la fonction ```torch.empty``` en exécutant la cellule suivante."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8GOnu9o6tmb"},"outputs":[],"source":["# Initialisation d'une matrice non initialisée de taille 2x3\n","x = torch.empty(2, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"GQ0JG7QudIUa"},"source":["## <font color='#3b4859'>1.3. Initialiser un tenseur de façon aléatoire </font>\n","\n","On peut initialiser un tenseur de façon aléatoire, de différentes façons.\n","\n","- La fonction [```torch.rand```](https://pytorch.org/docs/stable/generated/torch.rand.html) retourne un tenseur dont les valeurs sont tirées aléatoirement en suivant une loi uniforme sur $[0,1[$.\n","- La fonction [```torch.randn```](https://pytorch.org/docs/stable/generated/torch.randn.html) retourne un tenseur dont les valeurs sont tirées aléatoirement en suivant une loi normale de moyenne 0 et d'écart type 1.\n","\n","Vous trouverez des exemples d'utilisation des fonctions ```torch.rand``` et ```torch.randn``` en exécutant la cellule suivante."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJ2GvIugc7eq"},"outputs":[],"source":["# Initialisation d'une matrice aléatoire de taille 2x3\n","# les valeurs suivent une loi uniforme sur [0,1[\n","x = torch.rand(2, 3)\n","print(x)\n","\n","# Initialisation d'une matrice aléatoire de taille 2x3\n","# les valeurs suivent une loi normale de moyenne 0 et d'écart type 1\n","x = torch.randn(2, 3)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"oiUnw5uxdTEN"},"source":["## <font color='#3b4859'>1.4. Initialiser un tenseur contenant uniquement des 0 ou des 1 </font>\n","\n","- La fonction [```torch.zeros```](https://pytorch.org/docs/stable/generated/torch.zeros.html) retourne un tenseur composé uniquement de 0\n","- La fonction [```torch.ones```](https://pytorch.org/docs/stable/generated/torch.ones.html) retourne un tenseur composé uniquement de 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M543AqqpdOQy"},"outputs":[],"source":["# Initialisation d'une matrice aléatoire de taille 2x3\n","# Remplie de valeurs 0\n","x = torch.zeros(2, 3)\n","print(\"Matrice de zeros\")\n","print(x)\n","\n","# Initialisation d'une matrice aléatoire de taille 2x3\n","# Remplie de valeurs 1\n","x = torch.ones(2, 3)\n","print(\"Matrice de uns\")\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"s0HQKRy2wsDd"},"source":["## <font color='#3b4859'>1.5. Créer un tenseur à partir d'un tenseur existant </font>\n","\n","Il est possible d'initialiser un tenseur à partir d'un tenseur existant."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"visdOehpxqd-"},"outputs":[],"source":["x = x.new_ones(2, 3, dtype=torch.double)      # new_* methods take in sizes\n","print(x)\n","\n","x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n","print(x)                                      # result has the same size"]},{"cell_type":"markdown","metadata":{"id":"ehiXz-UuwRJp"},"source":["## <font color='#3b4859'>1.6. Il est important de typer les tenseurs </font>\n","\n","Lorsque vous définissez un tenseur, il est important de lui donner un type, de façon similaire aux objets array de Numpy.\n","\n","Les types de tenseurs sont les types usuels :\n"," - ```torch.int32``` (équivalent à ```torch.long```)\n"," - ```torch.int64```\n"," - ```torch.float32``` (équivalent à ```torch.float```)\n"," - ```torch.float64```\n"," - ```torch.bool```\n"]},{"cell_type":"markdown","metadata":{"id":"LxHQ39iRwRJp"},"source":["Il existe plusieurs façon de préciser le type d'un tenseur :\n","- lors de son initialisation en précisant en paramètre  ```dtype = TYPE```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzjM0teMwRJp"},"outputs":[],"source":["data = [[1, 2, 3], [4, 5, 6]]\n","\n","# Initialisation d'un tenseur d'int\n","int_tensor = torch.tensor(data, dtype = torch.int32)\n","print(int_tensor)\n","print(int_tensor.dtype)\n","\n","# Initialisation d'un tenseur de float\n","float_tensor = torch.tensor(data, dtype = torch.float32)\n","print(float_tensor)\n","print(float_tensor.dtype)"]},{"cell_type":"markdown","metadata":{"id":"kk1tbHqPwRJq"},"source":["- en appelant le méthode ```.to(TYPE)```ou bien ```.float()``` (fonctionne également avec la méthod ```.int()``` ou ```.bool()```)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKLqE358wRJq"},"outputs":[],"source":["int_tensor = torch.tensor(data)\n","\n","# en utilisant .to(TYPE)\n","float_tensor = int_tensor.to(torch.float32)\n","print(float_tensor)\n","print(float_tensor.dtype)\n","\n","# en utilisant .float()\n","float_tensor = int_tensor.float()\n","print(float_tensor)\n","print(float_tensor.dtype)"]},{"cell_type":"markdown","metadata":{"id":"HLZfCcObzBTe"},"source":["## <font color='#3b4859'>1.7. Size et Shape d'un Tenseur </font>\n","\n","Pour connaître les dimensions d'un tenseur, on peut utiliser les méthodes suivantes :\n","- ```.shape```\n","- ```.size()```\n","\n","On obtient un objet `torch.Size`. Il s'agit d'un tuple. Par conséquent, il supporte toutes les opérations valables sur les tuples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVIu-uZnzEm-"},"outputs":[],"source":["# Example with 1-D data\n","data = [1.0, 2.0, 3.0]\n","tensor = torch.Tensor(data)\n","print(\"Exemple en 1-D\")\n","print(tensor)\n","print(tensor.size())\n","print(tensor.shape)\n","\n","# Example with 2-D data\n","data = [[1., 2., 3.], [4., 5., 6]]\n","tensor = torch.Tensor(data)\n","print(\"\\nExemple en 2-D\")\n","print(tensor)\n","print(tensor.size())\n","print(tensor.shape)\n","\n","# Example with 3-D data\n","data = [[[1.,2.], [3.,4.]],\n","        [[5.,6.], [7.,8.]]]\n","tensor = torch.Tensor(data)\n","print(\"\\nExemple en 3-D \")\n","print(tensor)\n","print(tensor.size())\n","print(tensor.shape)"]},{"cell_type":"markdown","metadata":{"id":"cYai6C6H0kKx"},"source":["## <font color='#3b4859'>1.8. Opérations avec les Tenseurs </font>\n","La plupart des opérations avec les Tenseurs sont similaires à celles implémentées dans NumPy pour les arrays :\n","\n","- addition\n","- soustraction\n","- multiplication (division) par un scalaire\n","- produit scalaire\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRj42hPO0prW"},"outputs":[],"source":["x = torch.Tensor([ 1., 2., 3. ])\n","y = torch.Tensor([ 4., 5., 6. ])\n","\n","# Opérations arithmétiques\n","# Addition\n","z1 = x + y\n","print(z1)\n","\n","# Addition\n","z2 = x - y\n","print(z2)\n","\n","# Multiplication (division) par un scalaire\n","z3 = x / 3\n","print(z3)\n","\n","# fonction dot : produit scalaire\n","print(torch.dot(x,y))\n","print(x @ y )"]},{"cell_type":"markdown","metadata":{"id":"W9Y79GmuwRJr"},"source":["## <font color='#3b4859'>1.9. Opérations in-place</font>\n","\n","Les opérations sur les tenseurs avec un underscore `_` sont appelées \"in place\" opérations. Elles s'appliquent directement à la suite du tenseur (exemples: `x.copy_(y)`, `x.t_()`) et changent la valeurs de `x`.\n","\n","Voir la documentation officielle ([PyTorch official documentation](http://pytorch.org/docs/torch.html)) pour voir la liste, non exhaustive, des opérations proposées par Pytorch.\n","\n","Vous trouverez un exemple d'utilisation de fonction in-place dans la cellule suivante :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjy0Zqik0-wL"},"outputs":[],"source":["# In-place addition\n","\n","x = torch.Tensor([ 1., 2., 3. ])\n","y = torch.Tensor([ 4., 5., 6. ])\n","\n","y.add_(x)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"_IqeYxdg3Bvk"},"source":["## <font color='#3b4859'>1.10. Indexation et reshaping des Tenseurs</font>\n","\n","L'indexation des tenseurs est similaire à celle d'un Numpy Array.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9REyiT-wRJr"},"outputs":[],"source":["x = torch.Tensor([[1., 2., 3.], [4., 5., 6]])\n","print(x[:, 1]) # selectionne la colonne numéro 1\n","print(x[-1, :]) # selectionne la dernière ligne"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUwsXsfw26g_"},"outputs":[],"source":["x = torch.randn(4, 4)\n","y = x.view(16)\n","z = x.view(-1, 8)  # l'index -1 permet d'inférer à partir des autres dimensions définies\n","#ici comme l'autre dimension est 8, la première dimension sera égale à 16/2=8)\n","print(x.size(), y.size(), z.size())"]},{"cell_type":"markdown","metadata":{"id":"f33ljXvsh5IB"},"source":["Pour sélectionner les Tenseurs contenant un objet en 1-D, `.item()` permet de récupérer l'objet."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clJzuzI93U7k"},"outputs":[],"source":["x = torch.randn(1)\n","print(x)\n","print(x.item())"]},{"cell_type":"markdown","metadata":{"id":"zRbkdZcArlk7"},"source":["## <font color='#3b4859'>1.11. Convertir un Tenseur depuis Numpy ou en Numpy array</font>\n","La conversion d'un Tenseur en NumPy array et vice versa est très simple.\n","\n","Le Torch Tenseur et la Numpy Array vont **partager le même espace en mémoire** (si le Tenseur est sur CPU) et tout changement de l'un entraîne un changement de l'autre.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlLtucZM3g-N"},"outputs":[],"source":["a = torch.ones(5)\n","print(\"Original a:\", a)\n","\n","b = a.numpy()\n","print(\"Original b:\", b)\n","\n","a.add_(1)\n","print(\"New a:\", a)\n","print(\"New b:\", b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cij37AWq31XG"},"outputs":[],"source":["a = np.ones(5)\n","b = torch.from_numpy(a)\n","np.add(a, 1, out=a)\n","print(a)\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"npIpV4SpjQuG"},"source":["## <font color='#3b4859'>1.12. Les Tenseurs CUDA </font>\n","\n","Les Tenseurs peuvent changés facilement de device (CPU ou GPU) en utilisant `.to()`.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvBpFQTulo95"},"outputs":[],"source":["# Essayez d'exécuter cette cellule avec et sans GPU\n","import torch\n","print(\"CUDA available?\", torch.cuda.is_available())\n","\n","# Nous allons utiliser des objets ``torch.device`` pour déplacer les tensors à\n","# l'intérieur et en dehors des GPU\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")          # un objet CUDA device\n","    x = torch.Tensor([1.0, 2.0, 3.0])\n","    y = torch.ones_like(x, device=device)  # Créé un tenseur sur GPU\n","    x = x.to(device)                       # ou utilisez just le string ``.to(\"cuda\")``\n","    z = x + y\n","    print(z)\n","    print(z.to(\"cpu\", torch.double))       # ``.to`` peut également changer le dtype"]},{"cell_type":"markdown","source":["## <font color='#3b4859'>1.13. ⚒️ **EXERCICE** </font>\n","\n","Ce bref exercice va vous permettre de tester les différentes notions vues dans la première partie."],"metadata":{"id":"IVykFZdVzMcJ"}},{"cell_type":"markdown","source":["1️⃣ Créez un tenseur matriciel ```t```, de dimensions $50 \\times 100$ initialisé avec des valeurs entières entre 0 et 10."],"metadata":{"id":"n-fTonnmzcYG"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","t = None\n","\n","### FIN DU CODE ###"],"metadata":{"id":"u-YtxnN51VNo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2️⃣ Changez le type du tenseur ```t``` précédemment créé en ```float32```."],"metadata":{"id":"bLZ1uPok13Rl"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","t = None\n","\n","### FIN DU CODE ###"],"metadata":{"id":"rKNJTJfdzc26"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3️⃣ Redimensionnez le tenseur ```t``` en ```t2``` matrice de dimension $1000 \\times 5$."],"metadata":{"id":"Xs5TVXL82n1_"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","t2 = None\n","\n","### FIN DU CODE ###"],"metadata":{"id":"j51vLaM53Uhl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["4️⃣ Effectuez le calcul suivant : $t3 = t2 . t2^T$\n"],"metadata":{"id":"ndU3YgeaaJZF"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","t3 = None\n","\n","### FIN DU CODE ###"],"metadata":{"id":"lrD-ZBtI46Ms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color='#3b4859'>2. Autograd: Différentiation automatique</font>"],"metadata":{"id":"AVxOoNJDll2K"}},{"cell_type":"markdown","metadata":{"id":"UqClHzpw6Lg3"},"source":["## <font color='#3b4859'>2.1. Définition et rappel</font>\n","\n","Le package autograd permet la différentiation automatique de toutes les opérations effectuées sur un tenseur. Le framework créé un graphe de calcul et le complète au fur et a mesure des opérations, ce qui signifie que la backpropagation est définie selon l'exécution du code.\n","\n","``torch.Tensor`` est la classe centrale du package. En exécutant l'attribut ``.requires_grad`` comme ``True``, **toutes le opérations sont trackées dans le Tenseur**.\n","A la fin des calculs, l'appel de ``.backward()`` permet de **récupérer le calcul des gradients de façon automatique**. Les gradients du Tenseurs seront accumulés dans l'attribut ``.grad``.\n","\n","Pour **arrêter le tracking de l'historique des gradients**, il faut utiliser ``.detach()`` pour détacher le Tenseur de l'historique de calculs, et pour empêcher de tracker les calculs futurs.\n","\n","Pour **empêcher l'historique de tracking (et l'utilisation de la mémoire)**, il est aussi possible d'écrire le code dans une condition ``with torch.no_grad():``. C'est en général utile lors de l'évaluation d'un modèle car le modèle peut avoir des paramètres entraînables contenant le paramètre `requires_grad=True` alors que l'on n'a pas besoin d'utiliser les gradients.\n"]},{"cell_type":"markdown","source":["## <font color='#3b4859'>2.2. Application</font>\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"omhgncA5e1oH"}},{"cell_type":"markdown","source":["### <font color='#3b4859'>2.2.1. Exemple 1</font>\n","\n","On s'intéresse à l'opération suivante :\n","\n","$Y = \\frac{1}{4}\\sum_i z_i$ avec $z_i = 3(x_i+2)^2$\n","\n","On veut calculer la dérivée suivante : $\\frac{\\partial Y}{\\partial x_i}$.\n","\n","Analytiquement, le résultat est obtenu avec le calcul suivant : $\\frac{\\partial Y}{\\partial x_i} = \\frac{1}{4}\\frac{\\partial z_i}{\\partial x_i} = \\frac{1}{4}.3.2(x_i+2) = \\frac{3}{2}(x_i+2)$\n","</br>\n","Par conséquent :\n","$\\frac{\\partial Y}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$. </br>\n","\n","Dans la cellule suivante, on implémente le calcul qui permet d'obtenir $Y$ :"],"metadata":{"id":"qmvt769YAvt3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3JXX8xr8HWJ"},"outputs":[],"source":["x = torch.ones(2, 2, requires_grad=True)\n","print(x)\n","z = 3 * (x + 2)**2\n","print(z)\n","Y = z.mean()\n","print(Y)"]},{"cell_type":"markdown","source":["A la fin du calcul, la fonction ``.backward()`` permet de calculer les gradients."],"metadata":{"id":"z6Kb2nfIifUk"}},{"cell_type":"code","source":["Y.backward()"],"metadata":{"id":"0t_q4S2RhT4Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Et ainsi, l'attribut ``.grad`` permet d'accéder aux valeurs des gradients :"],"metadata":{"id":"v1W1JzG_jFGj"}},{"cell_type":"code","source":["x.grad"],"metadata":{"id":"eWaLoLZXhXU4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La fonction ``.backward()`` nous a permis de calculer la valeur : $\\frac{\\partial Y}{\\partial x_i}\\bigr\\rvert_{x_i=1}$"],"metadata":{"id":"P7lvfAwkkc2T"}},{"cell_type":"markdown","metadata":{"id":"TbbCf8kM8-hd"},"source":["### <font color='#3b4859'>2.2.2. Exemple 2</font>\n","**💡 Astuce :** ``.requires_grad_( ... )`` permet de changer l'état ``requires_grad`` d'un Tenseur existant. L'input par défaut de ``.requires_grad_( ... ) `` est ``False``.\n","\n","Dans cet exemple, nous considérons la fonction suivante :\n","\n","$Y = {[\\frac{3x}{x-1}]}^{2}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrCqBdbE9Avl"},"outputs":[],"source":["x = 2 * torch.ones(2, 2)\n","x = ((x * 3) / (x - 1))\n","print(x.requires_grad)\n","\n","x.requires_grad_(True)\n","\n","print(x.requires_grad)\n","y = (x * x).sum()\n","print(y.grad_fn)"]},{"cell_type":"markdown","source":["A la fin du calcul, l'appel de la fonction ``.backward()`` permet de calculer les gradients à partir du graphe de calculs."],"metadata":{"id":"sRaHqcgSFEtV"}},{"cell_type":"code","source":["y.backward()"],"metadata":{"id":"sVBRlkW1mvCm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Et ainsi, l'attribut ``.grad`` permet d'accéder aux valeurs des gradient :"],"metadata":{"id":"3pxijXJQFOQ0"}},{"cell_type":"code","source":["x.grad"],"metadata":{"id":"_HZcmpWzmwZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On obtient ainsi $\\frac{\\partial Y}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 12$"],"metadata":{"id":"Od_P0JAIF9pU"}},{"cell_type":"markdown","source":["⚒️ **EXERCICE :**\n","\n","1️⃣\n","- Écrivez à l'aide de tenseurs PyTorch une fonction $g$ qui calcule la **cosine similarity** de deux vecteurs *float* $\\mathbf{x}$ and $\\mathbf{y}$ selon la formule\n","$$g(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x}^T \\mathbf{y}}{|| \\mathbf{x} ||_2 || \\mathbf{y} ||_2 }$$\n","\n","- Vous utiliserez l'**autograd** pour calculer les dérivées par rapport à $\\mathbf{x} \\in \\mathbb{R}^3$ et $\\mathbf{y} \\in \\mathbb{R}^3$ pour les valeurs données.\n","\n","\n","Vous pourrez utiliser `torch.linalg.norm` pour le calcul de la norme 2 : [voir la documentation](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm)\n","\n","2️⃣\n","- Quelle est la valeur attendue pour la cosine similarity de deux vecteurs colinéaires ? quelle est la valeur attendue des gradients de la fonction cosine par rapport à chacun des vecteurs en input ?\n","\n","- Calculez $\\nabla_x g(x, y)$ et $\\nabla_y g(x, y)$ avec $\\mathbf{x}$ et $\\mathbf{y}$ définis selon $\\mathbf{x} = \\alpha \\cdot \\mathbf{y}$ avec $\\alpha \\in \\mathbb{R}$.\n","\n","Vérifiez vos résultats avec PyTorch.\n"],"metadata":{"id":"O9bli1Z4GGK3"}},{"cell_type":"code","source":["### CODEZ ICI : Remplacez les None par votre code ###\n","\n","#1)\n","def g(x, y):\n","  return None\n","\n","x = torch.tensor([0, 1, 2], dtype=torch.float32, requires_grad=True)\n","y = torch.tensor([3, 0.9, 2.2], dtype=torch.float32, requires_grad=True)\n","\n","cosine = g(x, y)\n","print(\"cosine: \", cosine)\n","\n","None\n","print('x.grad:', x.grad)\n","print('y.grad:', y.grad)\n","\n","#2)\n","x = None\n","y = None\n","\n","cosine = g(x, y)\n","print(\"cosine: \", cosine)\n","\n","None\n","print('x.grad:', x.grad)\n","print('y.grad:', y.grad)\n","### FIN DU CODE ###"],"metadata":{"id":"9AkPe4ryGFo_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La cosine similarity de deux vecteurs colinéaires est égale à $±1$ selon que les vecteurs soient orientés dans le même sens. Ainsi, le gradient de la cosine similarity sera nul car la valeur de la cosine similarity est maximale (ou minimale)."],"metadata":{"id":"OQHeiUkpMNDo"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"vscode":{"interpreter":{"hash":"b807bed79a7ee0d6e063739364c317c149a792c3c6ff4c65e6a49611843ffbd8"}},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}