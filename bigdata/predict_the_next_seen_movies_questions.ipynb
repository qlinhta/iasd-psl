{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoNpnIQOBcfx"
   },
   "source": [
    "# Predict the next seen movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nMdnOrdauzb"
   },
   "source": [
    "> The goal of this project is to validate your skills learned during the previous tutorials with the PySpark distributed computing software. You will build and test several implementations of recommendation systems. Here are the parts of the project:\n",
    "> \n",
    "> * Part A: Load and preprocess the dataset. \n",
    "> * Part B: Build a first set of recommendation algorithms: naive with recurring pairs, a priori, and fp-growth to infer rules from a dataset.\n",
    "> * Part C: Implement the PLSI algorithm (\"Probabilistic Latent Semantic Indexing\"), which is one way to create embeddings from a dataset. These embeddings can be used to fuel a recommendation engine.\n",
    "> * Part D: Predict the next movies seen by a user in function of the last movies that he/she has seen with the previous implemented algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69sUuh3EEEBw"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxRuqODPWgYM"
   },
   "source": [
    "## Install Spark Environment\n",
    "\n",
    "> Since you are running on Google Colab, you will need to install Spark by ourselves, every time we run a new session. You need to install Spark, as well as a Java Runtime Environment.  Then you need to setup a few environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vFR3wKqDrrsZ",
    "ExecuteTime": {
     "end_time": "2024-01-30T15:17:40.555916Z",
     "start_time": "2024-01-30T15:17:37.511788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/quyenlinhta/IASD/iasd/lib/python3.10/site-packages (23.3.2)\r\n",
      "Requirement already satisfied: pyspark in /Users/quyenlinhta/IASD/iasd/lib/python3.10/site-packages (3.5.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/quyenlinhta/IASD/iasd/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip\n",
    "! pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBTHZp82G_Ea"
   },
   "source": [
    "Create and launch a Spark session with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "RKP62k6BW5Xz",
    "outputId": "3717708e-b09c-4b2d-82e6-508091a9f422",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-13T22:25:59.252403Z",
     "start_time": "2024-02-13T22:25:57.073086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/13 23:25:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/13 23:25:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set('spark.ui.port', '4050')\n",
    "\n",
    "\"\"\"spark = SparkSession.builder.config(conf=conf) \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Movie Recommendations\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "yA7rmX4p8BBo",
    "outputId": "032fdcab-c4d1-46f1-fa52-81c9c6aeef0c",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-13T22:25:59.692012Z",
     "start_time": "2024-02-13T22:25:59.255752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<pyspark.sql.session.SparkSession at 0x1085d7940>",
      "text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.0.27:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Movie Recommendations</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWQFahH3HfJ9"
   },
   "source": [
    "/!\\ The Spark UI link is not accessible. Use the optional next session if you want to access it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bDPyOrBGzkC"
   },
   "source": [
    "Uncomment and execute the following line, if you want to close and stop the created Spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FXvFSpAkGyMm",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-13T22:26:01.905243Z",
     "start_time": "2024-02-13T22:26:01.863672Z"
    }
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5yHDm0vXkIQ"
   },
   "source": [
    "## Optional step: Enable Spark UI access through a secure tunnel\n",
    "\n",
    "> This step is useful if you want to look at Spark UI.\n",
    "First, you need to create a free ngrok account : https://dashboard.ngrok.com/login.  \n",
    "Then connect on the website and copy your AuthToken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2bV9Ydi8sLs",
    "outputId": "c05116b9-54c9-489e-e61c-3545e13d01d6",
    "ExecuteTime": {
     "end_time": "2024-02-13T22:26:02.556311Z",
     "start_time": "2024-02-13T22:26:02.545774Z"
    }
   },
   "outputs": [],
   "source": [
    "# !uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCerOlcLXmgj",
    "outputId": "1ebd17be-82b4-4b5f-e7fc-36e8d1ada469",
    "ExecuteTime": {
     "end_time": "2024-02-13T22:26:03.085412Z",
     "start_time": "2024-02-13T22:26:03.074212Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "#!unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHgGqKOeAYVH",
    "outputId": "09fe2b9d-700d-4c9f-d911-3e4fd11c866f",
    "ExecuteTime": {
     "end_time": "2024-02-13T22:26:04.103111Z",
     "start_time": "2024-02-13T22:26:04.086167Z"
    }
   },
   "outputs": [],
   "source": [
    "#!./ngrok authtoken '2HYfdysedjgB5lcoeE8lqqShgIe_sZUm49MquX5okpssVMYQ' # <-------------- change this line !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1G-XfsyHAT84",
    "ExecuteTime": {
     "end_time": "2024-02-13T22:26:04.500428Z",
     "start_time": "2024-02-13T22:26:04.489507Z"
    }
   },
   "outputs": [],
   "source": [
    "#get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jWA3ILoOBAw",
    "outputId": "1b9b2d20-9fb8-4679-bf8b-e3648f16b31b",
    "ExecuteTime": {
     "end_time": "2024-02-13T22:26:05.508590Z",
     "start_time": "2024-02-13T22:26:05.495646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'import json\\nimport urllib\\n\\nngrok_tunnels = urllib.request.urlopen(\\'http://localhost:4040/api/tunnels\\').read().decode(\\'utf8\\')\\nspark_ui_url = json.loads(ngrok_tunnels)[\\'tunnels\\'][0][\\'public_url\\']\\nprint(\"Spark UI:\", spark_ui_url)'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import json\n",
    "import urllib\n",
    "\n",
    "ngrok_tunnels = urllib.request.urlopen('http://localhost:4040/api/tunnels').read().decode('utf8')\n",
    "spark_ui_url = json.loads(ngrok_tunnels)['tunnels'][0]['public_url']\n",
    "print(\"Spark UI:\", spark_ui_url)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSX0QbniXqdR"
   },
   "source": [
    "## Other Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vb4tXmFUWMVr",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-13T22:26:07.772231Z",
     "start_time": "2024-02-13T22:26:07.298099Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import urllib.request as req\n",
    "import zipfile\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from fractions import Fraction\n",
    "from decimal import Decimal\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T22:26:08.241076Z",
     "start_time": "2024-02-13T22:26:08.198837Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFUvUg2ufEkJ"
   },
   "source": [
    "# Part A - Dataset (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGC6YNJm8wh2"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "r_d9HFUnfPYY",
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:44.098451Z",
     "start_time": "2024-02-13T22:26:27.645845Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'\n",
    "filehandle, _ = req.urlretrieve(url)\n",
    "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
    "zip_file_object.namelist()\n",
    "zip_file_object.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3lK0389fXkK",
    "outputId": "6799d146-b413-49bd-bc75-07ec85377e8b",
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:44.242566Z",
     "start_time": "2024-02-13T22:27:44.091688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ml-20m/links.csv\r\n",
      "ml-20m/tags.csv\r\n",
      "ml-20m/genome-tags.csv\r\n",
      "ml-20m/ratings.csv\r\n",
      "ml-20m/README.txt\r\n",
      "ml-20m/genome-scores.csv\r\n",
      "ml-20m/movies.csv\r\n"
     ]
    }
   ],
   "source": [
    "!find ml-20m -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Uwruo0oIfaop",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:44.262648Z",
     "start_time": "2024-02-13T22:27:44.241529Z"
    }
   },
   "outputs": [],
   "source": [
    "movies_path = \"ml-20m/movies.csv\"\n",
    "ratings_path = \"ml-20m/ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hWQSjOtYWMVx",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:52.280557Z",
     "start_time": "2024-02-13T22:27:44.248665Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_df_csv = spark.read.options(header=True, inferSchema=True).csv(ratings_path)\n",
    "movies_df_csv = spark.read.options(header=True, inferSchema=True).csv(movies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1M21UsEBv5Up",
    "outputId": "3ed86c02-39e1-4d19-fea9-211c80f6d4c9",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:52.303185Z",
     "start_time": "2024-02-13T22:27:52.282487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y39-SiqZv9hu",
    "outputId": "3fc2191b-389b-4695-dddd-dc1c85282188",
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:52.341489Z",
     "start_time": "2024-02-13T22:27:52.303163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtR77Ttq8zT-"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "> In the following sections, we will work with algorithms that may scale more or less well with the size of the dataset. In this section we will develop several functions that will allow you to sample the dataset. Then in the next part of the notebook, it will be up to you to choose which tools you want to apply to your dataset in order to train your models, knowing that we want to have reasonable processing times (in the order of a minute), while having models that 'work' well.\n",
    ">\n",
    "> **Test all the functions that process the dataframe on a toy example, like what is done in first data processing question.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1zW1MHjDZnu"
   },
   "source": [
    "### Question A1\n",
    "\n",
    ">Write the *ratings_df_csv* and *movies_df_csv* DataFrame in a compressed parquet format.\n",
    ">\n",
    ">Reload them from parquet, to make next computations faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:58.693584Z",
     "start_time": "2024-02-13T22:27:52.307769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/13 23:27:52 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ratings_df_csv.write.parquet(\"ratings.parquet\", compression=\"gzip\")\n",
    "movies_df_csv.write.parquet(\"movies.parquet\", compression=\"gzip\")\n",
    "\n",
    "ratings_df = spark.read.parquet(\"ratings.parquet\")\n",
    "movies_df = spark.read.parquet(\"movies.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stqYh6SJGTCa"
   },
   "source": [
    "### Question A2\n",
    "\n",
    "> Compute an estimation of the whole *ratings_df* dataset size in memory.\n",
    ">\n",
    ">Find the amount of partitions used by *ratings_df*.\n",
    ">\n",
    ">We try to have partitions not too big (in order not to crash our executors), and not to small (dealing with too many small partitions can lead to issues, on driver side for example). For this reason, a rule of thumb to have partitions of 128MB is okay. Given this, what do you think of the dataframe? Is the amount of partitions okay? Or should we repartition it? What would be the function to use if we need to repartition the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:58.758517Z",
     "start_time": "2024-02-13T22:27:58.695865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IESUqOGp_rDf"
   },
   "source": [
    "### Question A3\n",
    "\n",
    ">Create a function named *remove_bad_ratings* that takes a rating dataframe as an argument, and returns a dataframe whose ratings are greater or equals to a rating_threshold, with default value of *3.5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:58.764706Z",
     "start_time": "2024-02-13T22:27:58.758746Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_bad_ratings(ratings_df, rating_threshold=3.5):\n",
    "    return ratings_df.filter(ratings_df.rating >= rating_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|      2|   3.5|1112486027|\n",
      "|     1|     29|   3.5|1112484676|\n",
      "|     1|     32|   3.5|1112484819|\n",
      "|     1|     47|   3.5|1112484727|\n",
      "|     1|     50|   3.5|1112484580|\n",
      "|     1|    112|   3.5|1094785740|\n",
      "|     1|    151|   4.0|1094785734|\n",
      "|     1|    223|   4.0|1112485573|\n",
      "|     1|    253|   4.0|1112484940|\n",
      "|     1|    260|   4.0|1112484826|\n",
      "|     1|    293|   4.0|1112484703|\n",
      "|     1|    296|   4.0|1112484767|\n",
      "|     1|    318|   4.0|1112484798|\n",
      "|     1|    337|   3.5|1094785709|\n",
      "|     1|    367|   3.5|1112485980|\n",
      "|     1|    541|   4.0|1112484603|\n",
      "|     1|    589|   3.5|1112485557|\n",
      "|     1|    593|   3.5|1112484661|\n",
      "|     1|    919|   3.5|1094785621|\n",
      "|     1|    924|   3.5|1094785598|\n",
      "+------+-------+------+----------+\n"
     ]
    }
   ],
   "source": [
    "ratings_removed_bad_ratings = remove_bad_ratings(ratings_df)\n",
    "ratings_removed_bad_ratings.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:58.965572Z",
     "start_time": "2024-02-13T22:27:58.763379Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMKLPBXaI0TA"
   },
   "source": [
    "### Question A4\n",
    "\n",
    ">Create a function named *sample_users* that takes a rating dataframe as an argument, and returns a dataframe with only a *ratio* of users (we want to keep all ratings from users that we keep) ; default value of *ratio* parameter is *0.1*. Function should be deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:27:59.071854Z",
     "start_time": "2024-02-13T22:27:58.965063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|    223|   4.0|1112485573|\n",
      "|     1|    589|   3.5|1112485557|\n",
      "|     1|    653|   3.0|1094785691|\n",
      "|     1|   1291|   3.5|1112485525|\n",
      "|     1|   1374|   4.0|1094785746|\n",
      "|     1|   1848|   3.5|1112486032|\n",
      "|     1|   1994|   3.5|1094786087|\n",
      "|     1|   2140|   4.0|1112485705|\n",
      "|     1|   2288|   4.0|1094786077|\n",
      "|     1|   2716|   3.5|1094786012|\n",
      "|     1|   2762|   4.0|1112485367|\n",
      "|     1|   3265|   3.5|1112484525|\n",
      "|     1|   3476|   3.5|1094786139|\n",
      "|     1|   3479|   4.0|1112485734|\n",
      "|     1|   3889|   4.0|1112486138|\n",
      "|     1|   4571|   4.0|1112485880|\n",
      "|     1|   4915|   3.0|1112486076|\n",
      "|     1|   5679|   3.5|1094786108|\n",
      "|     1|   5898|   3.5|1112486002|\n",
      "|     2|      3|   4.0| 974820889|\n",
      "+------+-------+------+----------+\n"
     ]
    }
   ],
   "source": [
    "def sample_users(ratings_df, ratio=0.1):\n",
    "    return ratings_df.sample(False, ratio, seed=42)\n",
    "\n",
    "\n",
    "ratings_sampled_users = sample_users(ratings_df)\n",
    "ratings_sampled_users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJV6UbAiLGiv"
   },
   "source": [
    "### Question A5\n",
    "\n",
    "> Create a function named *remove_exotic_movies*, taking a rating dataframe as argument, and that removes all movies which have less than *nb_min_ratings* ; *nb_min_ratings* parameter has a default value of *1000*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:28:02.442536Z",
     "start_time": "2024-02-13T22:27:59.071811Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==>               (1 + 7) / 8][Stage 13:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+----------+\n",
      "|movieId|count|userId|rating| timestamp|\n",
      "+-------+-----+------+------+----------+\n",
      "|   3997| 2047|     1|   3.5|1112486192|\n",
      "|   1580|35580|     2|   4.0| 974820748|\n",
      "|   3918| 1246|     2|   3.0| 974820943|\n",
      "|   2366| 6627|     3|   4.0| 944918310|\n",
      "|   1580|35580|     7|   3.0|1011206832|\n",
      "|   3175|13945|     7|   2.0|1011206805|\n",
      "|   4519| 1936|     9|   2.0| 994019800|\n",
      "|   1580|35580|    11|   5.0|1230788734|\n",
      "|   1591| 5255|    11|   5.0|1230782724|\n",
      "|    471|11268|    14|   5.0|1225308771|\n",
      "|   1580|35580|    14|   3.5|1225319912|\n",
      "|   3175|13945|    14|   4.5|1225320332|\n",
      "|  36525| 1169|    14|   4.5|1225311849|\n",
      "|  44022| 2465|    14|   4.0|1225310443|\n",
      "|   1580|35580|    16|   4.0| 990970213|\n",
      "|   1580|35580|    17|   4.0| 979686312|\n",
      "|   1580|35580|    18|   2.5|1236356556|\n",
      "|   2866| 1407|    21|   3.0| 992188523|\n",
      "|   1580|35580|    22|   3.0| 994638608|\n",
      "|   1580|35580|    23|   5.0| 914458526|\n",
      "+-------+-----+------+------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def remove_exotic_movies(ratings_df, nb_min_ratings=1000):\n",
    "    return ratings_df.groupBy(\"movieId\").count().filter(F.col(\"count\") >= nb_min_ratings).join(ratings_df, \"movieId\",\n",
    "                                                                                               \"inner\")\n",
    "\n",
    "\n",
    "ratings_removed_exotic_movies = remove_exotic_movies(ratings_df)\n",
    "ratings_removed_exotic_movies.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTKUFM95pAwP"
   },
   "source": [
    "### Question A6\n",
    "\n",
    "> Compute the following stats on the dataset:\n",
    "> - Amount of distinct users\n",
    "> - Amount of distinct movies\n",
    "> - Total amount of ratings\n",
    "> - Let r_u be the amount of ratings made by user u. Study the distribution of r_u over all users (quantiles, histogram...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:28:05.380237Z",
     "start_time": "2024-02-13T22:28:02.445291Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+-------------+\n",
      "|distinct_users|distinct_movies|total_ratings|\n",
      "+--------------+---------------+-------------+\n",
      "|        138493|          26744|     20000263|\n",
      "+--------------+---------------+-------------+\n",
      "+----------+----------+-----------------+------------------+\n",
      "|min(count)|max(count)|       avg(count)|     stddev(count)|\n",
      "+----------+----------+-----------------+------------------+\n",
      "|        20|      9254|144.4135299257002|230.26725699673392|\n",
      "+----------+----------+-----------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "ratings_df.agg(F.countDistinct(\"userId\").alias(\"distinct_users\"),\n",
    "               F.countDistinct(\"movieId\").alias(\"distinct_movies\"),\n",
    "               F.count(\"*\").alias(\"total_ratings\")).show()\n",
    "\n",
    "ratings_df.groupBy(\"userId\").count().agg(F.min(\"count\"), F.max(\"count\"), F.avg(\"count\"), F.stddev(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[<Axes: title={'center': 'count'}>]], dtype=object)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 600x600 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAIbCAYAAACDoBYRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApzklEQVR4nO3dsW/baJ7G8cfZNQwEGIfr6RZrYI7CNcFVSnz/wIrZYlrZBq63XahLIY6LwU2wRUbqVUjuD3DEdga4iPMXyFa1SCfeAHOYLjJXCwQQBEdXzFFrWZL9yqJJ0f5+gGBXFPPy9W9s68nL9325NhqNRgIAADDwJO0OAACA7CA4AAAAYwQHAABgjOAAAACMERwAAIAxggMAADBGcAAAAMYIDgAAwBjBAQAAGCM4AAAAYwQHAJnW6XTS7gLwqBAcAGRavV5PuwvAo0JwAJBpQRCk3QXgUSE4AMgsz/PS7gLw6BAcAGRSEAQ6ODhIuxvAo7M2Go1GaXcCQPo8z1O73daXX36pjx8/KpfL6fDwcOZ5QRDIsiyFYShJKpfL4/fDMNSf//xnhWEoy7J0fn4+fq9araper6vX66nZbKpQKEj6LQTs7u4qCAK9fPlSrVZLjUZj3F673ValUpFt25KkRqOhVqulTqejXq83bkfSxHkA4kdwACDXdRUEgZrN5vhYGIZ6+/atKpXK+Nju7q4cx5kIFNGHfrPZnPjAjoLA1eAQnZ/L5dRqtSY+8KO/E4ahdnd3tbe3J8uyJP0WVg4ODnRxcTFx/tHRkYIgUKvVWroGAMxwqwJ45HzfV7Va1cnJycTxIAjGowvSb//KD4JgahTCtm0dHR3p6Oho4vjOzs7M6900GmDbts7OzvTy5ctxaJCkQqGgMAxZegmsAIID8Mi5rqtisTjxQS1JvV5PvV5v4rz9/f2Zbezt7cn3ffm+v1RfvvzyS4VhqHw+P3E86tvV/gBIB8EBeOQ6nc7MUYBCoaCLiwvZtq0gCGZ+oEeiD/Y4RgSYnwCsNoID8IhFtyG+/PJLo/NuYlmW2u320n26PvIBYLUQHIBHLPrX/cePH43Oi1ZRzBKG4UqMFrC3A3C/CA7AIxfdipjnaiCYd150fN6EyOvtxen6vAd2kgTuF8EBeOQqlYo8z5v5gR4EwXjCY6VSmftcCM/zlM/nVSwWx8eu7vNw1bITKK/K5XKxBxEANyM4AI9csVjU4eGhdnd3p97zPG8cBsrlsvL5vFzXnTin0+moXq9P7AEh/bbSIppUGQnDcDxCMGtk4OPHj7feDrmqUChMXKPT6cydwAkgHmwABUDSbyGh1Wopl8vJtm31er2ZO0c2Gg11u93xhMqPHz/q+Ph45qRG3/dVr9e1s7Mzfv/w8FBra2uyLEuFQkHNZlNhGOrg4EC+7ysMQxWLRe3v76tYLKpararVasn3fdm2rXw+PxFSon6/ePFi3D6A+0NwAAAAxrhVAQAAjBEcAACAMYIDAAAwRnAAAADGCA4AAMAYwQEAABj7fdodiMvnz5/166+/6osvvtDa2lra3QEAIDNGo5H+8Y9/6I9//KOePLl5TOHBBIdff/1V29vbaXcDAIDM+uWXX/SnP/3pxnMeTHD44osvJP32RW9ubi7d3nA41Pv37/Xq1Sutr68v3R5uRr2TQ62TRb2TRb3vpt/va3t7e/xZepMHExyi2xObm5uxBYenT59qc3OTb74EUO/kUOtkUe9kUe/lmNzqZ3IkAAAwRnAAAADGCA4AAMBYqnMcOp2OXNeV4ziybVtBEKjb7aper6fZLQAAMEfqkyODIJDrurJtW8VikdAAAMAKSz04NJtN5fP5tLsBAAAMMMcBAAAYW3jEIQxDua4ry7JUqVTmnuf7vlqtlnK5nMIwlCSVy+WZ552dnWlra0vtdlvHx8eyLGvRbgEAgAQYBwfXdRUEgXZ2duT7vgqFwtxzPc/T6empms3m+Jjv+3IcR61Wa+LcaG6DJOXzeb148ULdbnfRrwMAACTA+FZFpVJRs9lUuVy+cUQgDEMdHBzo5ORk4nihUFCv11Oj0Rgfy+fz49Ag/RYier2ePM9b4EsAAABJiX2Ow7t372Tb9sxwsb+/f+uqCdu2p0YlAADAaog9ODSbTW1tbc18z7ZtdTqd8ZyHP/zhD+p0OnF3AQAA3JPYg8PZ2Zls2575XnQ8CAJJ0suXL6fODYJAL168iLtbAAAgBrEHhzAMb10VEQWH6/s3eJ4n27Z1eHgYd7cAAEAMEt0AKgoUvV5P0m8TLqvVqiTp48ePCsNQ5+fnRm0NBgMNBoPx636/L+m3R6oOh8Ol+xq1EUdbuB31Tg61Thb1Thb1vptF6pX6zpGz9nYw8fbtW71582bq+Pv37/X06dNluzXGRM1kUe/kUOtkUe9kUe/FfPr0yfjcRINDNCly3uTJRRwfH+v169fj1/1+X9vb23r16pU2NzeXbn84HKrVaslxHK2vry/dHm5GvZNDrZNFvZNFve8mGrU3kfqIw11tbGxoY2Nj6vj6+nqs3yxxt4ebUe/kUOtkUe9kUe/FLFKr2IND9HjsWaK5DfNWXayif/vuvzW4XBu//vn7r1PsDQAA6Yp9VUU+nx/fkrhu3mqKZdRqNT1//lw7OzuxtQkAAGaLPTg4jjN3xKHb7d74jIu7KJVK+vDhg9rtdqztAgCAabEHh729PfV6vZnhwfM8HR0dxX1JAACQkDsFhzAMx/MVrrMsSycnJ3Jdd+J4tLnT1YdaAQCAbDGeHFmtVtVutxUEwfiP4ziyLEv7+/sTgaBYLMqyLLmuq1wuN57zcB/ramu1mmq1mi4vL2NvGwAATDIODotu1FQoFGKfzzBLqVRSqVRSv9/Xs2fP7v16AAA8ZrHPcQAAAA8XwQEAABgjOAAAAGOZDw5sAAUAQHIyHxzYAAoAgORkPjgAAIDkEBwAAIAxggMAADBGcAAAAMYyHxxYVQEAQHIyHxxYVQEAQHIyHxwAAEByCA4AAMAYwQEAABgjOAAAAGMEBwAAYCzzwYHlmAAAJCfzwYHlmAAAJCfzwQEAACSH4AAAAIwRHAAAgDGCAwAAMEZwAAAAxggOAADAWOaDA/s4AACQnMwHB/ZxAAAgOZkPDgAAIDkEBwAAYIzgAAAAjBEcAACAMYIDAAAwRnAAAADGCA4AAMAYwQEAABjLfHBg50gAAJKT+eDAzpEAACQn88EBAAAkh+AAAACMERwAAIAxggMAADBGcAAAAMYIDgAAwBjBAQAAGCM4AAAAYwQHAABgjOAAAACMERwAAIAxggMAADD2+7Q7kDVfffPD1LGfv/86hZ4AAJC8zI848FhtAACSk/ngwGO1AQBITuaDAwAASA7BAQAAGCM4AAAAYwQHAABgjOAAAACMERwAAIAxggMAADBGcAAAAMYIDgAAwBjBAQAAGCM4AAAAYwQHAABgbKWCQ7Vale/7aXcDAADMsTLBIQgCua6bdjcAAMANViY4+L4v27bT7gYAALjBSgQHz/O0t7eXdjcAAMAtfr/oXwjDUK7ryrIsVSqVuef5vq9Wq6VcLqcwDCVJ5XJ5ZnuSZFnWol0BAAAJMw4OrusqCALt7OzI930VCoW553qep9PTUzWbzfEx3/flOI5ardbEue/evdPh4eEdug4AAJJmfKuiUqmo2WyqXC7fODoQhqEODg50cnIycbxQKKjX66nRaIyPdTqdGwMIAABYLbHPcXj37p1s254ZLvb391Wv18evz87OmBAJAECGLDzH4TbNZlNbW1sz37NtW51OR2EYqtFo6OPHj6pWq+P3e72e6vW6Op3OzPkQAAAgXbEHh7Ozs7krJKLRhSAIZgYD13V1dHTE7QsAAFZU7LcqwjC8dYVEEARxXxYAACQg9hGHm0SBotfrTRz3fX+8AqNSqRjdqhgMBhoMBuPX/X5fkjQcDjUcDpfua9TGxpOR8bm4u6iG1PL+UetkUe9kUe+7WaReiQaHeQqFggqFwsTEydu8fftWb968mTr+/v17PX36NLa+/fXl51vP+fHHH2O73mN3fbku7g+1Thb1Thb1XsynT5+Mz000OESbPc2bPLmI4+NjvX79evy63+9re3tbr1690ubm5tLtD4dDtVotfXv2RIPPazee+7fv/rL09R67qN6O42h9fT3t7jxo1DpZ1DtZ1PtuolF7Eysx4nAXGxsb2tjYmDq+vr4e6zfL4POaBpc3B4d//fb9xOufv/86tus/NnH/98N81DpZ1DtZ1Hsxi9Qq9smRtm3PnfwYzW1g7wYAALIp9uCQz+fHtySuiwJFPp+P7Xq1Wk3Pnz/Xzs5ObG0CAIDZYg8OjuPMHXHodrux79FQKpX04cMHtdvtWNsFAADTYg8Oe3t76vV6M8OD53k6OjqK+5IAACAhdwoOYRhO7cUQsSxLJycncl134rjnebJtW8Vi8S6XBAAAK8B4VUW1WlW73VYQBOM/juPIsizt7+9PBIJisSjLsuS6rnK53HjOw32sq63VaqrVarq8vIy9bQAAMMk4OCz60KloU6f7ViqVVCqV1O/39ezZs3u/HgAAj1nscxwAAMDDRXAAAADGCA4AAMBY5oMDG0ABAJCczAcHNoACACA5mQ8OAAAgOQQHAABgjOAAAACMZT44MDkSAIDkZD44MDkSAIDkZD44AACA5BAcAACAMYIDAAAwZvx0TJj76psfJl7//P3XKfUEAIB4MeIAAACMZT44sBwTAIDkZD44sBwTAIDkZD44AACA5BAcAACAMYIDAAAwRnAAAADGCA4AAMAYwQEAABjLfHBgHwcAAJKT+eDAPg4AACQn88EBAAAkh+AAAACMERwAAIAxggMAADBGcAAAAMYIDgAAwBjBAQAAGPt92h14DL765oeJ1z9//3VKPQEAYDmMOAAAAGOZDw5sOQ0AQHIyHxzYchoAgORkPjgAAIDkEBwAAIAxggMAADBGcAAAAMYIDgAAwBjBAQAAGCM4AAAAYwQHAABgjOAAAACMERwAAIAxno6ZgutPy5R4YiYAIBsYcQAAAMYIDgAAwFjmgwOP1QYAIDmZDw48VhsAgORkPjgAAIDkEBwAAIAxggMAADBGcAAAAMYIDgAAwBjBAQAAGCM4AAAAYwQHAABgjOAAAACMERwAAIAxHqu9Iq4/apvHbAMAVlHqwaHRaEiSwjBUu93W0dGRCoVCyr0CAACzpBocjo6OtLu7Ow4Kvu/LcRxdXFzIsqw0uwYAAGZIdY5Dr9dTs9kcv7ZtW5J0dnaWVpcAAMANUh1xuBoaJKnT6UiSXr58mUZ3AADALRYODmEYynVdWZalSqUy9zzf99VqtZTL5RSGoSSpXC7f2Ha9XlelUuE2BQAAK8o4OLiuqyAItLOzI9/3b5zA6HmeTk9PJ0YUovkLrVZr6vxGo6FWqyXHcW4NFwAAID3GcxwqlYqazabK5fKNIwJhGOrg4EAnJycTxwuFgnq93ngVxVWHh4c6OTlRt9uV67rmvQcAAImKfXLku3fvZNv2zHCxv7+ver0+8+9ZlqV6va5qtapqtRp3twAAQAxiDw7NZlNbW1sz37NtW51OZzzn4ejoSEEQTJ0z63YGAABIX+zB4ezsbLys8rroeBAE6nQ6ajQaU8Gh1+sxORIAgBUVe3AIw/DWD/4gCJTP51UulycmWfq+rzAMb1ytAQAA0pPoPg5RoOj1epKk4+PjifkM7XZb5+fnc0csrhoMBhoMBuPX/X5fkjQcDjUcDpfua9TGxpPR0m0tc/3HIvp6H9vXnQZqnSzqnSzqfTeL1CvVDaAsy7rz8su3b9/qzZs3U8ffv3+vp0+fLtu1sb++/BxbW4v48ccfU7lu2pjfkhxqnSzqnSzqvZhPnz4Zn5tocIgmRc6bPLmI4+NjvX79evy63+9re3tbr1690ubm5tLtD4dDtVotfXv2RIPPa0u3t6i/ffeXxK+ZpqjejuNofX097e48aNQ6WdQ7WdT7bqJRexOpPx3zrjY2NrSxsTF1fH19PdZvlsHnNQ0ukw8O//rt+4nXj+Ux23H/98N81DpZ1DtZ1Hsxi9Qq9smRtm1PrZSIRHMbTOYwAACA1RN7cMjn8+NbEtdFgSKfz8d2vVqtpufPn2tnZye2NgEAwGyxBwfHceaOOHS73RufcXEXpVJJHz58ULvdjrVdAAAwLfbgsLe3p16vNzM8eJ6no6OjuC8JAAAScqfgEIbheL7CdZZl6eTkZOphVZ7nybZtFYvFu1wSAACsAONVFdVqVe12W0EQjP84jiPLsrS/vz8RCIrFoizLkuu6yuVy4zkP97GutlarqVar6fLyMva2AQDAJOPgsOhGTYVCIfb5DLOUSiWVSiX1+309e/bs3q8HAMBjltl9HB6br775YeL1Y9nXAQCwWmKfHAkAAB4uggMAADCW+eDABlAAACQn88GBDaAAAEhO5oMDAABIDsEBAAAYIzgAAABjBAcAAGAs88GBVRUAACQn88GBVRUAACQn88EBAAAkh+AAAACMERwAAIAxno6ZUdeflinxxEwAwP1jxAEAABjLfHBgOSYAAMnJfHBgOSYAAMnJfHAAAADJITgAAABjBAcAAGCM4AAAAIwRHAAAgDGCAwAAMJb5nSNrtZpqtZouLy/T7krqru8myU6SAIC4ZX7EgX0cAABITuZHHDAfIxAAgLhlfsQBAAAkh+AAAACMERwAAIAxggMAADBGcAAAAMYIDgAAwBjBAQAAGCM4AAAAY5kPDrVaTc+fP9fOzk7aXQEA4MHLfHBgy2kAAJKT+eAAAACSQ3AAAADGCA4AAMAYwQEAABjjsdqPCI/ZBgAsixEHAABgjOAAAACMERwAAIAxggMAADBGcAAAAMYIDgAAwBjLMR8xlmcCABaV+REHno4JAEByMh8ceDomAADJyXxwAAAAySE4AAAAYwQHAABgjOAAAACMERwAAIAxggMAADDGBlAYu74hlMSmUACASYw4AAAAYwQHAABgjOAAAACMERwAAIAxggMAADCW+qqKarUqSep2uwqCQPV6XbZtp9wrAAAwS6rBwXVdHR8fy7IsSZLnecrlcup2u4SHFXF9iSbLMwHgcUv1VoXv++r1euPXxWJRlmWpXq+n2CsAADBPasEhDEMFQaAgCCaOb21tKQzDdDoFAAButPCtijAM5bquLMtSpVKZe57v+2q1WsrlcuMgUC6Xx+9blqWLi4upvxcEgV68eLFotwAAQAKMg4PrugqCQDs7O/J9X4VCYe65nufp9PRUzWZzfMz3fTmOo1arNffvNRoN2batw8ND024BAIAEGQeHq6MLp6enc88Lw1AHBwf6n//5n4njhUJBruuq0WjMDAbRSMb5+blplwAAQMJin+Pw7t072bY9Xilx1f7+/tyJjwcHB/rpp59YTQEAwAqLPTg0m01tbW3NfM+2bXU6nanJj9GyzHw+H3d3AABAjGIPDmdnZ3NHDaLjV1dSeJ4nx3EmQkOj0Yi7WwAAIAaxB4cwDGfeprgqCg6+76vdbmtra0udTkedTkee58XdJQAAEJNEd46MAkWv11MYhtrd3VUYhuNtpyM3rbyIDAYDDQaD8et+vy9JGg6HGg6HS/c1amPjyWjpth6SOGp7U7v31T7+iVoni3oni3rfzSL1Sm3L6Xn7OJh6+/at3rx5M3X8/fv3evr06TJdm/DXl59ja+sh+PHHH++1fZPQiHhQ62RR72RR78V8+vTJ+NxEg0M0KXLe5MlFHB8f6/Xr1+PX/X5f29vbevXqlTY3N5dufzgcqtVq6duzJxp8Xlu6vYfqb9/9JZZ2ono7jqP19fVY2sRs1DpZ1DtZ1PtuolF7E6k/HfOuNjY2tLGxMXV8fX091m+Wwec1DS4JDvPE/YMZ938/zEetk0W9k0W9F7NIrWKfHGnb9tTzJyLRA63YqwEAgGyKPTjk8/m5D6mKAkWc+zXUajU9f/5cOzs7sbUJAABmiz04OI4zd8Sh2+3e+IyLuyiVSvrw4YPa7Xas7QIAgGmxB4e9vT31er2Z4cHzPB0dHcV9SQAAkJA7BYcwDMfzFa6zLEsnJydyXXfiuOd5sm1bxWLxLpcEAAArwHhVRbVaVbvdVhAE4z+O48iyLO3v708EgmKxKMuy5LqucrnceM7DfayrrdVqqtVqury8jL1tAAAwyTg4lMvlhRouFAqxz2eYpVQqqVQqqd/v69mzZ/d+PQAAHrPY5zgAAICHi+AAAACMERwAAICxzAcHNoACACA5mX1WRYTJkavlq29+mDr28/dfp9ATAMB9yHxwQLpmBQUAwMOV+VsVAAAgOQQHAABgjOAAAACMZT44sKoCAIDkZD448FhtAACSk/ngAAAAkkNwAAAAxggOAADAGMEBAAAYy3xwYFUFAADJyXxwYFUFAADJyXxwAAAAySE4AAAAYwQHAABgjOAAAACMERwAAIAxggMAADBGcAAAAMZ+n3YHllWr1VSr1XR5eZl2VzDHV9/8cOP7P3//dUI9AQAsK/PBoVQqqVQqqd/v69mzZ2l3B/dgVvAgbABAOrhVAQAAjBEcAACAMYIDAAAwRnAAAADGCA4AAMAYwQEAABgjOAAAAGMEB6Tuq29+0L9999+SNP5fAMBqynxwqNVqev78uXZ2dtLuCgAADx47R2Ll3LZFNQAgPZkfcQAAAMkhOAAAAGMEBwAAYCzzcxzwOF2fB8HTMgEgGYw4AAAAYwQHAABgjOAAAACMERwAAIAxggMAADBGcAAAAMYIDgAAwBjBAQAAGCM4AAAAY5kPDjxWGwCA5GQ+OJRKJX348EHtdjvtrgAA8OBlPjgAAIDkEBwAAIAxggMAADBGcAAAAMZ+n3YHgDh89c0PE69//v7rG9+fdQ4A4HaMOAAAAGMEBwAAYIzgAAAAjBEcAACAMSZHAv/vtgmWAABGHAAAwAIIDgAAwFjqwSEMQ3mep1wul3ZXAADALVKd49DpdHR2dqatrS0FQZBmVwAAgIFUg0M+n1c+nyc0IBWzdpMEANws9VsVAAAgOxYecQjDUK7ryrIsVSqVuef5vq9Wq6VcLqcwDCVJ5XL5zh0FAADpMw4OrusqCALt7OzI930VCoW553qep9PTUzWbzfEx3/flOI5ardZyPQYAAKkxvlVRqVTUbDZVLpdlWdbc88Iw1MHBgU5OTiaOFwoF9Xo9NRqNO3cWAACkK/Y5Du/evZNt2zPDxf7+vur1etyXBAAACYk9ODSbTW1tbc18z7ZtdTqd8ZwHAACQLbEHh7OzM9m2PfO96DjLLwEAyKbYg0MYhjfOgZCmgwMjEAAAZEOiG0BFgaLX60n6LUB4njdeabG7u6udnR0dHh7eGj4Gg4EGg8H4db/flyQNh0MNh8Ol+xq1sfFktHRbuF1U57jqff17YON3i7cbx/fRKoq+rof69a0a6p0s6n03i9Qr1Z0jbdtWuVy+0/4Ob9++1Zs3b6aOv3//Xk+fPo2je5Kkv778HFtbuF1c9f7xxx8nXlf/ffk2HhqWRieLeieLei/m06dPxucmGhyiWxLzJk8u4vj4WK9fvx6/7vf72t7e1qtXr7S5ubl0+8PhUK1WS9+ePdHg89rS7eFmG09G+uvLzytV779995e0u3Avou9tx3G0vr6edncePOqdLOp9N9GovYlURxyWsbGxoY2Njanj6+vrsX6zDD6vaXC5Gh9kj8Eq1fuh/9KJ+2cFN6PeyaLei1mkVrEHB9u2566aiOY2zFt1cRe1Wk21Wk2Xl5extQlI0w/B+vn7r1PqCQCsjthXVeTz+bmrJKJAkc/nY7teqVTShw8f1G63Y2sTAADMFntwcBxn7ohDt9u98RkXAABgtcUeHPb29tTr9WaGB8/zdHR0FPclAQBAQu4UHMIwHM9XuM6yLJ2cnMh13YnjnufJtm0Vi8W7XBIAAKwA48mR1WpV7XZbQRCM/ziOI8uytL+/PxEIisWiLMuS67rK5XLjOQ/3sa6WyZEAACTHODgsuklToVBIZD5DqVRSqVRSv9/Xs2fP7v16AAA8ZrHPcQAAAA8XwQEAABgjOAAAAGOZ3XI6wuRIJOX6TpLS9G6S7DYJ4KHL/IgDO0cCAJCczAcHAACQHIIDAAAwRnAAAADGMh8carWanj9/rp2dnbS7AgDAg5f54MDkSAAAkpP54AAAAJJDcAAAAMYIDgAAwBjBAQAAGCM4AAAAYzyrAkiRyfMvAGCVZH7EgeWYAAAkJ/PBAQAAJIfgAAAAjBEcAACAMYIDAAAwRnAAAADGCA4AAMAY+zgAK2bW3g5Xsc8DgDRlfsSBfRwAAEhO5oMDAABIDsEBAAAYIzgAAABjBAcAAGCM4AAAAIwRHAAAgDGCAwAAMEZwAAAAxggOAADAGFtOA0u4bXvo6+/HsV30bde8y3Xuo58AHqbMjziw5TQAAMnJfHAAAADJITgAAABjBAcAAGCM4AAAAIwRHAAAgDGCAwAAMEZwAAAAxggOAADAGMEBAAAYIzgAAABjBAcAAGCM4AAAAIwRHAAAgDEeqw0kyOSR2Pd93Y3fjVT9d/PzI1l91LbJI8N5rDhgLvMjDjxWGwCA5GQ+OAAAgOQQHAAAgDGCAwAAMEZwAAAAxggOAADAGMEBAAAYIzgAAABjBAcAAGCM4AAAAIwRHAAAgDGCAwAAMJb6Q64ajcb4/3e7XR0fH8uyrPQ6BAAA5ko1OHiep1arpWazKUkKw1B//vOfdX5+nma3AADAHKneqnj79q2Oj4/Hry3L0tbWlnzfT7FXAABgntSCQxiG6nQ6sm174rht2+MRCAAAsFoWvlURhqFc15VlWapUKnPP831frVZLuVxOYRhKksrl8vj9IAgkaWo+g2VZ6nQ6i3YLAAAkwDg4uK6rIAi0s7Mj3/dVKBTmnut5nk5PTydGDnzfl+M4arVakqRerzf379/0HgAASI/xrYpKpaJms6lyuXzjqocwDHVwcKCTk5OJ44VCQb1eb2IVRXQ+AADIhtjnOLx79062bc8MF/v7+6rX65Kkra2tmX8/DMO57wEAgHTFHhyazebcD37bttXpdBSG4XhS5PXbEr1eT/l8Pu5uAQCAGMQeHM7OzqZWSkSi40EQyLIs5fP58STJSBAEchwn7m4BAIAYxB4cwjC8defHKCwcHx9PTKCM5jvcNPESAACkJ9GdI6NAEd2eKBaL4wmTW1tbarfb+umnn4zaGgwGGgwG49f9fl+SNBwONRwOl+5r1MbGk9HSbeF2UZ2pdzyu/wxs/O6fdY1qfPWcq++btpkV17+2WV+HyTl3FbWV1fplDfW+m0XqtTYajRb+Tf3ixQu9fPlyPNFxosG1NZXL5Zl7PARBoFwup3q9rsPDw0UvO+G7777Tmzdvpo7/13/9l54+fbpU2wAAPCafPn3Sf/zHf+jvf/+7Njc3bzw30RGH6FZEHKsmjo+P9fr16/Hrfr+v7e1tvXr16tYv2sRwOFSr1dK3Z080+Ly2dHu42caTkf768jP1TkBUa8dxtL6+Lkn6t+/+e+F2/vbdXyZeX2/j+vsm17mtzbtc9y5f223XXET0u+RqvWH2/XIXD7Xe91WvSDRqbyL1p2Pe1cbGhjY2NqaOr6+vx/rNMvi8psElH2RJod7JufqzcpeaX/85u97GrJ/D265zW5t3uW4c309x/E6J+3dT1pl8vyzjodU7iXqZin1ypG3bUyslItHchnmrLgAAwGqLPTjk8/m5u0FGgSLOfRpqtZqeP3+unZ2d2NoEAACzxR4cHMeZO+LQ7XZjX2pZKpX04cMHtdvtWNsFAADTYg8Oe3t76vV6M8OD53k6OjqK+5IAACAhdwoOYRjOfYKlZVk6OTmR67oTxz3Pk23bKhaLd7kkAABYAcarKqrVqtrttoIgGP9xHEeWZWl/f38iEBSLRVmWJdd1lcvlxnMeokdqx6lWq6lWq+ny8jL2tgEAwCTj4FAulxdquFAoJLJ1dKlUUqlUUr/f17Nnz+79egAAPGaxz3EAAAAPF8EBAAAYIzgAAABjmQ8ObAAFAEByMh8c2AAKAIDkZD44AACA5BAcAACAMYIDAAAwRnAAAADGMh8cWFUBAEByjLecXlXRltN///vfZVmW+v1+LO0Oh0N9+vRJl4Pf6fPlWixtYr7L34306dMl9U5AVOt+v6/19XVJ0ufBp4Xbuf6zdr2NWT+Lt13ntjbvct27fG23XXMR0e+Sq/WG2ffLXTzUet9Xva63NxqNbj13bWRyVgb87//+r7a3t9PuBgAAmfXLL7/oT3/6043nPJjg8PnzZ/3666/64osvtLa2/L9Y+/2+tre39csvv2hzczOGHuIm1Ds51DpZ1DtZ1PtuRqOR/vGPf+iPf/yjnjy5eRZD5m9VRJ48eXJrSrqLzc1NvvkSRL2TQ62TRb2TRb0XZ/qE6cxPjgQAAMkhOAAAAGMEhzk2Njb0n//5n9rY2Ei7K48C9U4OtU4W9U4W9b5/D2ZyJAAAuH+MOAAAAGMEBwAAYIzgAAAAjBEcAACAsQezAVScfN9Xq9VSLpdTGIaSpHK5nG6nVkyj0VC321Wn01Gv11OhUFClUpl57iL1vK9zH6Jqtap8Pq9CoTD1HjVfXhAEcl1XkrS1tSXLsmZ+j1Pr5Xmep1arNXGsUqnIsqypc6n3ChhhQrPZHBWLxYljrVZrVCgUUurR6imXy6Nutzt+fXFxMSoUCiPLskYXFxcT5y5Sz/s69yG6uLgYSRo1m82p96j58prN5iifz099n5fL5anzqPVyyuXyqNVqTRzrdrujfD7P75MVRXC44uLiYuaH32g0GuXz+VG9Xk++Uyum2WyOzs/Pp45HH2RXf9AWqed9nftQVSqVmcGBmi/v/Px85tdaLBZHtm2PX1Pr5Z2fn0+FsavvXf0wp96rgzkOV7x79062bc8cHtvf31e9Xk++Uyum3W4rn89PHbcsS4eHh/J9fzzMt0g97+vch8j3/Zm3JyRqHgfXdXV8fDz1tTqOo6Ojo/Frar083/e1s7Mz8718Pq9OpzN+Tb1XB8Hhimazqa2trZnv2batTqcz/lB8rBqNhhzHmfneixcvJElnZ2eSFqvnfZ37EHU6nZnhTaLmy+p0OvJ9X4eHh1PvHR4eTtzzptbxmPfBHASBbNsev6beq4PgcMXZ2dnEN+pV0fEgCJLs0sp5+fLl3PeiH67oh3CRet7XuQ9No9G4ccIWNV9OvV6f+6/P66j18orFonzf1+7u7tSHc6VSGU9Olaj3KiE4XBGG4a2/MB77N1Cr1Zqa/RzpdruSNP7X8CL1vK9zH5IgCOb+yyhCzZfj+/74w6JarapararRaMh13akPNmq9PNu2ValU5Hme/uVf/kW+70v658jm1Vty1Ht1sBzTUPSN1ev10u3ICms0GjOHeGdZpJ73dW7WeJ631PIwan67IAiUz+dVrVYnah0EgV68eKHz83Oj0Qhqba5cLsu2be3u7spxHNm2rVarNXcUYBbqnSxGHBAL13XH/3pA/DzPU7FYTLsbj0Kn05mqtW3bKhQKOjg4SKlXD5tlWSqXyyoUCgqCQI7jTEyMxGohOBi6fv8e/9TpdNRoNNRqtYz+NSYtVs/7OjcrwjBUr9db6F9g89qRqLmJWbV+8eKFPM8zmihHrc1Ft4EqlYparZbq9fp4hCe6dXEb6p0sggOWtru7q59++mnpDzbMtsgtICzvtoly0aohLK/RaEjSxAjP4eGhut3u+PYFKxpWD8HhCtu2506Cie5x8eE4yXEc1ev1mcsDF6nnfZ2bdTctvZyFmi/HZEVFVAdqvbxKpTLz9qZt2zo/P5ek8agD9V4dTI68Ip/Pz0230TfWIr/EH7qjoyO5rjt3M6JF6nlf52ZdEAQ6PT2dWuseff1v377V6emptra2xgGOmt9dPp+/dQZ9tCSZWi/nttUMlmXp+Ph44mum3quBEYcrHMeZ+0uj2+3O/YB8jKrVqnZ3d6dqEgTB+F8Ii9Tzvs7NumKxqGazOfOPJB0fH6vZbI6DBTVfzv7+/txJeVdHGiRqvSzLsoxuQ1DvFZT2nterJNqz/OqDbSK2bc98oNBj1Gw2px5Kc/W9qH6L1PO+zn2o5j3kipovz7KsmV9PoVAYHR4ejl9T6+UVCoW5v0ui96NnSFDv1UFwuGbWU9KazSZPSft/5+fno0KhMKrX6xN/KpXKqFKpjPL5/MT5i9Tzvs59iM7Pz0eSZj6Ah5ovp9VqjWzbnnjoUb1enzo2GlHrZV1cXIzy+fxUeLi4uBgdHh5OHafeq2FtNBqN0h71WDU8l32+P/zhDzcOL9q2Pd5BMrJIPe/r3Iei0+no7du3CoJAnU5HlmWpUCjIcZyJlRfUfDm+76ter2tra2u8FHbeHiXUenmzduasVCoz50BQ7/QRHAAAgDEmRwIAAGMEBwAAYIzgAAAAjBEcAACAMYIDAAAwRnAAAADGCA4AAMAYwQEAABgjOAAAAGMEBwAAYIzgAAAAjBEcAACAMYIDAAAw9n8SmJKmSsMUAwAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratings_df.groupBy(\"userId\").count().toPandas().hist(\"count\", bins=100, figsize=(6, 6), log=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T22:28:06.958662Z",
     "start_time": "2024-02-13T22:28:05.379861Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFjZb6tFmEPt"
   },
   "source": [
    "### Question A7\n",
    "\n",
    "> Create a function named *remove_old_movies_in_timelines*, that takes a ratings dataframe as parameter, and only keeps the *nb_max_movies* most recent movies seen by each user ; *nb_max_movies* parameter is defaulted at 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:28:12.583311Z",
     "start_time": "2024-02-13T22:28:06.958021Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|    12|    231|   3.0|859064063|\n",
      "|    12|    110|   4.0|859064062|\n",
      "|    12|    364|   4.0|859064062|\n",
      "|    12|    480|   3.0|859064062|\n",
      "|    12|    527|   4.0|859064062|\n",
      "|    12|    585|   3.0|859064062|\n",
      "|    12|    595|   4.0|859064062|\n",
      "|    12|    356|   4.0|859064001|\n",
      "|    12|    208|   3.0|859064000|\n",
      "|    12|    590|   3.0|859063999|\n",
      "|    12|    589|   4.0|859063998|\n",
      "|    12|     34|   4.0|859063997|\n",
      "|    12|    344|   4.0|859063995|\n",
      "|    12|    380|   4.0|859063994|\n",
      "|    12|    104|   4.0|859063825|\n",
      "|    12|    260|   4.0|859063825|\n",
      "|    12|    376|   3.0|859063825|\n",
      "|    12|    653|   4.0|859063825|\n",
      "|    12|    784|   4.0|859063825|\n",
      "|    12|    788|   4.0|859063825|\n",
      "+------+-------+------+---------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def remove_old_movies_in_timelines(ratings_df, nb_max_movies=100):\n",
    "    window = Window.partitionBy(\"userId\").orderBy(F.desc(\"timestamp\"))\n",
    "    return ratings_df.withColumn(\"rank\", F.rank().over(window)).filter(F.col(\"rank\") <= nb_max_movies).drop(\"rank\")\n",
    "\n",
    "\n",
    "ratings_removed_old_movies_in_timelines = remove_old_movies_in_timelines(ratings_df)\n",
    "ratings_removed_old_movies_in_timelines.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTveqlrl_T8u"
   },
   "source": [
    "# Part B - Association Rules (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fukr5HVii2BG"
   },
   "source": [
    "## Naive: Recurring pairs\n",
    "\n",
    "> This approach is simple and not efficient but gives you a baseline and intuition for the next steps.\n",
    ">\n",
    "> Morally, what we want to do is:\n",
    "> - for each user, regroup all the movies they have liked inside a single row. We will call this the 'user timeline'\n",
    "> - for each user, generate all pairs of movies across their list of movies.\n",
    "> - for each pair of movies, count the amount of distinct users with this pair.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKvQ69TpwF_N"
   },
   "source": [
    "### Question B1\n",
    "\n",
    "> Create a function named *compute_timeline*, that takes a ratings dataframe as parameter, and returns the 'user timeline', a dataframe following this schema:\n",
    "> - userId : integer\n",
    "> - movies : list[integer] (list of movieId seen by user)\n",
    ">\n",
    "> Test it on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:28:15.490253Z",
     "start_time": "2024-02-13T22:28:12.579092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|              movies|\n",
      "+------+--------------------+\n",
      "|    12|[1, 3, 5, 6, 7, 1...|\n",
      "|    26|[10, 19, 21, 22, ...|\n",
      "|    27|[11, 29, 47, 104,...|\n",
      "|    28|[150, 161, 165, 1...|\n",
      "|    31|[1, 110, 260, 364...|\n",
      "|    34|[1, 2, 7, 10, 15,...|\n",
      "|    44|[260, 296, 349, 5...|\n",
      "|    53|[1, 21, 29, 32, 4...|\n",
      "|    65|[24, 318, 356, 36...|\n",
      "|    76|[47, 110, 316, 48...|\n",
      "|    78|[29, 110, 170, 19...|\n",
      "|    81|[356, 480, 593, 7...|\n",
      "|    85|[17, 262, 596, 89...|\n",
      "|   101|[6, 10, 11, 19, 2...|\n",
      "|   103|[7, 58, 186, 252,...|\n",
      "|   108|[14, 16, 23, 32, ...|\n",
      "|   115|[1, 22, 47, 88, 1...|\n",
      "|   126|[1, 3, 5, 6, 14, ...|\n",
      "|   133|[1, 6, 16, 32, 39...|\n",
      "|   137|[1, 2, 6, 10, 15,...|\n",
      "+------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def compute_timeline(ratings_df):\n",
    "    return ratings_df.groupBy(\"userId\").agg(F.collect_list(\"movieId\").alias(\"movies\"))\n",
    "\n",
    "\n",
    "compute_timeline(ratings_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGO98KnLxKpe"
   },
   "source": [
    "### Question B2\n",
    "\n",
    "> Let's imagine that all of our executors have 4GB of memory. If we consider the 'user timeline' dataset where movie ratings are greater or equal than 3.5, is it okay to store list of movie ids inside rows, as far as memory is concerned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:28:17.177876Z",
     "start_time": "2024-02-13T22:28:15.491243Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|              movies|\n",
      "+------+--------------------+\n",
      "|    12|[1, 34, 36, 104, ...|\n",
      "|    26|[21, 39, 47, 50, ...|\n",
      "|    27|[11, 47, 104, 162...|\n",
      "|    28|[213, 296, 356, 3...|\n",
      "|    31|[110, 260, 616, 1...|\n",
      "|    34|[1, 10, 17, 21, 3...|\n",
      "|    44|[260, 296, 349, 5...|\n",
      "|    53|[1, 21, 29, 32, 4...|\n",
      "|    65|[24, 318, 356, 36...|\n",
      "|    76|[47, 110, 480, 54...|\n",
      "|    78|[29, 170, 198, 26...|\n",
      "|    81|[356, 480, 593, 7...|\n",
      "|    85|[17, 262, 596, 89...|\n",
      "|   101|[6, 10, 11, 21, 2...|\n",
      "|   103|[7, 252, 852, 902...|\n",
      "|   108|[14, 32, 39, 111,...|\n",
      "|   115|[47, 88, 296, 318...|\n",
      "|   126|[1, 3, 5, 14, 62,...|\n",
      "|   133|[1, 6, 16, 32, 47...|\n",
      "|   137|[1, 6, 15, 70, 11...|\n",
      "+------+--------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "compute_timeline(remove_bad_ratings(ratings_df)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rSlT9Rd3w23"
   },
   "source": [
    "### Question B3\n",
    "\n",
    "> Create a function named *compute_pairs*, that takes a user timeline dataframe as parameter, and returns a dataframe of movie pairs (generated across all movies of their timeline) following this schema:\n",
    "> - userId : integer\n",
    "> - movieId1 : integer\n",
    "> - movieId2 : integer\n",
    ">\n",
    "> You can rely on an udf to generate list of pair of movies from a list of movies.\n",
    "> \n",
    "> Test it on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:28:23.269651Z",
     "start_time": "2024-02-13T22:28:17.176367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+\n",
      "|userId|movieId1|movieId2|\n",
      "+------+--------+--------+\n",
      "|    12|       1|      34|\n",
      "|    12|       1|      36|\n",
      "|    12|       1|     104|\n",
      "|    12|       1|     110|\n",
      "|    12|       1|     260|\n",
      "|    12|       1|     344|\n",
      "|    12|       1|     356|\n",
      "|    12|       1|     364|\n",
      "|    12|       1|     380|\n",
      "|    12|       1|     527|\n",
      "|    12|       1|     589|\n",
      "|    12|       1|     595|\n",
      "|    12|       1|     608|\n",
      "|    12|       1|     653|\n",
      "|    12|       1|     733|\n",
      "|    12|       1|     784|\n",
      "|    12|       1|     786|\n",
      "|    12|       1|     788|\n",
      "|    12|      34|      36|\n",
      "|    12|      34|     104|\n",
      "+------+--------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/13 23:28:23 WARN PythonUDFRunner: Detected deadlock while completing task 0.0 in stage 42 (TID 109): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def generate_pairs(movies):\n",
    "    return [(movies[i], movies[j]) for i in range(len(movies)) for j in range(i + 1, len(movies))]\n",
    "\n",
    "\n",
    "generate_pairs_udf = F.udf(generate_pairs, ArrayType(StructType([\n",
    "    StructField(\"movieId1\", IntegerType(), False),\n",
    "    StructField(\"movieId2\", IntegerType(), False)\n",
    "])))\n",
    "\n",
    "user_timeline_df = compute_timeline(remove_bad_ratings(ratings_df))\n",
    "user_timeline_df = user_timeline_df.withColumn(\"moviePairs\", generate_pairs_udf(F.col(\"movies\")))\n",
    "\n",
    "pairs_df = user_timeline_df.select(\n",
    "    F.col(\"userId\"),\n",
    "    F.explode(F.col(\"moviePairs\")).alias(\"pair\")\n",
    ").select(\n",
    "    \"userId\",\n",
    "    F.col(\"pair.movieId1\").alias(\"movieId1\"),\n",
    "    F.col(\"pair.movieId2\").alias(\"movieId2\")\n",
    ")\n",
    "\n",
    "pairs_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD-P_FNL8yEk"
   },
   "source": [
    "### Question B4\n",
    "\n",
    "> Let's imagine that all of our executors have 4GB of memory. \n",
    "> \n",
    "> If we consider If we consider the 'user timeline where movie ratings are greater or equal than 3.5, what will happen when we generate pairs dataframe for this dataset ?\n",
    "> \n",
    "> You need to consider:\n",
    "> - amount of bytes retained by lists of pairs\n",
    "> - amount of partitions we have in user timeline\n",
    ">\n",
    "> Also, consider what may happen because of skew. We may have all big user timelines inside same partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of bytes retained by lists of pairs: 107018480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Amount of bytes retained by lists of pairs\n",
    "print(\n",
    "    f'Amount of bytes retained by lists of pairs: {compute_timeline(remove_bad_ratings(ratings_df)).rdd.map(lambda x: sys.getsizeof(x[1])).sum()}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T22:29:12.131100Z",
     "start_time": "2024-02-13T22:29:08.283039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of partitions we have in user timeline: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:==============>                                           (2 + 6) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Amount of partitions we have in user timeline\n",
    "print(\n",
    "    f'Amount of partitions we have in user timeline: {compute_timeline(remove_bad_ratings(ratings_df)).rdd.getNumPartitions()}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T22:29:13.602277Z",
     "start_time": "2024-02-13T22:29:12.126506Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YhKxZd6BdWx"
   },
   "source": [
    "\n",
    "### Question B5\n",
    "\n",
    "> Create a function named *compute_pair_frequencies*, that takes a movie pair dataframe as parameter, and returns a dataframe of movie pairs and their user count, following this schema:\n",
    "> - movieId1 : integer\n",
    "> - movieId2 : integer\n",
    "> - count : integer\n",
    "> \n",
    "> Dataframe should be **ordered**, with most frequent pairs first.\n",
    ">\n",
    "> Test it on a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T22:29:58.814836Z",
     "start_time": "2024-02-13T22:29:58.731185Z"
    }
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 10\u001B[0m\n\u001B[1;32m      3\u001B[0m     pair_frequencies_df \u001B[38;5;241m=\u001B[39m movie_pair_df\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmovieId1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmovieId2\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;241m.\u001B[39mcount() \\\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcount\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muserCount\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;241m.\u001B[39morderBy(F\u001B[38;5;241m.\u001B[39mdesc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muserCount\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pair_frequencies_df\n\u001B[0;32m---> 10\u001B[0m toy \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m300\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m400\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmovieId1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmovieId2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muserId\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m pair_frequencies_df \u001B[38;5;241m=\u001B[39m compute_pair_frequencies(toy\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muserId\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m     20\u001B[0m pair_frequencies_df\u001B[38;5;241m.\u001B[39mshow()\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/pyspark/sql/session.py:1383\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1381\u001B[0m SparkSession\u001B[38;5;241m.\u001B[39m_activeSession \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m   1382\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1383\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSparkSession\u001B[49m\u001B[38;5;241m.\u001B[39msetActiveSession(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession)\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, DataFrame):\n\u001B[1;32m   1385\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   1386\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSHOULD_NOT_DATAFRAME\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1387\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m   1388\u001B[0m     )\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/java_gateway.py:1712\u001B[0m, in \u001B[0;36mJVMView.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1709\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;241m==\u001B[39m UserHelpAutoCompletion\u001B[38;5;241m.\u001B[39mKEY:\n\u001B[1;32m   1710\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m UserHelpAutoCompletion()\n\u001B[0;32m-> 1712\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproto\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREFLECTION_COMMAND_NAME\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[1;32m   1714\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproto\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_id\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[1;32m   1715\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mproto\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEND_COMMAND_PART\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1716\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer \u001B[38;5;241m==\u001B[39m proto\u001B[38;5;241m.\u001B[39mSUCCESS_PACKAGE:\n\u001B[1;32m   1717\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m JavaPackage(name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client, jvm_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_id)\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "def compute_pair_frequencies(movie_pair_df):\n",
    "    # Group by movieId1 and movieId2, and count the occurrences\n",
    "    pair_frequencies_df = movie_pair_df.groupBy(\"movieId1\", \"movieId2\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"userCount\") \\\n",
    "        .orderBy(F.desc(\"userCount\"))\n",
    "    return pair_frequencies_df\n",
    "\n",
    "\n",
    "toy = spark.createDataFrame([\n",
    "    (100, 200, 1),\n",
    "    (100, 300, 1),\n",
    "    (200, 300, 2),\n",
    "    (100, 200, 2),\n",
    "    (400, 500, 1)\n",
    "], [\"movieId1\", \"movieId2\", \"userId\"])\n",
    "\n",
    "pair_frequencies_df = compute_pair_frequencies(toy.drop(\"userId\"))\n",
    "\n",
    "pair_frequencies_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d81d3Thf6DPD"
   },
   "source": [
    "### Question B6\n",
    "\n",
    "> Quickly test the whole algorithm on *ratings_df* or a subset of it.\n",
    "> \n",
    "> How many shuffles for the whole algorithm ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CiZfGfshztLZ",
    "ExecuteTime": {
     "end_time": "2024-02-13T22:30:13.889992Z",
     "start_time": "2024-02-13T22:30:13.819990Z"
    }
   },
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m user_timeline_df \u001B[38;5;241m=\u001B[39m compute_timeline(\u001B[43mremove_bad_ratings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mratings_df\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m, in \u001B[0;36mremove_bad_ratings\u001B[0;34m(ratings_df, rating_threshold)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mremove_bad_ratings\u001B[39m(ratings_df, rating_threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3.5\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ratings_df\u001B[38;5;241m.\u001B[39mfilter(\u001B[43mratings_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrating\u001B[49m \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m rating_threshold)\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3126\u001B[0m, in \u001B[0;36mDataFrame.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   3122\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[1;32m   3123\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m   3124\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name)\n\u001B[1;32m   3125\u001B[0m     )\n\u001B[0;32m-> 3126\u001B[0m jc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Column(jc)\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m--> 291\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/py4j/clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[1;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[0;32m--> 438\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_address\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_port\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "user_timeline_df = compute_timeline(remove_bad_ratings(ratings_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuqp0Y6ky8ed"
   },
   "source": [
    "## A priori\n",
    "\n",
    "> You can find a good description of Apriori algorithm here:  \n",
    "> https://en.wikipedia.org/wiki/Apriori_algorithm\n",
    ">\n",
    "> Some other resources:  \n",
    "> [Apriori — Association Rule Mining In-depth Explanation and Python Implementation](https://towardsdatascience.com/apriori-association-rule-mining-explanation-and-python-implementation-290b42afdfc6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0E8hbjPFLgd"
   },
   "source": [
    "### Question B7\n",
    "\n",
    "> Implement your own version of A priori to compute most frequent pairs and quickly test it on *ratings_df* or a subset of it.\n",
    "> \n",
    "> You may want to rely on *F.explode* as an alternative to udf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.219512Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYjT4dDVFSGv"
   },
   "source": [
    "### Question B8\n",
    "\n",
    "> Implement your own version of A priori to compute most frequent triplets.\n",
    "> \n",
    "> A this stage of the 'A priori' section, you are probably doing the same thing multiple times. \n",
    "> \n",
    "> Maybe it's time to factorize your code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.220813Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRUB_yQA7I-m"
   },
   "source": [
    "## 3. FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You can find a good description of FP-Growth algorithm with Spark here:  \n",
    "> https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html\n",
    ">\n",
    "> Some other resources:  \n",
    "[FP Growth — Frequent Pattern Generation in Data Mining with Python Implementation](https://towardsdatascience.com/fp-growth-frequent-pattern-generation-in-data-mining-with-python-implementation-244e561ab1c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5CJmMyFW9Q"
   },
   "source": [
    "### Question B9\n",
    "\n",
    "> Use the Spark version of FP-Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.221980Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj1HcZkRGRRs"
   },
   "source": [
    "# Part C - Probabilistic Latent Semantic Model (5 points)\n",
    "\n",
    "> Aim of this section is to implement a Probabilistic Latent Semantic Model.\n",
    "> \n",
    "> We will use an expectation maximization algorithm to learn its parameters.\n",
    ">\n",
    "> In the first set of questions you will implement some utility functions to deal with matrix manipulations.\n",
    ">\n",
    "> In the second set of questions, you will implement the algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "papRtCuWJxho"
   },
   "source": [
    "## Matrix manipulation functions\n",
    "\n",
    "> We will implement matrix operations that will be usefull to run the PLSI algorithm afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtRkYH9NC-RO"
   },
   "source": [
    "### Question CMatrix1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzVkNoL4KQ0L"
   },
   "source": [
    "#### `matrix_sum_rows` \n",
    "\n",
    "> Takes a matrix (a column containing arrays of fixed length) and returns the sum of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "P1VyUs79KUeg",
    "ExecuteTime": {
     "end_time": "2024-01-30T15:19:10.900819Z",
     "start_time": "2024-01-30T15:19:09.326032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input array\n",
      "[[ 1.  2.  3.  4.]\n",
      " [40. 30. 20. 10.]]\n",
      "Expected output\n",
      "[ 10. 100.]\n",
      "Obtained output\n",
      "+------------------------+-------+\n",
      "|matrix                  |row_sum|\n",
      "+------------------------+-------+\n",
      "|[1.0, 2.0, 3.0, 4.0]    |10.0   |\n",
      "|[40.0, 30.0, 20.0, 10.0]|100.0  |\n",
      "+------------------------+-------+\n"
     ]
    }
   ],
   "source": [
    "# Hint: https://stackoverflow.com/a/57448698/2015762\n",
    "from prettytable import PrettyTable\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "def matrix_sum_rows(col_name, length_of_array):\n",
    "    cols = [F.col(col_name)[i] for i in range(length_of_array)]\n",
    "    return F.udf(lambda *xs: float(sum(xs)), FloatType())(*cols)\n",
    "\n",
    "\n",
    "input_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "expected_output = input_array.sum(axis=1)\n",
    "print('Input array')\n",
    "print(input_array)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(input_array.tolist()).map(lambda x: Row(matrix=x)).toDF()\n",
    "    .withColumn('row_sum', matrix_sum_rows('matrix', 4))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRczikuRIiwH"
   },
   "source": [
    "### Question CMatrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFJwS3HAKYDj",
    "tags": []
   },
   "source": [
    "#### `matrix_sum_columns`\n",
    "\n",
    "> Takes a matrix (a column containing arrays of fixed length) and returns the sum of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "df36LGTSKclE",
    "ExecuteTime": {
     "end_time": "2024-01-30T15:32:16.852358Z",
     "start_time": "2024-01-30T15:32:16.570687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input array\n",
      "[[ 1.  2.  3.  4.]\n",
      " [40. 30. 20. 10.]]\n",
      "Expected output\n",
      "[41. 32. 23. 14.]\n",
      "Obtained output\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'alias'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 18\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(expected_output)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObtained output\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     16\u001B[0m (\n\u001B[1;32m     17\u001B[0m     spark\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39mparallelize(input_array\u001B[38;5;241m.\u001B[39mtolist())\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m x: Row(matrix\u001B[38;5;241m=\u001B[39mx))\u001B[38;5;241m.\u001B[39mtoDF()\n\u001B[0;32m---> 18\u001B[0m     \u001B[38;5;241m.\u001B[39mselect(\u001B[43mmatrix_sum_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmatrix\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43malias\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcol_sum\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     19\u001B[0m )\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'alias'"
     ]
    }
   ],
   "source": [
    "# Hint: https://stackoverflow.com/a/54382990/2015762\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "\n",
    "def matrix_sum_columns(col_name, length_of_array):\n",
    "    pass\n",
    "\n",
    "\n",
    "input_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "expected_output = input_array.sum(axis=0)\n",
    "print('Input array')\n",
    "print(input_array)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(input_array.tolist()).map(lambda x: Row(matrix=x)).toDF()\n",
    "    .select(matrix_sum_columns('matrix', 4).alias('col_sum'))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47EAeV3_IqpH"
   },
   "source": [
    "### Question CMatrix3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCWm2vK4KhES"
   },
   "source": [
    "#### `matrix_normalize_rows`\n",
    "\n",
    "> Takes a matrix (a column containing arrays of fixed length) and returns the same matrix where the rows have been divded by their sum, such that each row sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsNhEtSuKZBI",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.225218Z"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_normalize_rows(col_name, length_of_array):\n",
    "\n",
    "\n",
    "input_array = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "expected_output = input_array / input_array.sum(axis=1).reshape(-1, 1)\n",
    "print('Input array')\n",
    "print(input_array)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(input_array.tolist()).map(lambda x: Row(numbers=x)).toDF()\n",
    "    .withColumn('normalized_elements', matrix_normalize_rows('numbers', 4))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdOIy3UaIu9P"
   },
   "source": [
    "### Question CMatrix4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPWSuJVkKkoS"
   },
   "source": [
    "#### `matrix_elementwise_product`\n",
    "\n",
    "> Takes two matrices and return their elementwise product (aka. Hadamard product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EHBDsfzKovu",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.226322Z"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_elementwise_product(col_name_1, col_name_2, length_of_array):\n",
    "\n",
    "\n",
    "input_array_1 = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "input_array_2 = np.array([[1, 2, 1, 2], [10, 20, 10, 20]], dtype=float)\n",
    "expected_output = input_array_1 * input_array_2\n",
    "print('Input array')\n",
    "print(input_array_1)\n",
    "print(input_array_2)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(zip(input_array.tolist(), input_array_2.tolist())).map(\n",
    "        lambda x: Row(numbers_1=x[0], numbers_2=x[1])).toDF()\n",
    "    .withColumn('elementwise_products', matrix_elementwise_product('numbers_1', 'numbers_2', 4))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G7umlGqIx0F"
   },
   "source": [
    "### Question CMatrix5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLayzlbbKvS2"
   },
   "source": [
    "#### `matrix_elementwise_divide`\n",
    "\n",
    "> Takes two matrices and divide elementwise the first one by the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZ2Mhzj9KwT6",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.227546Z"
    }
   },
   "outputs": [],
   "source": [
    "def matrix_elementwise_divide(col_name_1, col_name_2, length_of_array):\n",
    "\n",
    "\n",
    "input_array_1 = np.array([[1, 2, 3, 4], [40, 30, 20, 10]], dtype=float)\n",
    "input_array_2 = np.array([[1, 2, 1, 2], [10, 20, 10, 20]], dtype=float)\n",
    "expected_output = input_array_1 / input_array_2\n",
    "print('Input array')\n",
    "print(input_array_1)\n",
    "print(input_array_2)\n",
    "print('Expected output')\n",
    "print(expected_output)\n",
    "print('Obtained output')\n",
    "(\n",
    "    spark.sparkContext.parallelize(zip(input_array.tolist(), input_array_2.tolist())).map(\n",
    "        lambda x: Row(numbers_1=x[0], numbers_2=x[1])).toDF()\n",
    "    .withColumn('elementwise_divided', matrix_elementwise_divide('numbers_1', 'numbers_2', 4))\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwbYk4FxJ4oF"
   },
   "source": [
    "## PLSI\n",
    "\n",
    "> With\n",
    "> * N the number of users u\n",
    "> * M the number of movies s\n",
    "> * L the number of latent classes z\n",
    "> * T number of (user, movie) interactions (each interaction (s_t, u_t) means user u_t liked movie s_t)\n",
    ">\n",
    ">We suppose that the probability that a user will like a movie can be written in the form of a mixture model given by the equation:\n",
    "$$\n",
    "p(s|u) = \\sum_{z=1}^L p(s|z) p(z|u)\n",
    "$$\n",
    "And we want to optimize the likelihood of the observed user interactions\n",
    "$$\n",
    "L = - \\frac{1}{T} \\sum_{1}^{T} \\log p(s_t|u_t) = - \\frac{1}{T} \\sum_{1}^{T} \\sum_{z=1}^L p(s_t|z) p(z|u_t)\n",
    "$$\n",
    "That can be done using an EM algorithm working as follow:\n",
    ">\n",
    ">**E step**\n",
    ">\n",
    ">For each interaction (u_t, s_t), compute for all z = 1, ..., L:\n",
    "$$\n",
    "p(z|(u_t, s_t)) = \\frac{p(s_t|z) p(z|u_t)}{\\sum_z p(s_t|z) p(z|u_t)}\n",
    "$$\n",
    ">\n",
    ">**M step**\n",
    ">\n",
    ">Find each movie probability given a latent class\n",
    "$$\n",
    "p(s|z) = \\frac{N(z, s)}{N(z)} \n",
    "\\quad \\text{where} \\quad N(z, s) = \\sum_s \\sum_u p(z|(u, s)) \n",
    "\\quad \\text{and} \\quad N(z) = \\sum_s N(z, s)\n",
    "$$\n",
    "Find each latent class probability given each user.\n",
    "$$\n",
    "p(z|u) = \\frac{\\sum_s p(z|(u, s))}{\\sum_z \\sum_s p(z|(u, s))}\n",
    "$$\n",
    ">\n",
    ">We will have the following dataframes\n",
    ">\n",
    ">* `count_z_s`: M rows, with columns  `movieId`, `N(z,s)`.\n",
    ">* `count_z`: 1 row, with column `N(z)`.\n",
    ">* `p_s_knowing_z`: M rows, with columns  `movieId`, `p(s|z)`. For a given z, the sum of p(s|z) equals 1.\n",
    ">* `p_z_knowing_u`: N rows, with columns `userId`, `p(z|u)`. For a given u, the sum of p(z|u) equals 1.\n",
    ">* `p_z_knowing_u_and_s`: N x M rows, with columns `userId`, `movieId`, `p(z|u,s)`.\n",
    ">\n",
    "> \n",
    "> Implement the PLSI algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question CPLSI1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDGcbe41LwPV",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.228558Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_count_z(count_z_s, n_latent_classes):\n",
    "    \"\"\"Compute N(z) = sum_s N(z,s)\n",
    "    \"\"\"\n",
    "    N_z_s = count_z_s.groupBy().agg(\n",
    "        F.array(*[F.sum(F.col(\"N(z,s)\")[i]) for i in range(n_latent_classes)]).alias(\"N(z)\"))\n",
    "    return N_z_s\n",
    "\n",
    "\n",
    "count_z_s = ss.sparkContext.parallelize([\n",
    "    st.Row(**{\"movieId\": 1, \"N(z,s)\": [1., 3., 4.]}),\n",
    "    st.Row(**{\"movieId\": 2, \"N(z,s)\": [4., 5., 0.]}),\n",
    "]).toDF()\n",
    "get_count_z(count_z_s, 3).show()\n",
    "# Expected [5., 8., 4.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question CPLSI1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyftz13QLyez",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.229472Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_count_z_s(p_z_knowing_u_and_s, n_latent_classes):\n",
    "    \"\"\"Compute N(z,s) = sum_u p(z|u,s)\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-hhH_jgLzaN",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.230501Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_p_s_knowing_z(count_z_s, count_z, n_latent_classes):\n",
    "    \"\"\"Compute p(s|z) = N(z,s) / N(z)\n",
    "    \n",
    "    Hint: crossJoin may help\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8qlxgcWL2Pc",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.231393Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_p_z_knowing_u(p_z_knowing_u_and_s, n_latent_classes):\n",
    "    \"\"\"Compute p(z|u) = sum_s p(z|u,s) / sum_z sum_s p(z|u,s)\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvjuAPagL49D",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.232303Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_p_z_knowing_u_and_s(observed_pairs, count_z_s, count_z, p_z_knowing_u, n_latent_classes):\n",
    "    \"\"\"For all pairs of observed (u, s)\n",
    "    \n",
    "    Compute p(z|u,s) = [N(z, s) / N(z) * p(z|u)] / sum_z [N(z, s) / N(z) * p(z|u)]\n",
    "                     = [p(s|z) * p(z|u)] / sum_z [p(s|z) * p(z|u)]\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Lb6J5x_L7sj",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.233214Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(observed_pairs, count_z_s, count_z, p_z_knowing_u, n_latent_classes):\n",
    "    \"\"\"Compute the log likelihood of the observed pairs\n",
    "    \n",
    "    L = - 1 / T * sum_t log[ p(s|u) ]\n",
    "      = - 1 / T * sum_t log[ sum_z p(s|z) * p(z|u) ]\n",
    "    \"\"\"\n",
    "    # ...\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question CPLSI1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-x-AT8iL-TD",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.234128Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_statistics(observed_pairs, n_latent_classes):\n",
    "    \"\"\"Initialize either p(s|z) and p(z|u) or p(z|(u, s)) to be able to fuel the first iteration of the EM algorithm.\n",
    "    What would happen if you initialize these to a constant value ?\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question CPLSI1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQiwyBZVMAu-",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.235063Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_plsi(observed_pairs, n_iterations, n_latent_classes, checkpoint_every=1):\n",
    "    start_init_time = time.time()\n",
    "    spark.sparkContext.setJobDescription(\"Initialization\")\n",
    "\n",
    "    # ... = initialize_statistics(observed_pairs, n_latent_classes)\n",
    "    llh = log_likelihood(observed_pairs,  # ... #, n_latent_class\n",
    "                         mlflow.log_metric(key=\"llh\", value=llh, step=0)\n",
    "    print(f'LLH: {llh:.10f}')\n",
    "\n",
    "    end_init_time = time.time()\n",
    "    print(f'Initialization: {end_init_time - start_init_time:.1f}s')\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        start_e_step = time.time()\n",
    "    spark.sparkContext.setJobDescription(f\"Iteration {i + 1}: E-step\")\n",
    "    # E step\n",
    "    # ...\n",
    "\n",
    "    end_e_step = time.time()\n",
    "    print(f'Iteration {i + 1}: E-step: {end_e_step - start_e_step:.1f}s')\n",
    "\n",
    "    spark.sparkContext.setJobDescription(f\"Iteration {i + 1}: M-step\")\n",
    "    # M step\n",
    "    # ...\n",
    "\n",
    "    llh = log_likelihood(observed_pairs, count_z_s, count_z, p_z_knowing_u, n_latent_classes)\n",
    "    mlflow.log_metric(key=\"llh\", value=llh, step=i + 1)\n",
    "\n",
    "    end_m_step = time.time()\n",
    "    print(f'Iteration {i + 1}: M-step: {end_m_step - end_e_step:.1f}s')\n",
    "    print(f'LLH: {llh:.10f}')\n",
    "\n",
    "    return get_p_s_knowing_z(count_z_s, count_z, n_latent_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er6q4Xm5Jj7W",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.236065Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dV0zwPSlJkqk",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.237276Z"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpBo-j0dMGIs",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.238269Z"
    }
   },
   "outputs": [],
   "source": [
    "n_iterations = 20\n",
    "n_latent_classes = 5\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"n_iterations\", n_iterations)\n",
    "    mlflow.log_param(\"n_latent_classes\", n_latent_classes)\n",
    "    run_plsi(ratings_df.sample(0.1), n_iterations=n_iterations, n_latent_classes=n_latent_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK3XqiypKREd"
   },
   "source": [
    "### Question CPLSI2.1\n",
    "\n",
    "> How does the EM algorithm is supposed to scale with the number of EM steps ? Do you observe such a scaling ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.239300Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK3XqiypKREd"
   },
   "source": [
    "### Question CPLSI2.2\n",
    "> If each steap takes longer than the previous one: Try using .cache() wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.240386Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK3XqiypKREd"
   },
   "source": [
    "### Question CPLSI2.3\n",
    "\n",
    "> Try to unpersist your dataframes when they become unneeded (look at the Storage tab in the Spark UI) (Optional + 2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.241417Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK3XqiypKREd"
   },
   "source": [
    "### Question CPLSI2.4\n",
    "\n",
    "> If after few steps (typically 5), your algorithm starts being much slower and spend more and more time scheduling jobs (look in the Spark UI), try using [.localCheckpoint()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD.localCheckpoint). How does it differ from caching ? What are the benefits and the drawbacks ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.242645Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbstFMhYnpSk"
   },
   "source": [
    "# Part D - Test them all (5 points)\n",
    "\n",
    "> In this section, we create training and test datasets, and test all the different prediction algorithms described above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "engmZXePsEfv"
   },
   "source": [
    "## Training and Testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIxB2g3QsKGC"
   },
   "source": [
    "### Question D1\n",
    "\n",
    "> Create the training dataset named *training_df*. It is made of raw ratings dataframe, where:\n",
    "> - *hash(userId) % 2 == 0*\n",
    "> - and *rating >= 3.5*\n",
    "> \n",
    "> You should rely on functions written in Part A.\n",
    "> \n",
    "> Persist it on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.243819Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iK43OefMszyg"
   },
   "source": [
    "### Question D2\n",
    "\n",
    "> Create a function named *create_test_df*, that creates a test dataset from a ratings dataframe ; it only retain the following records:\n",
    "> - hash(userId) % 2 == 1\n",
    "> - rating >= 3.5\n",
    "> \n",
    "> Also, the function returns a dataframe structured like this:\n",
    "> - userid : the user id\n",
    "> - movies : list[integer] (all the movies in the user timeline minus the *K* most recent ones)\n",
    "> - label : list[integer] (all the *K* most recent movies in user timeline)\n",
    ">\n",
    "> *K* is parameter whose default value is 5.\n",
    "> \n",
    "> Test the test dataset creation on a toy example.\n",
    "> \n",
    "> Create the real test dataset from the whole movieLense dataset. Name it *test_df*. Persist it on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.244829Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_test_df(ratings_df, K=5):\n",
    "    return ratings_df.filter(F.col(\"userId\") % 2 == 1).groupBy(\"userId\").agg(\n",
    "        F.collect_list(\"movieId\").alias(\"movies\")).withColumn(\"label\", F.slice(F.col(\"movies\"), -K, K)).withColumn(\n",
    "        \"movies\", F.slice(F.col(\"movies\"), 1, F.size(F.col(\"movies\")) - K))\n",
    "\n",
    "\n",
    "test_df = create_test_df(ratings_df)\n",
    "test_df.show()\n",
    "test_df.write.parquet(\"test.parquet\", compression=\"gzip\")\n",
    "test_df = spark.read.parquet(\"test.parquet\")\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CIzNEs5zgVW"
   },
   "source": [
    "### Question D3\n",
    "\n",
    "> Use/adapt each of the algorithms defined in previous sections (naïve, a-priori, FP-growth, PLSI) to predict the 5 next movies that will be seen by the user based on previously seen movies.\n",
    "> \n",
    "> Each algorithm can be 'trained' on *training_df* or a subset of it ; choose and justify.\n",
    "> \n",
    "> For each algorithm, make a quick qualitative analysis, to see how relevant recommended movies are. You should rely on *movies_df* for this question.\n",
    ">\n",
    "> Then, compare the algorithms with the *test_df*, with metrics like *recall* and *precision at k* (define some methods that compute recall and precision at k from test dataframe and predictions dataframe parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAyI7OsuWMWe",
    "ExecuteTime": {
     "start_time": "2024-01-30T15:17:56.245892Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
