{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 2: Exploring the Movielens dataset with Spark Dataframes and Spark SQL\n",
    "\n",
    "In this session we will use the movielens dataset to introduce the essential features of the Spark DataFrame API and showcase its power. This tutorial also has important links to the Spark documentation and/or other relevant material."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "170fa17c-ff25-4d87-a019-11d3d9fcd796"
    },
    "id": "9okae-NnxhR0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prerequisites"
   ],
   "metadata": {
    "id": "aHGnavRyEEDQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Spark Environment\n",
    "Since we are not running on databricks, we will need to install Spark by ourselves, every time we run the session.  "
   ],
   "metadata": {
    "id": "q392zOmQyG1T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pyspark"
   ],
   "metadata": {
    "id": "FwjlINKayMry",
    "ExecuteTime": {
     "end_time": "2024-01-23T13:04:29.804504Z",
     "start_time": "2024-01-23T13:04:27.479596Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/quyenlinhta/IASD/iasd/lib/python3.10/site-packages (3.5.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/quyenlinhta/IASD/iasd/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set('spark.ui.port', '4050')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()"
   ],
   "metadata": {
    "id": "Fx0O5nUByWKr",
    "ExecuteTime": {
     "end_time": "2024-01-23T13:12:43.825129Z",
     "start_time": "2024-01-23T13:12:43.630802Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /var/folders/sm/lw2bbffs15g5l3r4ldd25hc00000gn/T/ipykernel_64606/2894928871.py:6 ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext, SparkConf\n\u001B[1;32m      5\u001B[0m conf \u001B[38;5;241m=\u001B[39m SparkConf()\u001B[38;5;241m.\u001B[39mset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspark.ui.port\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m4050\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mmaster(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal[*]\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/pyspark/context.py:201\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mappName \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.app.name\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkHome \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.home\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 201\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (k, v) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_conf\u001B[38;5;241m.\u001B[39mgetAll():\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m k\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executorEnv.\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    203\u001B[0m         varName \u001B[38;5;241m=\u001B[39m k[\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executorEnv.\u001B[39m\u001B[38;5;124m\"\u001B[39m):]\n",
      "File \u001B[0;32m~/IASD/iasd/lib/python3.10/site-packages/pyspark/context.py:449\u001B[0m, in \u001B[0;36m_ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    445\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefaultParallelism\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    446\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    447\u001B[0m \u001B[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for\u001B[39;00m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;124;03m    reduce tasks)\u001B[39;00m\n\u001B[0;32m--> 449\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m    450\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39mdefaultParallelism()\n",
      "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /var/folders/sm/lw2bbffs15g5l3r4ldd25hc00000gn/T/ipykernel_64606/2894928871.py:6 "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "f623d620-792d-47b5-b629-e099b76f5b4f"
    },
    "id": "PwYcRsyHxhR4",
    "ExecuteTime": {
     "end_time": "2024-01-23T13:04:33.987567Z",
     "start_time": "2024-01-23T13:04:33.558928Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional step : Enabling Spark UI through secure tunnel\n",
    "\n",
    "This step is useful if you want to look at Spark UI.\n",
    "First, you need to create a free ngrok account : https://dashboard.ngrok.com/login.  \n",
    "Then connect on the website and copy your AuthToken."
   ],
   "metadata": {
    "id": "78ZSghQoE5Gu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"# this step downloads ngrok, configures your AuthToken, then starts the tunnel\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip\n",
    "#!./ngrok authtoken my_ngrok_auth_token_retrieved_from_website # <-------------- change this line !\n",
    "get_ipython().system_raw('./ngrok http 4050 &')\"\"\""
   ],
   "metadata": {
    "id": "m-Bz2KdLE_Ot",
    "ExecuteTime": {
     "end_time": "2024-01-23T13:04:45.115255Z",
     "start_time": "2024-01-23T13:04:45.057635Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\"# this step downloads ngrok, configures your AuthToken, then starts the tunnel\\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\\n!unzip ngrok-stable-linux-amd64.zip\\n#!./ngrok authtoken my_ngrok_auth_token_retrieved_from_website # <-------------- change this line !\\nget_ipython().system_raw('./ngrok http 4050 &')\""
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Other imports"
   ],
   "metadata": {
    "id": "jIooy0loFKkq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import urllib\n",
    "import urllib.request as req\n",
    "import zipfile\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark DataFrame\n",
    "import pyspark.sql.functions as F"
   ],
   "metadata": {
    "id": "YxEXH_TGFPb9",
    "ExecuteTime": {
     "end_time": "2024-01-23T13:04:49.681142Z",
     "start_time": "2024-01-23T13:04:48.409979Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "\n",
    "## Downloading and unzipping the data (run only once !)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "041cdfc4-46fc-4490-962a-72baa30440e4"
    },
    "id": "LPc7bEydxhR9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"url = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'\n",
    "filehandle, _ = urllib.request.urlretrieve(url)\n",
    "zip_file_object = zipfile.ZipFile(filehandle, 'r')\n",
    "zip_file_object.namelist()\"\"\""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "c43c2d2f-ea5a-4555-b855-caca395647ee"
    },
    "id": "uEW_NgvIxhR_",
    "ExecuteTime": {
     "end_time": "2023-12-19T14:46:57.167479Z",
     "start_time": "2023-12-19T14:46:57.160421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"url = 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'\\nfilehandle, _ = urllib.request.urlretrieve(url)\\nzip_file_object = zipfile.ZipFile(filehandle, 'r')\\nzip_file_object.namelist()\""
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "# zip_file_object.extractall()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "712ce604-ad47-4319-ba49-b8d0df40ea9b"
    },
    "id": "TOF9I4dOxhSB",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:44:33.180039Z",
     "start_time": "2023-12-21T17:44:33.134160Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "!cd ml-20m ; ls"
   ],
   "metadata": {
    "id": "XgxL32xVz9N0",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:44:33.782564Z",
     "start_time": "2023-12-21T17:44:33.652283Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt          genome-tags.csv     movies.csv          \u001B[1m\u001B[36msampled_ratings.csv\u001B[m\u001B[m\r\n",
      "genome-scores.csv   links.csv           ratings.csv         tags.csv\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spark DataFrames essentials\n",
    "\n",
    "\n",
    "### Reading data"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "1400a8fd-86c9-4e41-b39d-9a3dc2b105fd"
    },
    "id": "yWe5-STxxhSD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_path = \"ml-20m/movies.csv\"\n",
    "ratings_path = \"ml-20m/ratings.csv\""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "6ab802b9-46a0-40f6-89ae-2221cf4b000b"
    },
    "id": "pmb5u3pLxhSE",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:44:34.556367Z",
     "start_time": "2023-12-21T17:44:34.483284Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "We read the csv files using [`spark.read`](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "bd59e075-f2f0-4ce3-ae63-abb2ac5ad515"
    },
    "id": "4deTpFphxhSG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df = spark.read.options(header=True).csv(movies_path)\n",
    "ratings_df = spark.read.options(header=True).csv(ratings_path).sample(0.01)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "e17a1b72-3e17-4a33-ba51-b09ea109743c"
    },
    "id": "VQYgL_HCxhSJ",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:44:55.856372Z",
     "start_time": "2023-12-21T17:44:52.277513Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "We cache the read dataframes to avoid reloading them in subsequent computation."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "4901a996-6efc-465a-b5c7-152850acab78"
    },
    "id": "zxbPyi8VxhSK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.cache()\n",
    "ratings_df.cache()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "85124b91-e8be-4ef6-aac6-303fb084d3b2"
    },
    "id": "bdpJE_RPxhSL",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:44:55.918809Z",
     "start_time": "2023-12-21T17:44:55.858801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[userId: string, movieId: string, rating: string, timestamp: string]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then print a few rows from each dataframe."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "5c0f0847-69ca-4181-b49e-6ec3a7aa8f06"
    },
    "id": "sWcVp3uJxhSN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "164bdf37-ee60-4c5e-b684-61f7ba0ac370"
    },
    "id": "Ax0dB-GRxhSN",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:44:56.457250Z",
     "start_time": "2023-12-21T17:44:55.907754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_df.show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "c9689128-5540-46fd-9f6d-a54e7704abad"
    },
    "id": "6jMFWy-wxhSO",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:02.760818Z",
     "start_time": "2023-12-21T17:44:56.452693Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==============>                                            (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|   3000|   3.5|1112484569|\n",
      "|     1|   3265|   3.5|1112484525|\n",
      "|     3|   1214|   5.0| 944918856|\n",
      "|     4|    733|   5.0| 840879322|\n",
      "|     5|    454|   5.0| 851527723|\n",
      "+------+-------+------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Manipulating data"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "d7c226cb-65fd-45d2-ba31-fb0579cd60b9"
    },
    "id": "RYaJzQXjxhSP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.select(\"title\").show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "b3f5658d-46bd-431c-90e7-73aee307bc13"
    },
    "id": "lPhcwU2nxhSP",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:02.946901Z",
     "start_time": "2023-12-21T17:45:02.756041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|    Toy Story (1995)|\n",
      "|      Jumanji (1995)|\n",
      "|Grumpier Old Men ...|\n",
      "|Waiting to Exhale...|\n",
      "|Father of the Bri...|\n",
      "+--------------------+\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_df.filter(\"rating=5\").show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "864b079d-f282-4ae7-9acc-13dde09b9a74"
    },
    "id": "nB91iohMxhSQ",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:03.119412Z",
     "start_time": "2023-12-21T17:45:02.876442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     3|   1214|   5.0| 944918856|\n",
      "|     4|    733|   5.0| 840879322|\n",
      "|     5|    454|   5.0| 851527723|\n",
      "|    10|    858|   5.0| 943497439|\n",
      "|    11|   1225|   5.0|1230784460|\n",
      "+------+-------+------+----------+\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_df.groupby(\"userId\").agg({\"movieId\": \"count\"}).show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "ba116641-6eb3-4682-a89b-f00f339b6d04"
    },
    "id": "733lNSouxhSR",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:04.365738Z",
     "start_time": "2023-12-21T17:45:03.004481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:====================================>                      (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|userId|count(movieId)|\n",
      "+------+--------------+\n",
      "|   675|             1|\n",
      "|   829|             1|\n",
      "|  1090|             2|\n",
      "|  1159|             1|\n",
      "|  1436|             3|\n",
      "+------+--------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_df.withColumn(\"is_rating_high\", ratings_df[\"rating\"] >= 4).show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "4a8d55dc-eea6-4887-a1f0-62dab2fd049d"
    },
    "id": "CyLYxVZtxhSS",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:04.394117Z",
     "start_time": "2023-12-21T17:45:04.234838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+--------------+\n",
      "|userId|movieId|rating| timestamp|is_rating_high|\n",
      "+------+-------+------+----------+--------------+\n",
      "|     1|   3000|   3.5|1112484569|         false|\n",
      "|     1|   3265|   3.5|1112484525|         false|\n",
      "|     3|   1214|   5.0| 944918856|          true|\n",
      "|     4|    733|   5.0| 840879322|          true|\n",
      "|     5|    454|   5.0| 851527723|          true|\n",
      "+------+-------+------+----------+--------------+\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_df.withColumn(\"is_rating_low\", ratings_df.rating < 4).show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "fd9885ee-735c-469a-afaa-318f4c5ea790"
    },
    "id": "xxqzZTvSxhSS",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:04.583128Z",
     "start_time": "2023-12-21T17:45:04.341747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+-------------+\n",
      "|userId|movieId|rating| timestamp|is_rating_low|\n",
      "+------+-------+------+----------+-------------+\n",
      "|     1|   3000|   3.5|1112484569|         true|\n",
      "|     1|   3265|   3.5|1112484525|         true|\n",
      "|     3|   1214|   5.0| 944918856|        false|\n",
      "|     4|    733|   5.0| 840879322|        false|\n",
      "|     5|    454|   5.0| 851527723|        false|\n",
      "+------+-------+------+----------+-------------+\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_df.withColumnRenamed(\"rating\", \"note\").show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "24a0400b-4b93-4ec8-aecb-e81764eb9659"
    },
    "id": "IWbV8E-mxhST",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:04.634460Z",
     "start_time": "2023-12-21T17:45:04.412224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+----------+\n",
      "|userId|movieId|note| timestamp|\n",
      "+------+-------+----+----------+\n",
      "|     1|   3000| 3.5|1112484569|\n",
      "|     1|   3265| 3.5|1112484525|\n",
      "|     3|   1214| 5.0| 944918856|\n",
      "|     4|    733| 5.0| 840879322|\n",
      "|     5|    454| 5.0| 851527723|\n",
      "+------+-------+----+----------+\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "d70bac07-e07a-47e1-b41a-dc306bc157ba"
    },
    "id": "KEKoq5bAxhSU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Built-in transformations and aggregations"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "fb4faaa1-0aa2-4663-9cdf-a698bcf5c9da"
    },
    "id": "ucG-o5c8xhSV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_df.select(F.avg(\"rating\"), F.min(\"rating\"), F.max(\"rating\")).show()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "00c6d382-5208-4f6f-a5e9-4af83b63382a"
    },
    "id": "XEHHxaLfxhSW",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:05.300587Z",
     "start_time": "2023-12-21T17:45:04.839954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+-----------+\n",
      "|      avg(rating)|min(rating)|max(rating)|\n",
      "+-----------------+-----------+-----------+\n",
      "|3.528868591573276|        0.5|        5.0|\n",
      "+-----------------+-----------+-----------+\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joining Dataframes"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "34b47d97-66a2-4f1b-b8b4-1dc73eee3042"
    },
    "id": "0qrAMULXxhSX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_df.join(movies_df, \"movieId\").show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "4a14bd7c-6da3-4030-8124-48d696d2a71b"
    },
    "id": "2_EwAPfZxhSX",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:05.834047Z",
     "start_time": "2023-12-21T17:45:05.460071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|               title|              genres|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n",
      "|   3000|     1|   3.5|1112484569|Princess Mononoke...|Action|Adventure|...|\n",
      "|   3265|     1|   3.5|1112484525|Hard-Boiled (Lat ...|Action|Crime|Dram...|\n",
      "|   1214|     3|   5.0| 944918856|        Alien (1979)|       Horror|Sci-Fi|\n",
      "|    733|     4|   5.0| 840879322|    Rock, The (1996)|Action|Adventure|...|\n",
      "|    454|     5|   5.0| 851527723|    Firm, The (1993)|      Drama|Thriller|\n",
      "+-------+------+------+----------+--------------------+--------------------+\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "### User Defined functions (UDFs)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "72d716fa-d5d4-4605-a5dd-f2bbd78616dd"
    },
    "id": "ur3enjo1xhSY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "\n",
    "def length(string: str):\n",
    "    return len(string)\n",
    "\n",
    "\n",
    "length_udf = udf(length, LongType())"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "a5bd91e0-5711-48ca-a8ce-aaaa87fcd0d4"
    },
    "id": "Koj8J-LCxhSY",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:06.689747Z",
     "start_time": "2023-12-21T17:45:06.637137Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.select(length_udf(\"title\")).show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "5b8be010-571c-45ad-9097-4525c32dfd94"
    },
    "id": "x4RT7LIsxhSZ",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:07.875099Z",
     "start_time": "2023-12-21T17:45:07.103542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|length(title)|\n",
      "+-------------+\n",
      "|           16|\n",
      "|           14|\n",
      "|           23|\n",
      "|           24|\n",
      "|           34|\n",
      "+-------------+\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "@udf(\"string\")\n",
    "def length2(string: str):\n",
    "    return len(string)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "8eeb46c4-c5ef-4767-951c-7ebf31e28fcc"
    },
    "id": "KMEGeXQGxhSa",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:08.004183Z",
     "start_time": "2023-12-21T17:45:07.866260Z"
    }
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.select(length2(\"title\")).show(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "03c0ada0-cae5-46da-a607-ab69a1e18c96"
    },
    "id": "wyDzGprYxhSa",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:08.189556Z",
     "start_time": "2023-12-21T17:45:07.985120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|length2(title)|\n",
      "+--------------+\n",
      "|            16|\n",
      "|            14|\n",
      "|            23|\n",
      "|            24|\n",
      "|            34|\n",
      "+--------------+\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "title_lengths = movies_df.select(length2(\"title\").alias(\"title_length\"))"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "43ac47d8-b85b-4e34-beda-2ed5b0093f61"
    },
    "id": "3tijyUNQxhSc",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:08.191275Z",
     "start_time": "2023-12-21T17:45:08.150483Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "title_lengths.select(F.max(\"title_length\")).show()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "1903acfa-345f-4324-bc1e-602329c59616"
    },
    "id": "hzdNMc9qxhSc",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:08.586649Z",
     "start_time": "2023-12-21T17:45:08.311847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|max(title_length)|\n",
      "+-----------------+\n",
      "|               99|\n",
      "+-----------------+\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": [
    "title_lengths.select(F.min(\"title_length\")).show()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "dbcb3fe5-71c3-4b82-b814-7c96968d67bf"
    },
    "id": "S-Py0H0-xhSd",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:08.783342Z",
     "start_time": "2023-12-21T17:45:08.580281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|min(title_length)|\n",
      "+-----------------+\n",
      "|               10|\n",
      "+-----------------+\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query plan inspection and caching"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "d5c78c61-ebe9-4b3f-88da-5e70214906e7"
    },
    "id": "iJIK-1qwxhSd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "title_lengths.select(F.max(\"title_length\")).explain()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "686caef7-fcdb-48e2-9147-b551d9da7ce6"
    },
    "id": "PMeGqZiCxhSf",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:09.551548Z",
     "start_time": "2023-12-21T17:45:09.513856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortAggregate(key=[], functions=[max(title_length#1326)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=677]\n",
      "      +- SortAggregate(key=[], functions=[partial_max(title_length#1326)])\n",
      "         +- Project [pythonUDF0#1539 AS title_length#1326]\n",
      "            +- BatchEvalPython [length2(title#18)#1325], [pythonUDF0#1539]\n",
      "               +- InMemoryTableScan [title#18]\n",
      "                     +- InMemoryRelation [movieId#17, title#18, genres#19], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- FileScan csv [movieId#17,title#18,genres#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/quyenlinhta/IASD/bigdata/ml-20m/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<movieId:string,title:string,genres:string>\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "title_lengths.select(F.min(\"title_length\")).explain()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "5d6b4101-2ab6-4ee6-ad77-4f14d8e1a1de"
    },
    "id": "-P4YgNutxhSg",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:10.224463Z",
     "start_time": "2023-12-21T17:45:10.198168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortAggregate(key=[], functions=[min(title_length#1326)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=699]\n",
      "      +- SortAggregate(key=[], functions=[partial_min(title_length#1326)])\n",
      "         +- Project [pythonUDF0#1590 AS title_length#1326]\n",
      "            +- BatchEvalPython [length2(title#18)#1325], [pythonUDF0#1590]\n",
      "               +- InMemoryTableScan [title#18]\n",
      "                     +- InMemoryRelation [movieId#17, title#18, genres#19], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- FileScan csv [movieId#17,title#18,genres#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/quyenlinhta/IASD/bigdata/ml-20m/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<movieId:string,title:string,genres:string>\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "title_lengths.cache()\n",
    "title_lengths.select(F.max(\"title_length\")).show()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "ba1d51d1-7ebe-47b0-96e9-1bc61ee2c302"
    },
    "id": "EdqWQtTUxhSh",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:10.711667Z",
     "start_time": "2023-12-21T17:45:10.435304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|max(title_length)|\n",
      "+-----------------+\n",
      "|               99|\n",
      "+-----------------+\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "title_lengths.select(F.min(\"title_length\")).explain()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "4849d89e-af7a-4e73-923a-94f5526caee0"
    },
    "id": "daS5uHeTxhSh",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:10.766871Z",
     "start_time": "2023-12-21T17:45:10.714154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortAggregate(key=[], functions=[min(title_length#1326)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=775]\n",
      "      +- SortAggregate(key=[], functions=[partial_min(title_length#1326)])\n",
      "         +- InMemoryTableScan [title_length#1326]\n",
      "               +- InMemoryRelation [title_length#1326], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- AdaptiveSparkPlan isFinalPlan=false\n",
      "                        +- Project [pythonUDF0#1638 AS title_length#1326]\n",
      "                           +- BatchEvalPython [length2(title#18)#1325], [pythonUDF0#1638]\n",
      "                              +- InMemoryTableScan [title#18]\n",
      "                                    +- InMemoryRelation [movieId#17, title#18, genres#19], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                          +- FileScan csv [movieId#17,title#18,genres#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/quyenlinhta/IASD/bigdata/ml-20m/movies.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<movieId:string,title:string,genres:string>\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Writing csv"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "300ecd36-6f25-4143-9b53-038cd4edbabb"
    },
    "id": "V8Oqc96jxhSi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.sample(0.1).write.csv(\"ml-20m/movies-sample.csv\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "910a105e-8b86-4d98-8ed5-4e5d8feb8a7f"
    },
    "id": "wUu9fmNTxhSi",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:13.706076Z",
     "start_time": "2023-12-21T17:45:13.477171Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.sample(0.1).write.mode(\"overwrite\").csv(\"ml-20m/movies-sample.csv\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "ff09fef0-9948-4cd8-8106-144152b3f5eb"
    },
    "id": "puHBOTv1xhSk",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:14.151169Z",
     "start_time": "2023-12-21T17:45:14.000881Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure you have written on filesystem correctly."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "9c1a647b-a882-4ee8-be19-4b0d243bdb7f"
    },
    "id": "XwfnBqIPxhSl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!cd ml-20m ; ls"
   ],
   "metadata": {
    "id": "Qz3eY3Nfz2Ta",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:14.663043Z",
     "start_time": "2023-12-21T17:45:14.520723Z"
    }
   },
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.txt          links.csv           ratings.csv\r\n",
      "genome-scores.csv   \u001B[1m\u001B[36mmovies-sample.csv\u001B[m\u001B[m   \u001B[1m\u001B[36msampled_ratings.csv\u001B[m\u001B[m\r\n",
      "genome-tags.csv     movies.csv          tags.csv\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.listdir(\"ml-20m\")"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "5514eec6-3015-44fa-8e76-23ddf26f89f9"
    },
    "id": "wgXJEjduxhSm",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:14.866650Z",
     "start_time": "2023-12-21T17:45:14.835181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['links.csv',\n 'sampled_ratings.csv',\n 'movies-sample.csv',\n 'tags.csv',\n 'genome-tags.csv',\n 'ratings.csv',\n 'README.txt',\n 'genome-scores.csv',\n 'movies.csv']"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": [
    "This command writes a dataframe in parquet format :"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "bd03d9df-44ce-4f17-ba9a-fc24ed460331"
    },
    "id": "v43TCkLKxhSn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ratings_parquet_path = \"ml-20m/ratings.parquet\"\n",
    "spark.read.options(header=True).csv(ratings_path).write.parquet(ratings_parquet_path)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "b350b1d1-0f46-4d79-abd4-7602c646d62f"
    },
    "id": "RLC2OrUAxhSo",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:45:25.457980Z",
     "start_time": "2023-12-21T17:45:16.384599Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/21 18:45:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Questions"
   ],
   "metadata": {
    "id": "a2FhfvjCEWQW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 0: Compare processing time and amount of executors used, when reading from csv versus reading from parquet, for the following pipelines:\n",
    "- count total amount of records\n",
    "- count total amount of records for user 1\n",
    "- distinct count of timestamps"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "3490aef9-8fc9-43c6-8f47-581ba1995d03"
    },
    "id": "3GrJ_cooxhSo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**hint** `countDistinct` method can be used for third pipeline"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "957cb22c-bb77-4f4f-be22-a2243e39d628"
    },
    "id": "jl5jSFp2xhSo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:46:30.543751Z",
     "start_time": "2023-12-21T17:46:30.498962Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def timeit(func):\n",
    "    import time\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Execution time: {end - start}\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "349bc8b9-3c77-40f6-b5c9-27fce22156fc"
    },
    "id": "W9kd5Z0uxhSr",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:46:21.436316Z",
     "start_time": "2023-12-21T17:46:21.427475Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:46:21.971885Z",
     "start_time": "2023-12-21T17:46:21.961957Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.9606108665466309\n",
      "Counting records for csv: 20000263\n",
      "Execution time: 0.21493005752563477\n",
      "Counting records for parquet: 20000263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.397679328918457\n",
      "Counting records for user 1 for csv: 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.9519250392913818\n",
      "Counting records for user 1 for parquet: 175\n",
      "Counting distinct timestamps for csv:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/19 15:47:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT timestamp)|\n",
      "+-------------------------+\n",
      "|                 15351121|\n",
      "+-------------------------+\n",
      "\n",
      "Execution time: 9.734538078308105\n",
      "Counting distinct timestamps for parquet:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:47:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/12/19 15:48:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT timestamp)|\n",
      "+-------------------------+\n",
      "|                 15351121|\n",
      "+-------------------------+\n",
      "\n",
      "Execution time: 6.573633909225464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@timeit\n",
    "def count_records(df):\n",
    "    return df.count()\n",
    "\n",
    "\n",
    "@timeit\n",
    "def count_records_for_user(df, user_id):\n",
    "    return df.filter(f\"userId={user_id}\").count()\n",
    "\n",
    "\n",
    "@timeit\n",
    "def count_distinct_timestamps(df):\n",
    "    return df.select(F.countDistinct(\"timestamp\")).show()\n",
    "\n",
    "\n",
    "for_df = spark.read.options(header=True).csv(ratings_path)\n",
    "for_parquet = spark.read.parquet(ratings_parquet_path)\n",
    "print(\"Counting records for csv:\", count_records(for_df))\n",
    "print(\"Counting records for parquet:\", count_records(for_parquet))\n",
    "\n",
    "print(\"Counting records for user 1 for csv:\", count_records_for_user(for_df, 1))\n",
    "print(\"Counting records for user 1 for parquet:\", count_records_for_user(for_parquet, 1))\n",
    "\n",
    "print(\"Counting distinct timestamps for csv:\")\n",
    "count_distinct_timestamps(for_df)\n",
    "print(\"Counting distinct timestamps for parquet:\")\n",
    "count_distinct_timestamps(for_parquet)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T14:48:05.186433Z",
     "start_time": "2023-12-19T14:47:43.066459Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 1: Compute the (average, max, min) rating per movie, and get the highest and lowest rated movies ?"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "5a6d3266-ec98-4f17-998a-906c7db92d37"
    },
    "id": "1cGBCABrxhSv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**hint** Straightforward GroupBy then Aggregate"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "c9afac8a-2b7b-4f64-8558-07659bf6c01f"
    },
    "id": "jmpnXpqKxhSv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "agg_ratings = ratings_df.groupBy(\"movieId\").agg(\n",
    "    F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "    F.max(\"rating\").alias(\"max_rating\"),\n",
    "    F.min(\"rating\").alias(\"min_rating\")\n",
    ")\n",
    "\n",
    "agg_ratings_with_title = agg_ratings.join(movies_df, \"movieId\")\n",
    "highest_rated = agg_ratings_with_title.orderBy(F.desc(\"avg_rating\")).first()\n",
    "lowest_rated = agg_ratings_with_title.orderBy(\"avg_rating\").first()\n",
    "\n",
    "print(\"Highest Rated Movie:\", highest_rated)\n",
    "print(\"Lowest Rated Movie:\", lowest_rated)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "3826159f-9982-4754-8151-8f341f65e7ea"
    },
    "id": "kPEbH3ZXxhSw",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:50:17.428932Z",
     "start_time": "2023-12-21T17:50:15.100373Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Rated Movie: Row(movieId='100444', avg_rating=5.0, max_rating='5.0', min_rating='5.0', title=\"Rumble in the Air-Conditioned Auditorium: O'Reilly vs. Stewart 2012, The (2012)\", genres='Comedy')\n",
      "Lowest Rated Movie: Row(movieId='101243', avg_rating=0.5, max_rating='0.5', min_rating='0.5', title='Klip (Clip) (2012)', genres='Drama|Romance')\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 2: Amongst movies that were rated by at least 20 users, what are the movies with highest and lowest rating standard deviation ?"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "b8eadfa1-8d03-4723-ae18-ee507900df7b"
    },
    "id": "EeoE_lqtxhSx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**hint** How do you use a join to keep only a subset of movies ?"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "cc97e39c-8485-48c5-a293-76d7ebbd857a"
    },
    "id": "do2m3mXAxhSy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "std_dev_ratings = ratings_df.groupBy(\"movieId\").agg(\n",
    "    F.stddev(\"rating\").alias(\"rating_std_dev\"),\n",
    "    F.count(\"rating\").alias(\"rating_count\")\n",
    ")\n",
    "\n",
    "std_dev_ratings_filtered = std_dev_ratings.filter(\"rating_count >= 20\")\n",
    "\n",
    "std_dev_ratings_with_title = std_dev_ratings_filtered.join(movies_df, \"movieId\")\n",
    "\n",
    "highest_std_dev = std_dev_ratings_with_title.orderBy(F.desc(\"rating_std_dev\")).first()\n",
    "lowest_std_dev = std_dev_ratings_with_title.orderBy(\"rating_std_dev\").first()\n",
    "\n",
    "print(\"Movie with Highest Standard Deviation:\", highest_std_dev)\n",
    "print(\"Movie with Lowest Standard Deviation:\", lowest_std_dev)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "85091f9f-63e1-4e0d-b25a-44ab19e29ed0"
    },
    "id": "tfViVoQhxhSy",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:50:38.278358Z",
     "start_time": "2023-12-21T17:50:36.701432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie with Highest Standard Deviation: Row(movieId='711', rating_std_dev=1.459992790176863, rating_count=20, title='Flipper (1996)', genres='Adventure|Children')\n",
      "Movie with Lowest Standard Deviation: Row(movieId='3196', rating_std_dev=0.44577791412086437, rating_count=27, title='Stalag 17 (1953)', genres='Drama|War')\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 3: Compute the (average, max, min) rating per genre and get the highest and lowest rated genres, as well as the ones with the highest rating standard deviation ?"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "f2a1b266-a676-42ca-a5b3-2925d817da0e"
    },
    "id": "qcJlKxkOxhSz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**hint** How can you extract the individual genres from the genres column ? How do you use a custom function to do this ?"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "9b60f1f0-e62b-49da-b791-b38f7e3703e7"
    },
    "id": "ykzBbnz_xhSz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Rated Genre: Row(genre='Film-Noir', avg_rating=4.00327868852459, max_rating='5.0', min_rating='0.5', std_dev_rating=0.8607272946276987)\n",
      "Lowest Rated Genre: Row(genre='Horror', avg_rating=3.267199514366653, max_rating='5.0', min_rating='0.5', std_dev_rating=1.140700034285225)\n",
      "Genre with Highest Standard Deviation: Row(genre='Horror', avg_rating=3.267199514366653, max_rating='5.0', min_rating='0.5', std_dev_rating=1.140700034285225)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "exploded_genres = movies_df.withColumn(\"genre\", explode(F.split(\"genres\", \"\\\\|\")))\n",
    "\n",
    "genre_ratings = exploded_genres.join(ratings_df, \"movieId\")\n",
    "\n",
    "genre_agg = genre_ratings.groupBy(\"genre\").agg(\n",
    "    F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "    F.max(\"rating\").alias(\"max_rating\"),\n",
    "    F.min(\"rating\").alias(\"min_rating\"),\n",
    "    F.stddev(\"rating\").alias(\"std_dev_rating\")\n",
    ")\n",
    "\n",
    "highest_rated_genre = genre_agg.orderBy(F.desc(\"avg_rating\")).first()\n",
    "lowest_rated_genre = genre_agg.orderBy(\"avg_rating\").first()\n",
    "\n",
    "highest_std_dev_genre = genre_agg.orderBy(F.desc(\"std_dev_rating\")).first()\n",
    "\n",
    "print(\"Highest Rated Genre:\", highest_rated_genre)\n",
    "print(\"Lowest Rated Genre:\", lowest_rated_genre)\n",
    "print(\"Genre with Highest Standard Deviation:\", highest_std_dev_genre)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:51:06.319025Z",
     "start_time": "2023-12-21T17:51:04.027925Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 4: Extract the year information from the title and compute the average rating per year (for years where more than 10 movies came out), how does the this quantity evolve ?"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "4fe1a628-5a75-4cd7-b3ed-dfb70c976a4d"
    },
    "id": "mG0J52-ixhS4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**hint** Extracting the year from the title can be done with a Regular Expression"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "8804cc01-f75b-4250-b9ee-fa35996bda08"
    },
    "id": "tgH2_ld2xhS4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|        avg_rating|\n",
      "+----+------------------+\n",
      "|    |               3.5|\n",
      "|1915|               4.0|\n",
      "|1916|               2.5|\n",
      "|1917|               4.0|\n",
      "|1919|               3.8|\n",
      "|1920|3.4285714285714284|\n",
      "|1921|3.6666666666666665|\n",
      "|1922|3.8289473684210527|\n",
      "|1923|3.5833333333333335|\n",
      "|1924|3.1666666666666665|\n",
      "|1925|3.9482758620689653|\n",
      "|1926|          3.953125|\n",
      "|1927|4.1923076923076925|\n",
      "|1928| 3.893939393939394|\n",
      "|1929|3.7916666666666665|\n",
      "|1930|3.9423076923076925|\n",
      "|1931|               4.0|\n",
      "|1932|3.8424657534246576|\n",
      "|1933| 3.733128834355828|\n",
      "|1934|3.8472222222222223|\n",
      "+----+------------------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "movies_df_with_year = movies_df.withColumn(\"year\", regexp_extract(\"title\", \"\\\\((\\\\d{4})\\\\)\", 1))\n",
    "yearly_ratings = ratings_df.join(movies_df_with_year, \"movieId\")\n",
    "avg_rating_per_year = yearly_ratings.groupBy(\"year\").agg(F.avg(\"rating\").alias(\"avg_rating\"))\n",
    "movies_per_year = movies_df_with_year.groupBy(\"year\").count()\n",
    "years_with_more_than_10_movies = movies_per_year.filter(\"count > 10\").select(\"year\")\n",
    "result = avg_rating_per_year.join(years_with_more_than_10_movies, \"year\").orderBy(\"year\")\n",
    "result.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T17:53:34.959541Z",
     "start_time": "2023-12-21T17:53:33.867180Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 5: What are the top 3 genres per year ?"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "39ba5e62-e569-42e1-83bf-711e4b04b1d4"
    },
    "id": "vdB3jNiVxhS6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**hint** Look at the answer here https://stackoverflow.com/questions/38397796/retrieve-top-n-in-each-group-of-a-dataframe-in-pyspark"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "1dfb28a0-3013-47a3-9d9f-6b25ef84ab53"
    },
    "id": "HQCmWvUBxhS7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import regexp_extract, explode\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "movies_df_with_year = movies_df.withColumn(\"year\", regexp_extract(\"title\", \"\\\\((\\\\d{4})\\\\)\", 1))\n",
    "exploded_genres = movies_df_with_year.withColumn(\"genre\", explode(F.split(\"genres\", \"\\\\|\")))\n",
    "genre_ratings = exploded_genres.join(ratings_df, \"movieId\")\n",
    "genre_year_avg = genre_ratings.groupBy(\"year\", \"genre\").agg(F.avg(\"rating\").alias(\"avg_rating\"))\n",
    "windowSpec = Window.partitionBy(\"year\").orderBy(F.desc(\"avg_rating\"))\n",
    "\n",
    "ranked_genres = genre_year_avg.withColumn(\"rank\", F.rank().over(windowSpec))\n",
    "top_3_genres_per_year = ranked_genres.filter(\"rank <= 3\")\n",
    "top_3_genres_per_year.show()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "3c027f7f-2269-469f-adbd-7ddc39ba0ae8"
    },
    "id": "DmF4TSPTxhS7",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:55:41.399451Z",
     "start_time": "2023-12-21T17:55:39.749722Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------------+----+\n",
      "|year|    genre|        avg_rating|rank|\n",
      "+----+---------+------------------+----+\n",
      "|    |   Sci-Fi|               3.5|   1|\n",
      "|    |   Comedy|               3.5|   1|\n",
      "|1903|    Crime|               3.5|   1|\n",
      "|1903|  Western|               3.5|   1|\n",
      "|1915|      War|               4.0|   1|\n",
      "|1915|    Drama|               4.0|   1|\n",
      "|1916|Adventure|               4.0|   1|\n",
      "|1916|   Action|               4.0|   1|\n",
      "|1916|   Sci-Fi|               4.0|   1|\n",
      "|1917|    Drama|               4.0|   1|\n",
      "|1918|      War|               3.0|   1|\n",
      "|1918|   Comedy|               3.0|   1|\n",
      "|1919|  Romance|               4.5|   1|\n",
      "|1919|   Comedy|               4.0|   2|\n",
      "|1919|    Drama|               3.8|   3|\n",
      "|1920|    Drama|               4.0|   1|\n",
      "|1920|   Sci-Fi|               4.0|   1|\n",
      "|1920|   Horror|3.4285714285714284|   3|\n",
      "|1921|    Drama|3.7777777777777777|   1|\n",
      "|1921|   Comedy| 3.772727272727273|   2|\n",
      "+----+---------+------------------+----+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question 6: What words of the titles cooccure the most with each genre ? Is the number of cooccurence enough ? Compute the [pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) between genres and movie title words, and filter out words that appear fewer than 100 times."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "a64f1b52-66c6-473b-8fec-dbb140fb002e"
    },
    "id": "clOWC40XxhS9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import split, explode, size\n",
    "\n",
    "movies_with_words = movies_df.withColumn(\"word\", explode(split(\"title\", \"\\\\s+\")))\n",
    "genre_word = exploded_genres.join(movies_with_words, \"movieId\")\n",
    "\n",
    "word_genre_count = genre_word.groupBy(\"genre\", \"word\").count()\n",
    "filtered_word_genre_count = word_genre_count.filter(\"count >= 100\")\n",
    "total_count = filtered_word_genre_count.groupBy().sum(\"count\").first()[0]\n",
    "pmi = filtered_word_genre_count.withColumn(\"pmi\", F.log2((F.col(\"count\") / total_count) /\n",
    "                                                         (F.sum(\"count\").over(\n",
    "                                                             Window.partitionBy(\"genre\")) / total_count) /\n",
    "                                                         (F.sum(\"count\").over(\n",
    "                                                             Window.partitionBy(\"word\")) / total_count)))\n",
    "pmi.show()"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "title": "",
     "showTitle": false,
     "inputWidgets": {},
     "nuid": "ac8caaf4-daf0-4191-b7bd-bdca46fe45bc"
    },
    "id": "WuLPTvFexhS-",
    "ExecuteTime": {
     "end_time": "2023-12-21T17:54:28.200736Z",
     "start_time": "2023-12-21T17:54:26.480603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+------------------+\n",
      "| genre|  word|count|               pmi|\n",
      "+------+------+-----+------------------+\n",
      "|Comedy|     &|  134|1.4457503875762543|\n",
      "| Drama|     &|  111|0.3952230392473445|\n",
      "| Drama|(1970)|  116|1.5374451118998091|\n",
      "| Drama|(1971)|  102|1.5374451118998091|\n",
      "| Drama|(1972)|  111|1.5374451118998091|\n",
      "| Drama|(1982)|  106|1.5374451118998091|\n",
      "| Drama|(1984)|  114|1.5374451118998091|\n",
      "| Drama|(1985)|  103|1.5374451118998096|\n",
      "| Drama|(1986)|  117|1.5374451118998091|\n",
      "|Comedy|(1987)|  131|1.3162991361210525|\n",
      "| Drama|(1987)|  131|0.5374451118998093|\n",
      "|Comedy|(1988)|  111|1.1623477708864811|\n",
      "| Drama|(1988)|  136|0.6765407215654711|\n",
      "|Comedy|(1989)|  119|1.2922536621544014|\n",
      "| Drama|(1989)|  123|0.5610963799644545|\n",
      "|Comedy|(1990)|  109|1.1072035373980624|\n",
      "| Drama|(1990)|  143|0.7200365251782824|\n",
      "|Comedy|(1991)|  125|1.1946205795328002|\n",
      "| Drama|(1991)|  147|0.6496546154858341|\n",
      "|Comedy|(1992)|  135|1.2132056431569491|\n",
      "+------+------+-----+------------------+\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "spark-dataset-problems-databricks",
   "dashboards": [],
   "language": "python",
   "widgets": {},
   "notebookOrigID": 406204942312403
  },
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
