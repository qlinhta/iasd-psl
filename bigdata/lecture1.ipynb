{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Big Data\n",
    "\n",
    "Date: September 26, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c92dcb3bd850cc05"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-26T14:20:18.806076Z",
     "start_time": "2023-09-26T14:20:18.026302Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3V of big data:\n",
    "- Volume: It is the amount of data that is generated from different sources like social media, machines, internet, etc. It is the size of data that determines the value and potential of the data under consideration and whether it can actually be considered as Big Data or not.\n",
    "- Velocity: For Big Data, velocity is the rate at which data is generated and the speed at which the data moves from one point to another. The flow of data is massive and continuous.\n",
    "- Variety: It refers to the many types of data that are available. It can be in the form of structured, unstructured, and semi-structured form. Structured data is organized and is typically found in databases, while unstructured data is chaotic and raw that cannot be organized or easily interpreted. Semi-structured data is a combination of both structured and unstructured data.\n",
    "\n",
    "Example of structure, unstructured and semi-structured data:\n",
    "- Structured data (Schema first): Relational database\n",
    "- Unstructured data (Non-schema): Text, images, video, audio\n",
    "- Semi-structured data (Data first): XML, JSON, HTML\n",
    "\n",
    "BATCH vs STREAMING\n",
    "\n",
    "Batch processing is a method of running high-volume data that is collected over a period of time. Batch processing is used for tasks that require processing large volumes of data where a group of transactions is collected over a period of time. Data is collected, entered, processed and then the batch results are produced (Hadoop, Spark, Hive, Pig, etc.).\n",
    "\n",
    "Streaming data is data that is generated continuously by thousands of data sources, which typically send in the data records simultaneously, and in small sizes (order of Kilobytes). Streaming data includes a wide variety of data such as log files generated by customers using mobile or web applications, ecommerce purchases, in-game player activity, information from social networks, financial trading floors, or geospatial services, and telemetry from connected devices or instrumentation in data centers.\n",
    "\n",
    "Robustness: The system should be able to recover from failure. Moreover, it should be able to process data that is out of order or arrive late."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a609417cbc9e3b75"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "L = [2, 3, 1, 2, 1, 3, 4, 5, 6, 7]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T14:20:18.810123Z",
     "start_time": "2023-09-26T14:20:18.758511Z"
    }
   },
   "id": "a81f7bccbf2eaea1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "M = reduce(lambda x, y: x + y, list(map(lambda x: 1 if x % 2 == 0 else 0, L)))\n",
    "M1 = reduce(lambda x, y: x + y, list(map(lambda x: 1 if x % 2 != 0 else 0, L)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T14:20:18.810436Z",
     "start_time": "2023-09-26T14:20:18.773640Z"
    }
   },
   "id": "99c43f2b845130aa"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T14:20:18.811276Z",
     "start_time": "2023-09-26T14:20:18.780323Z"
    }
   },
   "id": "2ebf22be348eaf7b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T14:20:18.811663Z",
     "start_time": "2023-09-26T14:20:18.784667Z"
    }
   },
   "id": "b565d7497601e51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "P_{n, s} = \\mathbf{A}_{n, m} \\cdot \\mathbf{B}_{m, s}\n",
    "= \\begin{bmatrix}\n",
    "    a_{11} & a_{12} & \\cdots & a_{1m} \\\\\n",
    "    a_{21} & a_{22} & \\cdots & a_{2m} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{n1} & a_{n2} & \\cdots & a_{nm}\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "    b_{11} & b_{12} & \\cdots & b_{1s} \\\\\n",
    "    b_{21} & b_{22} & \\cdots & b_{2s} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    b_{m1} & b_{m2} & \\cdots & b_{ms}\n",
    "  \\end{bmatrix}\n",
    "  \n",
    "= P_{i, j} = \\sum_{k=1}^m a_{ik} b_{kj}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "945a297512ad452"
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$ \n",
    "\\mathbf{A} (i, k, a) \\rightarrow \\mathbf{A} (row, column, value) \\\\\n",
    "\\mathbf{B} (k, j, b) \\rightarrow \\mathbf{B} (row, column, value) \\\\\n",
    "\\mathbf{P} (i, j, p) \\rightarrow \\mathbf{P} (row, column, value) \\\\\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f057f1e4794e5bc8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In SQL, we can use the following query to calculate the matrix multiplication:\n",
    "```sql\n",
    "SELECT A.row, B.column, SUM(A.value * B.value) AS value\n",
    "FROM A, B\n",
    "WHERE A.column = B.row\n",
    "GROUP BY A.row, B.column\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e62c0404f0a420f9"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "A = [[1, 2, 3], [4, 5, 6]]\n",
    "B = [[1, 2], [3, 4], [5, 6]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-26T14:21:55.240055Z",
     "start_time": "2023-09-26T14:21:55.238282Z"
    }
   },
   "id": "3f0dcad6359fc3cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure (or method), which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd6455ea370c671e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MapReduce\n",
    "\n",
    "- Data model: key-value pairs $(k, v)$\n",
    "- Execution model: $map(k, udf) \\rightarrow m(k, v) \\rightarrow [(), (), ...]$, $reduce(k, [v_1, v_2, ...]) \\rightarrow [(), (), ...]$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57edd13264a7d656"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SPARK"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f58b39663f64b2cc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m316.9/316.9 MB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:02\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting py4j==0.10.9.7 (from pyspark)\r\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m200.5/200.5 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425347 sha256=d67954f814dae4d284a580410f3359b2c42d95bfd5aa9ad1a1e86fb6af40362a\r\n",
      "  Stored in directory: /Users/quyenlinhta/Library/Caches/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: py4j, pyspark\r\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# ! pip install pyspark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:28:44.750035Z",
     "start_time": "2023-11-06T08:27:18.534151Z"
    }
   },
   "id": "e976755ccc5d5ba7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/06 09:30:23 WARN Utils: Your hostname, Beta.local resolves to a loopback address: 127.0.0.1; using 10.113.217.28 instead (on interface en0)\n",
      "23/11/06 09:30:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/06 09:30:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:30:24.770499Z",
     "start_time": "2023-11-06T08:30:21.760753Z"
    }
   },
   "id": "c58ffa6b1424934a"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<SparkContext master=local[*] appName=pyspark-shell>",
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.113.217.28:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:30:28.312894Z",
     "start_time": "2023-11-06T08:30:28.250037Z"
    }
   },
   "id": "609520a3468ca23b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "list_of_number = [2, 4, 3, 2, 5, 5, 3, 5, 2, 6, 8, 3, 9, 9, 2, 3, 6, 4, 7, 9, 8]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:34:39.069873Z",
     "start_time": "2023-11-06T08:34:39.060572Z"
    }
   },
   "id": "b8e0c3a9eda0cfff"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 3, 2, 5, 5, 3, 5, 2, 6, 8, 3, 9, 9, 2, 3, 6, 4, 7, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "print(list_of_number)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:34:42.330686Z",
     "start_time": "2023-11-06T08:34:42.320920Z"
    }
   },
   "id": "116d4df3bf34ce5b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(list_of_number)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:35:06.079358Z",
     "start_time": "2023-11-06T08:35:05.902305Z"
    }
   },
   "id": "c78603c50611be1a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:35:07.898944Z",
     "start_time": "2023-11-06T08:35:07.867963Z"
    }
   },
   "id": "4a2a58175d237094"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD:  [2, 4, 3, 2, 5, 5, 3, 5, 2, 6, 8, 3, 9, 9, 2, 3, 6, 4, 7, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "# collect() is an action\n",
    "print(\"RDD: \", rdd.collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:42:15.057189Z",
     "start_time": "2023-11-06T08:42:15.034280Z"
    }
   },
   "id": "5dcfb753c465eabc"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: \", rdd.getNumPartitions())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:42:16.576096Z",
     "start_time": "2023-11-06T08:42:16.569478Z"
    }
   },
   "id": "98e5e8605df5594"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions structure:  [[2, 4], [3, 2], [5, 5, 3, 5], [2, 6], [8, 3], [9, 9, 2, 3], [6, 4], [7, 9, 8]]\n"
     ]
    }
   ],
   "source": [
    "# glom() is an transformation\n",
    "print(\"Partitions structure: \", rdd.glom().collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:42:16.963923Z",
     "start_time": "2023-11-06T08:42:16.805694Z"
    }
   },
   "id": "91479869759527f5"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plus one:  [3, 5, 4, 3, 6, 6, 4, 6, 3, 7, 9, 4, 10, 10, 3, 4, 7, 5, 8, 10, 9]\n",
      "Structure:  [[3, 5], [4, 3], [6, 6, 4, 6], [3, 7], [9, 4], [10, 10, 3, 4], [7, 5], [8, 10, 9]]\n"
     ]
    }
   ],
   "source": [
    "# map() is an transformation\n",
    "plus_one = rdd.map(lambda x: x + 1).collect()\n",
    "print(\"Plus one: \", plus_one)\n",
    "print(\"Structure: \", rdd.map(lambda x: x + 1).glom().collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:46:34.269201Z",
     "start_time": "2023-11-06T08:46:33.983827Z"
    }
   },
   "id": "f5e30ced2738689a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even:  [2, 4, 2, 2, 6, 8, 2, 6, 4, 8]\n",
      "Structure:  [[2, 4], [2], [], [2, 6], [8], [2], [6, 4], [8]]\n"
     ]
    }
   ],
   "source": [
    "# filter() is an transformation\n",
    "even = rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "print(\"Even: \", even)\n",
    "print(\"Structure: \", rdd.filter(lambda x: x % 2 == 0).glom().collect())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:46:49.022669Z",
     "start_time": "2023-11-06T08:46:48.752299Z"
    }
   },
   "id": "445fcc8845452671"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map:  [[3, 5], [4, 3], [6, 6, 4, 6], [3, 7], [9, 4], [10, 10, 3, 4], [7, 5], [8, 10, 9]]\n",
      "Map and reduce:  126\n"
     ]
    }
   ],
   "source": [
    "# map() and reduce() is an action which is not required shuffle operation\n",
    "print(\"Map: \", rdd.map(lambda x: x + 1).glom().collect())\n",
    "print(\"Map and reduce: \", rdd.map(lambda x: x + 1).reduce(lambda x, y: x + y))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:54:04.369024Z",
     "start_time": "2023-11-06T08:54:04.095295Z"
    }
   },
   "id": "e97d3edd175bc50e"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "assert rdd.map(lambda x: x + 1).reduce(lambda x, y: x + y) == sum(plus_one)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T08:58:46.012517Z",
     "start_time": "2023-11-06T08:58:45.868271Z"
    }
   },
   "id": "7728928fdddc7a76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d9b3bdb6c3806f04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
