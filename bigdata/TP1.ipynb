{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:34:58.953670Z",
     "start_time": "2023-10-03T12:34:58.929738Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def simple_list_search():\n",
    "    I = [random.randint(1, 1000000) for _ in range(900000)]\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(5):\n",
    "        num = int(input(\"Enter an integer number: \"))\n",
    "        print(\"YES\" if num in I else \"NO\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken for simple list: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "def partitioned_list_search():\n",
    "    partitions = 900\n",
    "    D = {}\n",
    "\n",
    "    for _ in range(900000):\n",
    "        num = random.randint(1, 1000000)\n",
    "        key = num % partitions\n",
    "        D.setdefault(key, []).append(num)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(5):\n",
    "        num = int(input(\"Enter an integer number: \"))\n",
    "        key = num % partitions\n",
    "        print(\"YES\" if key in D and num in D[key] else \"NO\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken for partitioned list: {end_time - start_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:34:58.957291Z",
     "start_time": "2023-10-03T12:34:58.937088Z"
    }
   },
   "id": "fcd437fc6e6d8b63"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using simple list:\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "Time taken for simple list: 5.17 seconds\n",
      "\n",
      "Using partitioned list:\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "YES\n",
      "NO\n",
      "Time taken for partitioned list: 5.53 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Using simple list:\")\n",
    "simple_list_search()\n",
    "\n",
    "print(\"\\nUsing partitioned list:\")\n",
    "partitioned_list_search()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:35:10.604346Z",
     "start_time": "2023-10-03T12:34:58.996097Z"
    }
   },
   "id": "3899447f01725cbe"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def generate_couples():\n",
    "    return [(random.randint(1, 7), random.randint(1, 1000)) for _ in range(30000000)]\n",
    "\n",
    "def groupAndSum(C):\n",
    "    sums = [0] * 7\n",
    "    for k, v in C:\n",
    "        sums[k-1] += v\n",
    "    return [(i+1, s) for i, s in enumerate(sums)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:35:10.604781Z",
     "start_time": "2023-10-03T12:35:10.600604Z"
    }
   },
   "id": "bf0368e06605242e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The complexity of this function is linear, $\\mathcal{O}(n)$. For every tuple in the list of $n$ tuples, it adds the value to the corresponding index in the sums list. It does not involve nested loops, which would imply a quadratic or higher complexity.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cc6c8b45d605b1d"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Execution Time: 3.42 seconds\n"
     ]
    }
   ],
   "source": [
    "C = generate_couples()\n",
    "start_time = time.time()\n",
    "groupAndSum(C)\n",
    "end_time = time.time()\n",
    "naive_time = end_time - start_time\n",
    "print(f\"Naive Execution Time: {naive_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:35:41.673862Z",
     "start_time": "2023-10-03T12:35:10.618236Z"
    }
   },
   "id": "7191a743ac486fd7"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def groupSortedAndSum(C):\n",
    "    C.sort(key=lambda x: x[0])\n",
    "    sums = [0] * 7\n",
    "    for k, v in C:\n",
    "        sums[k-1] += v\n",
    "    return [(i+1, s) for i, s in enumerate(sums)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:35:41.705205Z",
     "start_time": "2023-10-03T12:35:41.674612Z"
    }
   },
   "id": "51b13719c4b803df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The sort function in Python has a time complexity of $\\mathcal{O}(n \\times log(n))$, which is dominated by the linear-time grouping operation, making the total time complexity $\\mathcal{O}(n \\times log(n) + n)$, which simplifies to $\\mathcal{O}(n \\times log(n))$. So, while this is more optimized than a quadratic approach, it's still slower than the linear-time naive approach."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "633e0d31bebed33"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Execution Time: 6.82 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "groupSortedAndSum(C)\n",
    "end_time = time.time()\n",
    "sorted_time = end_time - start_time\n",
    "print(f\"Sorted Execution Time: {sorted_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:35:48.506922Z",
     "start_time": "2023-10-03T12:35:41.740497Z"
    }
   },
   "id": "4beaae2e1e7eb122"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def partition_and_group(C):\n",
    "    partitions = {i: [] for i in range(1, 8)}\n",
    "    for k, v in C:\n",
    "        partitions[k].append(v)\n",
    "    return [(k, sum(partitions[k])) for k in partitions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:35:48.513691Z",
     "start_time": "2023-10-03T12:35:48.507864Z"
    }
   },
   "id": "a491d126abc3319c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The partitioning process has a linear time complexity of $\\mathcal{O}(n)$ since we iterate through the list and append to the appropriate partition list. Summing each partition list also has a linear time complexity. Hence, the total time complexity remains linear $\\mathcal{O}(n)$, but the partitioning method is more memory-efficient than sorting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b76f9cc0269de35c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned Execution Time: 3.39 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "partition_and_group(C)\n",
    "end_time = time.time()\n",
    "partitioned_time = end_time - start_time\n",
    "print(f\"Partitioned Execution Time: {partitioned_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:35:51.911234Z",
     "start_time": "2023-10-03T12:35:48.518579Z"
    }
   },
   "id": "9687de55634950cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The naive method's execution time is $O(n)$.\n",
    "- Sorting the $(k,v)$ couples helps improve the execution time compared to quadratic methods, but remains slower than linear methods. Its advantage is in scenarios requiring further operations post-grouping.\n",
    "- The partitioning approach offers benefits in execution time and memory efficiency, especially when the range of $k$ values is limited.\n",
    "- Parallel processing: Partitioning is conducive to parallel processing. By distributing each partition to separate threads or processors, concurrent processing is achievable, especially beneficial for larger datasets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76aa750a19e10432"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def n_way_ms(lists):\n",
    "    if not lists:\n",
    "        return []\n",
    "\n",
    "    min_heap = []\n",
    "    # Push the first element of each list onto the heap.\n",
    "    for i in range(len(lists)):\n",
    "        if lists[i]:\n",
    "            heapq.heappush(min_heap, (lists[i][0], i, 0))\n",
    "            lists[i].pop(0)\n",
    "\n",
    "    merged_list = []\n",
    "    while min_heap:\n",
    "        val, list_idx, element_idx = heapq.heappop(min_heap)\n",
    "        merged_list.append(val)\n",
    "\n",
    "        if len(lists[list_idx]) > 0:\n",
    "            next_tuple = (lists[list_idx][0], list_idx, element_idx + 1)\n",
    "            heapq.heappush(min_heap, next_tuple)\n",
    "            lists[list_idx].pop(0)\n",
    "\n",
    "    return merged_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:35:51.911672Z",
     "start_time": "2023-10-03T12:35:51.908588Z"
    }
   },
   "id": "3f9c6e26327c2269"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pushing the first element from each of the n lists onto the heap takes $\\mathcal{O}(n \\times log(n))$.\n",
    "Each pop operation, followed by a push operation, takes $\\mathcal{O}(log(n))$. Since we have to perform this operation for each of the total elements across all the lists, let's say the total number of elements is m, the overall complexity would be $\\mathcal{O}(m \\times log(n))$.\n",
    "Therefore, the overall time complexity is $\\mathcal{O}(n \\times log(n) + m \\times log(n))$.\n",
    "\n",
    "More efficient we can actually do:\n",
    "\n",
    "The provided solution is already efficient given the constraints. The reason being, merging n sorted lists requires looking at each list at least once to place its elements in the correct position. Using a priority queue ensures we're always choosing the smallest available element from the lists in $\\mathcal{O}(n \\times log(n))$ time.\n",
    "However, in terms of space complexity, other implementations that involve dividing and conquering the lists (merging them in pairs) might be more space-efficient as they wouldn't need to maintain a heap with an entry for each list."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c51c17ae7c4c74f"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def count_words(T):\n",
    "    word_count = {}\n",
    "    for line in T:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "    return list(word_count.items())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T12:38:31.721286Z",
     "start_time": "2023-10-03T12:38:31.714052Z"
    }
   },
   "id": "e0d5c37b6efefb86"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 5.698204040527344e-05 seconds\n",
      "[('aa,', 2), ('bb,', 1), ('cc,', 3), ('dd,', 3), ('b,', 2), ('a,', 1), ('ff,', 1), ('gg', 1)]\n"
     ]
    }
   ],
   "source": [
    "T = [\"aa, bb, cc, dd, aa, b, cc, dd, a, b, cc, dd, ff, gg\"]\n",
    "start = time.time()\n",
    "word_count = count_words(T)\n",
    "end = time.time()\n",
    "print(f\"Execution Time: {end - start} seconds\")\n",
    "print(word_count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T13:39:50.014203Z",
     "start_time": "2023-10-03T13:39:50.010880Z"
    }
   },
   "id": "86d20d7ab9f07bf8"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def map_words(T):\n",
    "    for line in T:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            yield (word, 1)\n",
    "\n",
    "\n",
    "def reduce_words(word_count):\n",
    "    word_count = list(word_count)\n",
    "    word_count.sort(key=lambda x: x[0])  # Sort by word\n",
    "    word_count = [(k, sum([v[1] for v in g])) for k, g in\n",
    "                  itertools.groupby(word_count, key=lambda x: x[0])]\n",
    "    return word_count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T13:39:50.301528Z",
     "start_time": "2023-10-03T13:39:50.295857Z"
    }
   },
   "id": "8a2c6cf13b0c72bb"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 4.887580871582031e-05 seconds\n",
      "[('a,', 1), ('aa,', 2), ('b,', 2), ('bb,', 1), ('cc,', 3), ('dd,', 3), ('ff,', 1), ('gg', 1)]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "word_count = map_words(T)\n",
    "word_count = reduce_words(word_count)\n",
    "end = time.time()\n",
    "print(f\"Execution Time: {end - start} seconds\")\n",
    "print(word_count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T13:39:50.735603Z",
     "start_time": "2023-10-03T13:39:50.732570Z"
    }
   },
   "id": "4befa239984295d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Improving Execution Time for Large T:\n",
    "\n",
    "- Partitioning: If T is large, it can be partitioned into smaller chunks. Each chunk can be processed separately to generate word counts, and the results can then be merged. This is similar to the MapReduce model where the Map phase processes chunks of data to produce intermediate key-value pairs (in this case, word-count pairs) and the Reduce phase aggregates these intermediate results.\n",
    "- Sorting: Once the words are extracted, they can be sorted, which will place the same words together. A single pass can then be used to count occurrences of each word. However, this approach might not be as efficient as the simple counting method for this specific problem.\n",
    "- Parallel Processing: By using techniques like multi-threading or distributed processing, different portions of T can be processed concurrently, greatly improving execution times, especially for very large datasets.\n",
    "In essence, techniques like partitioning and parallel processing, inspired by the MapReduce model, can indeed help in improving execution times for large collections of text lines."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7217249d07618d6f"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-03T13:39:53.214316Z",
     "start_time": "2023-10-03T13:39:53.197374Z"
    }
   },
   "id": "a9fdc97fe9cda779"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
