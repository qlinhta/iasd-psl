\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\pgfsyspdfmark {pgfid3}{34664399}{2797020}
\pgfsyspdfmark {pgfid2}{4736286}{47362867}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\pgfsyspdfmark {pgfid5}{34664399}{2797020}
\pgfsyspdfmark {pgfid4}{4736286}{47362867}
\@writefile{toc}{\contentsline {paragraph}{Machine Learning}{2}{paragraph*.1}\protected@file@percent }
\pgfsyspdfmark {pgfid11}{34664399}{2797020}
\pgfsyspdfmark {pgfid10}{4736286}{47362867}
\@writefile{toc}{\contentsline {paragraph}{Fundamentals of Backpropagation in Deep Learning}{3}{paragraph*.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces We can see that the learning rule of neurons in deep learning relies on the backpropagation of the gradient, which adjusts the weights between neurons so that the network can learn to optimally represent complex relationships between data by minimizing a predefined cost function. This learning rule allows neural networks to adapt to the underlying patterns in the data and generalize their knowledge to make accurate predictions on new, unseen data.\relax }}{3}{figure.caption.3}\protected@file@percent }
\pgfsyspdfmark {pgfid14}{34664399}{2797020}
\pgfsyspdfmark {pgfid13}{4736286}{47362867}
\@writefile{toc}{\contentsline {paragraph}{Machine Learning Paradigms}{4}{paragraph*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning}{4}{section.2}\protected@file@percent }
\pgfsyspdfmark {pgfid16}{34664399}{2797020}
\pgfsyspdfmark {pgfid15}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Probabilistic Assumptions}{5}{subsection.2.1}\protected@file@percent }
\pgfsyspdfmark {pgfid18}{34664399}{2797020}
\pgfsyspdfmark {pgfid17}{4736286}{47362867}
\pgfsyspdfmark {pgfid20}{34664399}{2797020}
\pgfsyspdfmark {pgfid19}{4736286}{47362867}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces For simply imagine, in this figure, we denote $h^*$ as the best function we could choose from the hypothesis space $\mathcal  {H}$. The risk at $h^*$ is noted $\inf _{h \in \mathcal  {H}} R(h)$. The true risk $\inf _{f} R(f)$ found outside $\mathcal  {H}$.\relax }}{7}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:risk_class}{{2}{7}{For simply imagine, in this figure, we denote $h^*$ as the best function we could choose from the hypothesis space $\mathcal {H}$. The risk at $h^*$ is noted $\inf _{h \in \mathcal {H}} R(h)$. The true risk $\inf _{f} R(f)$ found outside $\mathcal {H}$.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Key Questions}{7}{subsection.2.2}\protected@file@percent }
\pgfsyspdfmark {pgfid24}{34664399}{2797020}
\pgfsyspdfmark {pgfid23}{4736286}{47362867}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Key Issues}{8}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Graph illustrating the trade-off between Estimation and Approximation Loss as model complexity increases. The blue curve (Approximation Loss) shows that as the model becomes more complex, it better approximates the underlying data distribution, hence the error decreases. However, after a certain point, the error plateaus, indicating that increasing complexity doesn't provide significant benefits. The red curve (Estimation Loss) indicates that simpler models generalize better, but as complexity increases, the models tend to overfit, leading to higher estimation errors. The optimal model complexity lies where the sum of these two errors is minimized.\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:est-app}{{3}{8}{Graph illustrating the trade-off between Estimation and Approximation Loss as model complexity increases. The blue curve (Approximation Loss) shows that as the model becomes more complex, it better approximates the underlying data distribution, hence the error decreases. However, after a certain point, the error plateaus, indicating that increasing complexity doesn't provide significant benefits. The red curve (Estimation Loss) indicates that simpler models generalize better, but as complexity increases, the models tend to overfit, leading to higher estimation errors. The optimal model complexity lies where the sum of these two errors is minimized.\relax }{figure.caption.6}{}}
\gdef \@abspage@last{8}
