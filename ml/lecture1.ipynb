{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Foundations of Machine Learning\n",
    "\n",
    "Date: September 21, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e585ca81ba4a4495"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-21T10:19:53.934554Z",
     "start_time": "2023-09-21T10:19:53.928889Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chapter 1: Probabilistic formulations of prediction problems (Supervised Learning)\n",
    "(a.k.a Statistical Learning Theory)\n",
    "\n",
    "#### Goal: \n",
    "Predict the outcome of $y$ from set $\\mathcal{Y}$ of possible outcomes on the basis of some observations $x$ from a feater space $\\mathcal{X}$.\n",
    "\n",
    "Example:\n",
    "$x$ : word in a document, image\n",
    "$y$ : category of document, image\n",
    "\n",
    "Denote $h(x)$ as the prediction of $y$ from $x$.\n",
    "\n",
    "Using dataset of $n$ pairs:\n",
    "$$\n",
    "(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\n",
    "$$\n",
    "to choose a function (hypothesis) $h : \\mathcal{X} \\rightarrow \\mathcal{Y}$ so that, for a new pair $(x, y)$, $h(x)$ is a good predictor of $y$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7434a0babf4c2ded"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To define the notion of a GOOD prediction, we can define a loss function $l : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ that measures the loss of predicting $y$ when the true outcome is $y$.\n",
    "\n",
    "So $l(y, h(x))$ quantifies the cost of predicting $h(x)$ when the true outcome is $y$, then we ensure that $l(y, h(x))$ is small.\n",
    "\n",
    "##### Example:\n",
    "In the pattern classification (binary classification) we could define the loss function as:\n",
    "$$\n",
    "l(y, h(x)) = \\begin{cases}\n",
    "0 & \\text{ if } y = h(x) \\\\\n",
    "1 & \\text{ if } y \\neq h(x)\n",
    "\\end{cases}\n",
    "$$\n",
    "For regression problems, wiht $\\mathcal{Y} = \\mathbb{R}$, we could define the loss function as:\n",
    "$$\n",
    "l(y, h(x)) = (y - h(x))^2\n",
    "$$\n",
    "Also known as the quadratic loss function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59db534b2187c537"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Probabilistic assumptions\n",
    "\n",
    "Assumes:\n",
    "- There is a probability distribution $P$ on $\\mathcal{X} \\times \\mathcal{Y}$.\n",
    "- The pair $(\\mathcal{X}, \\mathcal{Y}), \\dots (\\mathcal{X}_n, \\mathcal{Y}_n)$ are i.i.d. samples from $P$.\n",
    "\n",
    "The aime is to choose a function $h : \\mathcal{X} \\rightarrow \\mathcal{Y}$ with small risk:\n",
    "$$\n",
    "R(h) = \\mathbb{E}_{(x, y) \\sim P}[l(y, h(x))]\n",
    "$$\n",
    "where the expectation is taken with respect to the distribution $P$.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{(x, y) \\sim P}[l(y, h(x))] = \\int_{\\mathcal{X} \\times \\mathcal{Y}} l(y, h(x)) dP(x, y) \\\\\n",
    "= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} l(y, h(x)) dP(y|x) dP(x) \\\\\n",
    "= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} l(y, h(x)) P(y|x) dP(x) dy \\\\\n",
    "= \\int_{\\mathcal{X}} \\mathbb{E}_{y \\sim P(y|x)}[l(y, h(x))] dP(x) \\\\\n",
    "= \\mathbb{E}_{x \\sim P(x)}[\\mathbb{E}_{y \\sim P(y|x)}[l(y, h(x))]] \\\\\n",
    "= \\mathbb{E}_{x \\sim P(x)}[R_x(h)]\n",
    "$$\n",
    "where $P(y|x)$ is the conditional probability of $y$ given $x$.\n",
    "\n",
    "For instance, in binary classification, this is the misclassification probability:\n",
    "$$\n",
    "R(h) = \\mathbb{E}_P[1_{y \\neq h(x)}] = \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} 1_{y \\neq h(x)} P(y|x) dP(x) dy\n",
    "$$\n",
    "where:\n",
    "- $1_{y \\neq h(x)}$ is the indicator function that is 1 if $y \\neq h(x)$ and 0 otherwise. \n",
    "- $\\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} 1_{y \\neq h(x)} P(y|x) dP(x) dy$ is the probability of misclassification."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2145b24fe84326e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Remarks:\n",
    "- Capital letters $X$ and $Y$ are used to denote random variables, while lower case letters $x$ and $y$ are used to denote their values.\n",
    "- $P$ models both the relative frequency of different features $\\mathcal{X}$ and the relationship between the features and the labels $\\mathcal{Y}$.\n",
    "- The assumption that the samples are i.i.d. is very strong\n",
    "- The function $x \\mapsto h_n(x : \\mathcal{X}_1, \\mathcal{Y}_1 \\dots, \\mathcal{X}_n, \\mathcal{Y}_n$ is random, since it depends on the random samples $\\mathcal{D}_n = \\{(\\mathcal{X}_1, \\mathcal{Y}_1), \\dots, (\\mathcal{X}_n, \\mathcal{Y}_n)\\}$.\n",
    "\n",
    "Thus, the risk\n",
    "$$\n",
    "h_n = \\mathbb{E}_{P}[l(Y, h_n(X)) \\mid \\mathcal{D}_n] = \\mathbb{E}[l(\\mathcal{Y}_1, h_n(\\mathcal{X}_1)) + \\cdots + l(\\mathcal{Y}_n, h_n(\\mathcal{X}_n)) \\mid \\mathcal{D}_n]\n",
    "$$\n",
    "is a random variable. We might aime for $\\mathbb{E}[R(h_n)]$ to be small or for $R(h_n)$ to be small with high probability (over training sets $\\mathcal{D}_n$)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6156d98a692d736a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Key questions\n",
    "We might choose $h_n$ from some class $\\mathcal{H}$ of functions (such as (linear) classifiers, neural networks, decision trees, etc.). Question we are interested in:\n",
    "1. Can we design an algorithm for which $R(h_n)$ is closed to the best possible given that it was choosen from $\\mathcal{H}$? (that is $R(h_n) - \\inf_{h \\in \\mathcal{H}} R(h)$ is small)\n",
    "2. How does the performance of $h_n$ depend on the size of the training set $n$? On the complexity of the class $\\mathcal{H}$?\n",
    "3. Can we assure that $R(h_n)$ approaches the best performance possible: $\\inf_{f} R(f)$?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d660bb0ab17482b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Key issues\n",
    "\n",
    "**Approximation**: How good is the best possible $h$ in $\\mathcal{H}$?\n",
    "$$\n",
    "\\inf_{h \\in \\mathcal{H}} R(h) - \\inf_{f} R(f)\n",
    "$$\n",
    "\n",
    "**Estimation**: How close is our performance to the best possible $h$ in $\\mathcal{H}$?\n",
    "$$\n",
    "R(h_n) - \\inf_{h \\in \\mathcal{H}} R(h)\n",
    "$$\n",
    "\n",
    "**Computation**: We need to use data to choose $h_n$ from $\\mathcal{H}$, typically by slowing some kind of optimization problem. How can we do this efficiently?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cde2124e8845ebb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-21T10:19:53.936811Z",
     "start_time": "2023-09-21T10:19:53.932052Z"
    }
   },
   "id": "6ebd6a4f07d1b01b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
