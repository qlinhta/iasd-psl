{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Foundations of Machine Learning\n",
    "\n",
    "### Date: September 21, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e585ca81ba4a4495"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='sans-serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T09:03:56.308282Z",
     "start_time": "2023-12-14T09:03:55.705422Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chapter 1: Probabilistic formulations of prediction problems (Supervised Learning)\n",
    "(a.k.a Statistical Learning Theory)\n",
    "\n",
    "#### Goal: \n",
    "Predict the outcome of $y$ from set $\\mathcal{Y}$ of possible outcomes on the basis of some observations $x$ from a feater space $\\mathcal{X}$.\n",
    "\n",
    "Example:\n",
    "$x$ : word in a document, image\n",
    "$y$ : category of document, image\n",
    "\n",
    "Denote $h(x)$ as the prediction of $y$ from $x$.\n",
    "\n",
    "Using dataset of $n$ pairs:\n",
    "$$\n",
    "(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\n",
    "$$\n",
    "to choose a function (hypothesis) $h : \\mathcal{X} \\rightarrow \\mathcal{Y}$ so that, for a new pair $(x, y)$, $h(x)$ is a good predictor of $y$.\n",
    "To define the notion of a GOOD prediction, we can define a loss function $l : \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}$ that measures the loss of predicting $y$ when the true outcome is $y$.\n",
    "\n",
    "So $l(y, h(x))$ quantifies the cost of predicting $h(x)$ when the true outcome is $y$, then we ensure that $l(y, h(x))$ is small.\n",
    "\n",
    "##### Example:\n",
    "In the pattern classification (binary classification) we could define the loss function as:\n",
    "$$\n",
    "l(y, h(x)) = \\begin{cases}\n",
    "0 & \\text{ if } y = h(x) \\\\\n",
    "1 & \\text{ if } y \\neq h(x)\n",
    "\\end{cases}\n",
    "$$\n",
    "For regression problems, wiht $\\mathcal{Y} = \\mathbb{R}$, we could define the loss function as:\n",
    "$$\n",
    "l(y, h(x)) = (y - h(x))^2\n",
    "$$\n",
    "Also known as the quadratic loss function.\n",
    "### Probabilistic assumptions\n",
    "\n",
    "Assumes:\n",
    "- There is a probability distribution $P$ on $\\mathcal{X} \\times \\mathcal{Y}$.\n",
    "- The pair $(\\mathcal{X}, \\mathcal{Y}), \\dots (\\mathcal{X}_n, \\mathcal{Y}_n)$ are i.i.d. samples from $P$.\n",
    "\n",
    "The aime is to choose a function $h : \\mathcal{X} \\rightarrow \\mathcal{Y}$ with small risk:\n",
    "$$\n",
    "R(h) = \\mathbb{E}_{(x, y) \\sim P}[l(y, h(x))]\n",
    "$$\n",
    "where the expectation is taken with respect to the distribution $P$.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{(x, y) \\sim P}[l(y, h(x))] = \\int_{\\mathcal{X} \\times \\mathcal{Y}} l(y, h(x)) dP(x, y) \\\\\n",
    "= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} l(y, h(x)) dP(y|x) dP(x) \\\\\n",
    "= \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} l(y, h(x)) P(y|x) dP(x) dy \\\\\n",
    "= \\int_{\\mathcal{X}} \\mathbb{E}_{y \\sim P(y|x)}[l(y, h(x))] dP(x) \\\\\n",
    "= \\mathbb{E}_{x \\sim P(x)}[\\mathbb{E}_{y \\sim P(y|x)}[l(y, h(x))]] \\\\\n",
    "= \\mathbb{E}_{x \\sim P(x)}[R_x(h)]\n",
    "$$\n",
    "where $P(y|x)$ is the conditional probability of $y$ given $x$.\n",
    "\n",
    "For instance, in binary classification, this is the misclassification probability:\n",
    "$$\n",
    "R(h) = \\mathbb{E}_P[1_{y \\neq h(x)}] = \\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} 1_{y \\neq h(x)} P(y|x) dP(x) dy\n",
    "$$\n",
    "where:\n",
    "- $1_{y \\neq h(x)}$ is the indicator function that is 1 if $y \\neq h(x)$ and 0 otherwise. \n",
    "- $\\int_{\\mathcal{X}} \\int_{\\mathcal{Y}} 1_{y \\neq h(x)} P(y|x) dP(x) dy$ is the probability of misclassification.\n",
    "#### Remarks:\n",
    "- Capital letters $X$ and $Y$ are used to denote random variables, while lower case letters $x$ and $y$ are used to denote their values.\n",
    "- $P$ models both the relative frequency of different features $\\mathcal{X}$ and the relationship between the features and the labels $\\mathcal{Y}$.\n",
    "- The assumption that the samples are i.i.d. is very strong\n",
    "- The function $x \\mapsto h_n(x : \\mathcal{X}_1, \\mathcal{Y}_1 \\dots, \\mathcal{X}_n, \\mathcal{Y}_n$ is random, since it depends on the random samples $\\mathcal{D}_n = \\{(\\mathcal{X}_1, \\mathcal{Y}_1), \\dots, (\\mathcal{X}_n, \\mathcal{Y}_n)\\}$.\n",
    "\n",
    "Thus, the risk\n",
    "$$\n",
    "h_n = \\mathbb{E}_{P}[l(Y, h_n(X)) \\mid \\mathcal{D}_n] = \\mathbb{E}[l(\\mathcal{Y}_1, h_n(\\mathcal{X}_1)) + \\cdots + l(\\mathcal{Y}_n, h_n(\\mathcal{X}_n)) \\mid \\mathcal{D}_n]\n",
    "$$\n",
    "is a random variable. We might aime for $\\mathbb{E}[R(h_n)]$ to be small or for $R(h_n)$ to be small with high probability (over training sets $\\mathcal{D}_n$).\n",
    "### Key questions\n",
    "We might choose $h_n$ from some class $\\mathcal{H}$ of functions (such as (linear) classifiers, neural networks, decision trees, etc.). Question we are interested in:\n",
    "1. Can we design an algorithm for which $R(h_n)$ is closed to the best possible given that it was choosen from $\\mathcal{H}$? (that is $R(h_n) - \\inf_{h \\in \\mathcal{H}} R(h)$ is small)\n",
    "2. How does the performance of $h_n$ depend on the size of the training set $n$? On the complexity of the class $\\mathcal{H}$?\n",
    "3. Can we assure that $R(h_n)$ approaches the best performance possible: $\\inf_{f} R(f)$?\n",
    "### Key issues\n",
    "\n",
    "**Approximation**: How good is the best possible $h$ in $\\mathcal{H}$?\n",
    "$$\n",
    "\\inf_{h \\in \\mathcal{H}} R(h) - \\inf_{f} R(f)\n",
    "$$\n",
    "\n",
    "**Estimation**: How close is our performance to the best possible $h$ in $\\mathcal{H}$?\n",
    "$$\n",
    "R(h_n) - \\inf_{h \\in \\mathcal{H}} R(h)\n",
    "$$\n",
    "\n",
    "**Computation**: We need to use data to choose $h_n$ from $\\mathcal{H}$, typically by slowing some kind of optimization problem. How can we do this efficiently?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cde2124e8845ebb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 600x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_xlim(-1.05, 1.05)\n",
    "ax.set_ylim(-1.05, 1.05)\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(0, -0.24, marker='o', color='r', s=100)\n",
    "ax.scatter(0.5, 0, marker='v', color='b', s=100)\n",
    "ax.scatter(1, 0, marker='*', color='k', s=100)\n",
    "ax.add_artist(plt.Circle((0, 0), 0.5, color='green', fill=True, alpha=0.1, label='$\\mathcal{H}:$ Hypothesis Class'))\n",
    "ax.annotate('$h_n$', xy=(0, -0.25), xytext=(0, -0.35), ha='center', va='center')\n",
    "ax.annotate('$h^*$', xy=(0.5, 0), xytext=(0.45, 0.06), ha='center', va='center')\n",
    "ax.annotate('$f$', xy=(1, 0), xytext=(1.05, 0.07), ha='center', va='center')\n",
    "ax.text(0, 0, r'$\\mathcal{H}$', ha='center', va='center')\n",
    "ax.plot([0, 0.5], [-0.24, 0], '--', color='r', label='Estimation Error')\n",
    "ax.plot([0.5, 1], [0, 0], '--', color='b', label='Approximation Error')\n",
    "ax.legend(loc='upper right', fontsize=14)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "plt.savefig('scripts/figures/fig1.1.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T09:03:56.917373Z",
     "start_time": "2023-12-14T09:03:56.312703Z"
    }
   },
   "id": "15351b2d652fbb47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Date: September 28, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d47ae0180af6aec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have samples $S = \\{(x_1, y_1), \\dots, (x_n, y_n)\\}$ from a distribution $P$ on $\\mathcal{X} \\times \\mathcal{Y}$.\n",
    "\n",
    "The bad thing is that we could have bad luck and get a bad sample $S$. For example, if we have a sample of 1000 people and we want to predict the height of a person, we could have a sample of 1000 people that are all 2 meters tall. In this case, we would have a bad sample.\n",
    "\n",
    "Recall the **Approximation error**:\n",
    "$$\n",
    "\\inf_{h \\in \\mathcal{H}} R(h) - \\inf_{f} R(f)\n",
    "$$\n",
    "We find how close the best possible $h$ in $\\mathcal{H}$ is to the best possible $f$.\n",
    "\n",
    "We have $R(h) = \\mathbb{E}_{(x, y) \\sim P}[l(y, h(x))]$ and $R(f) = \\mathbb{E}_{(x, y) \\sim P}[l(y, f(x))]$.\n",
    "\n",
    "**Estimation error**:\n",
    "$$\n",
    "R(h_n) - \\inf_{h \\in \\mathcal{H}} R(h)\n",
    "$$\n",
    "For any $h$ how close is $R(h_n)$ to $R(h)$, where $R(h_n) = \\mathbb{E}_{(x, y) \\sim P}[l(y, h_n(x))] = \\frac{1}{n} \\sum_{i=1}^n l(y_i, h_n(x_i))$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7e6de46904bd552"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Excess risk**:\n",
    "$$\n",
    "R(h_n) - \\inf_{h \\in \\mathcal{H}} R(h) = \\underbrace{\\left(R(h_n) - \\mathbb{E}_{(x, y) \\sim P}[l(y, h_n(x))]\\right)}_{\\text{Estimation error}} + \\underbrace{\\left(\\mathbb{E}_{(x, y) \\sim P}[l(y, h_n(x))] - \\inf_{h \\in \\mathcal{H}} R(h)\\right)}_{\\text{Approximation error}}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27104b4743259383"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Pattern classification:**\n",
    "\n",
    "Consider two class classification with $\\mathcal{Y} = \\{-1, 1\\}$ and so the data is represented by $(x_1, y_1), \\dots, (x_n, y_n)$ where $x_i \\in \\mathcal{X}$ and $y_i \\in \\{-1, 1\\}$, distributed according to $P$ on $\\mathcal{X} \\times \\mathcal{Y}$ as the pair $(\\mu, \\eta)$ where $\\mu = (\\mu_{-1}, \\mu_1) \\in \\mathbb{R}^d \\times \\mathbb{R}^d$ and $\\eta \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "$\\mu$ is the marginal distribution of $x$ and $\\eta$ is the conditional distribution of $y$ given $x$.\n",
    "\n",
    "$$ \\eta(x) = \\mathbb{P}(y = 1 \\mid x) = \\frac{1}{1 + e^{-\\langle \\eta, x \\rangle}} = \\sigma(\\langle \\eta, x \\rangle)$$\n",
    "where $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ is the sigmoid function.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77b1b82ff43ec350"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Remark:**\n",
    "If we know $\\eta$ then we could use it to find a decision function $h$ that minimizes the risk.\n",
    "$$\n",
    "R(h) = \\mathbb{E}_{(x, y) \\sim P}[l(y, h(x))] \\\\\n",
    "= \\mathbb{E}_{x \\sim \\mu}[\\mathbb{E}_{y \\sim \\eta(x)}[l(y, h(x))]] \\\\\n",
    "= \\mathbb{E}_{x}[l(h(x), 1) \\times P(y = 1 \\mid x) + l(h(x), -1) \\times P(y = -1 \\mid x)] \\\\\n",
    "= \\mathbb{E}_{x}[l(h(x), 1) \\times \\eta(x) + l(h(x), -1) \\times (1 - \\eta(x))] \\\\\n",
    "= \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times \\eta(x) + \\mathbb{I}_{h(x) \\neq -1} \\times (1 - \\eta(x))] \\\\\n",
    "= \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times \\eta(x) + (1- \\mathbb{I}_{h(x) \\neq 1}) \\times (1 - \\eta(x))] \\\\\n",
    "= \\mathbb{E}_{x}[2 \\times \\mathbb{I}_{h(x) \\neq 1} \\times \\eta(x) + 1 - \\eta(x) - \\mathbb{I}_{h(x) \\neq 1}] \\\\\n",
    "= \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)] \\\\\n",
    "$$\n",
    "\n",
    "**Bayes Decision Rule:**\n",
    "This quantity is minimized by choosing $h = h^*$ where:\n",
    "$$\n",
    "h^*(x) = \\begin{cases}\n",
    "1 & \\text{ if } \\eta(x) \\geq \\frac{1}{2} \\\\\n",
    "-1 & \\text{ if } \\eta(x) < \\frac{1}{2}\n",
    "\\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "673105bd2c61e8d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exercise: Decompose the risk for quadratic loss in regression.\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(h) = \\mathbb{E}_{(x, y) \\sim P}[l(y, h(x))] \\\\\n",
    "l(y, h(x)) = (w^T x - y)^2 \\\\\n",
    "$$\n",
    "where $w \\in \\mathbb{R}^d$.\n",
    "\n",
    "Bias: $$Bias[\\theta] = \\mathbb{E}_{S}[\\hat{\\theta}] - \\theta$$\n",
    "Variances: $$Var[\\theta] = \\mathbb{E}[(\\mathbb{E}_{S}[\\hat{\\theta}] - \\theta)^2]$$\n",
    "\n",
    "$$\n",
    "(y - \\hat{y})^2 = (y - \\mathbb{E}_{S}[\\hat{y}] + \\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2 \\\\\n",
    "= (y - \\mathbb{E}_{S}[\\hat{y}])^2 + (\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2 + 2(y - \\mathbb{E}_{S}[\\hat{y}]) (\\mathbb{E}_{S}[\\hat{y}] - \\hat{y}) \\\\\n",
    "$$\n",
    "donc:\n",
    "$$\n",
    "\\mathbb{E}_{S}[(y - \\hat{y})^2] = \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])^2] + \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2] + 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}]) (\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})] \\\\\n",
    "= \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])^2] + \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2] + 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})] \\\\\n",
    "= \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])^2] + \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2]\n",
    "$$\n",
    "because $\\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] = 0$.\n",
    "To continue:\n",
    "$$\n",
    "\\mathbb{E}_{S}[(y - \\hat{y})^2] = \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])^2] + \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2] \\\\\n",
    "= \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])^2] + \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\mathbb{E}_{S}[\\hat{y}] + \\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2] \\\\\n",
    "= \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])^2] + \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\mathbb{E}_{S}[\\hat{y}])^2 + (\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2 + 2(y - \\mathbb{E}_{S}[\\hat{y}]) (\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})] \\\\\n",
    "= \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])^2] + \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2] + 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})] \\\\\n",
    "= \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])^2] + \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})^2]\n",
    "$$\n",
    "\n",
    "Prove that $2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})] = 0$:\n",
    "$$\n",
    "2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})] \\\\\n",
    "= 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] (\\mathbb{E}_{S}[\\hat{y}] - \\mathbb{E}_{S}[\\hat{y} + \\hat{y} - \\hat{y}]) \\\\\n",
    "= 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] (\\mathbb{E}_{S}[\\hat{y}] - \\mathbb{E}_{S}[\\hat{y}] + \\mathbb{E}_{S}[\\hat{y}] - \\hat{y}) \\\\\n",
    "= 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] (\\mathbb{E}_{S}[\\hat{y}] - \\hat{y}) \\\\\n",
    "= 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\mathbb{E}_{S}[\\hat{y}] - 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\hat{y} \\\\\n",
    "= 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\mathbb{E}_{S}[\\hat{y}] - 2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\mathbb{E}_{S}[\\hat{y}] \\\\\n",
    "$$\n",
    "So $2 \\mathbb{E}_{S}[(y - \\mathbb{E}_{S}[\\hat{y}])] \\mathbb{E}_{S}[(\\mathbb{E}_{S}[\\hat{y}] - \\hat{y})] = 0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "289a31344c0c0ddc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The optimal risk is called the **Bayes risk**, and is given by:\n",
    "$$\n",
    "R^* = \\inf_{f} R(f) = R(f^*)\n",
    "$$\n",
    "where $f^*$ is the Bayes decision rule."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e813494946c16ded"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Recal**: $R(h) = \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)]$\n",
    "$h = f^*$, so: \n",
    "$$\n",
    "R^* = \\begin{cases}\n",
    "\\mathbb{E}_{x}[\\mathbb{I}_{f^*(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)] & \\text{ if } \\eta(x) \\geq \\frac{1}{2} \\\\\n",
    "\\mathbb{E}_{x}[\\mathbb{I}_{f^*(x) \\neq -1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)] & \\text{ if } \\eta(x) < \\frac{1}{2}\n",
    "\\end{cases}\n",
    "= \\begin{cases}\n",
    "\\eta(x) & \\text{ if } \\eta(x) \\geq \\frac{1}{2} \\\\\n",
    "1 - \\eta(x) & \\text{ if } \\eta(x) < \\frac{1}{2}\n",
    "\\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "275cffb29075a898"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Risk and distance from $f^*$\n",
    "\n",
    "**Theorem**: For $h: \\mathcal{X} \\rightarrow \\mathcal{Y}$, we have:\n",
    "$$\n",
    "R(h) - R(f^*) = \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq f^*(x)} \\times (\\mid2 \\times \\eta(x) - 1\\mid)]\n",
    "$$\n",
    "\n",
    "Proof:\n",
    "$$\n",
    "R(h) - R(f^*) = \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)] - \\mathbb{E}_{x}[\\mathbb{I}_{f^*(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)] \\\\\n",
    "= \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1)] - \\mathbb{E}_{x}[\\mathbb{I}_{f^*(x) \\neq 1} \\times (2 \\times \\eta(x) - 1)] \\\\\n",
    "= \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) - \\mathbb{I}_{f^*(x) \\neq 1} \\times (2 \\times \\eta(x) - 1)] \\\\\n",
    "= \\mathbb{E}_{x}[(\\mathbb{I}_{h(x) \\neq 1} - \\mathbb{I}_{f^*(x) \\neq 1}) \\times (2 \\times \\eta(x) - 1)] \\\\\n",
    "= \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq f^*(x)} \\times (2 \\times \\eta(x) - 1)]\n",
    "$$\n",
    "Value absolute value of $2 \\times \\eta(x) - 1$ is the distance from $f^*$ because $f^*(x) = 1$ if $\\eta(x) \\geq \\frac{1}{2}$ and $f^*(x) = -1$ if $\\eta(x) < \\frac{1}{2}$.\n",
    "\n",
    "So we can write the risk as:\n",
    "$$\n",
    "R(h) - R(f^*) = \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq f^*(x)} \\times (\\mid2 \\times \\eta(x) - 1\\mid)]\n",
    "$$\n",
    "\n",
    "**Proof from Prof**:\n",
    "We have $R(h) = \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)]$ and $R(f^*) = \\mathbb{E}_{x}[\\mathbb{I}_{f^*(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)]$.\n",
    "Hence:\n",
    "$$\n",
    "R(h) - R(f^*) = \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)] - \\mathbb{E}_{x}[\\mathbb{I}_{f^*(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) + 1 - \\eta(x)] \\\\\n",
    "= \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1)] - \\mathbb{E}_{x}[\\mathbb{I}_{f^*(x) \\neq 1} \\times (2 \\times \\eta(x) - 1)] \\\\\n",
    "= \\mathbb{E}_{x}[\\mathbb{I}_{h(x) \\neq 1} \\times (2 \\times \\eta(x) - 1) - \\mathbb{I}_{f^*(x) \\neq 1} \\times (2 \\times \\eta(x) - 1)] \\\\\n",
    "= \\mathbb{E}_{x}[\\underbrace{(\\mathbb{I}_{h(x) \\neq 1} - \\mathbb{I}_{f^*(x) \\neq 1}) \\times (2 \\times \\eta(x) - 1)}_{A}] \\\\\n",
    "$$\n",
    "\n",
    "We can write $A$ as:\n",
    "$$\n",
    "A = \\mathbb{I}_{h(x) \\neq f^*(x)}(\\mathbb{I}_{h(x) \\neq 1} - \\mathbb{I}_{f^*(x) \\neq 1}) \\times (2 \\times \\eta(x) - 1) \\\\\n",
    "= \\begin{cases}\n",
    "\\mathbb{I}_{h(x) \\neq f^*(x)} \\times (2 \\times \\eta(x) - 1) & \\text{ if } \\eta(x) \\geq \\frac{1}{2} \\\\\n",
    "-\\mathbb{I}_{h(x) \\neq f^*(x)} \\times (2 \\times \\eta(x) - 1) & \\text{ if } \\eta(x) < \\frac{1}{2}\n",
    "\\end{cases}\n",
    "= \\mathbb{I}_{h(x) \\neq f^*(x)} \\times (\\mid2 \\times \\eta(x) - 1\\mid)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fabffca90d0a026f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Plug-in classifier**:\n",
    "Use the data to come up with an estimate $\\hat{\\eta}$ of $\\eta$ and then use the Bayes decision rule with $\\hat{\\eta}$ instead of $\\eta$.\n",
    "$$\n",
    "h_{\\hat{\\eta}}(x) = \\begin{cases}\n",
    "1 & \\text{ if } \\hat{\\eta}(x) \\geq \\frac{1}{2} \\\\\n",
    "-1 & \\text{ if } \\hat{\\eta}(x) < \\frac{1}{2}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In estimating $\\eta$, what criteria should we use? We could use the empirical risk minimization (ERM) criterion:\n",
    "$\\mathcal{L}_{\\mu}$ is the distance between $\\eta$ and $\\hat{\\eta}$:\n",
    "**Theorem**: For any $\\mu \\in \\mathbb{R}^d$ and $\\eta \\in \\mathbb{R}^{d \\times d}$, we have:\n",
    "$$\n",
    "R(h_{\\hat{\\eta}}) - R(f^*) \\leq \\mathcal{L}_{\\mu}(\\eta, \\hat{\\eta}) = 2 \\times \\mathbb{E}_{x \\sim \\mu}[\\mid \\eta(x) - \\hat{\\eta}(x) \\mid]\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f1dffd42e87d881"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Proof**:\n",
    "We can write that $R(h_{\\hat{\\eta}}) - R(f^*) = 2 \\times \\mathbb{E}_{x}[\\mathbb{I}_{h_{\\hat{\\eta}}(x) \\neq f^*(x)} \\times (\\mid \\eta(x) - \\frac{1}{2} \\mid)]$.\n",
    "\n",
    "See that $h_{\\hat{\\eta}}(x) \\neq f^*(x)$ if and only if $\\hat{\\eta}(x) \\neq \\eta(x)$ and $\\eta(x) \\geq \\frac{1}{2}$ or $\\hat{\\eta}(x) \\neq \\eta(x)$ and $\\eta(x) < \\frac{1}{2}$. They has to be different side of $\\frac{1}{2}$.\n",
    "So $|\\eta(x) - \\hat{\\eta}(x)| \\geq |\\eta(x) - \\frac{1}{2}|$.\n",
    "Then we have:\n",
    "$$\n",
    "R(h_{\\hat{\\eta}}) - R(f^*) = 2 \\times \\mathbb{E}_{x}[\\mathbb{I}_{h_{\\hat{\\eta}}(x) \\neq f^*(x)} \\times (\\mid \\eta(x) - \\frac{1}{2} \\mid)] \\\\\n",
    "\\leq 2|\\eta(x) - \\hat{\\eta}(x)| \\times \\mathbb{E}_{x}[\\mathbb{I}_{h_{\\hat{\\eta}}(x) \\neq f^*(x)}] \n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9252ecf2ae46e950"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### ESTIMATING $\\eta$ IS NOT NECESSARY\n",
    "\n",
    "Notice that estimating $\\eta$ accurately is not necessary for accurate classification. In particular, this bound a plugin classifier $h_{\\hat{\\eta}}$ can be very loose. For instance, if $\\eta(x) = \\{0, 1\\}$ then for any $\\epsilon > 0$, we can find $\\hat{\\eta}$ satisfying that:\n",
    "- $\\hat{\\eta}(x)$ and $\\eta(x)$ are always on the same side of $\\frac{1}{2}$. \n",
    "- $\\mathcal{L}_{\\mu}(\\eta, \\hat{\\eta}) = \\frac{1-\\epsilon}{2} \\text{Almost surely}$.\n",
    "\n",
    "So $R(h_{\\hat{\\eta}}) - R(f^*) = 0 \\ll 1 - \\epsilon = 2 \\times \\mathbb{E}_{x \\sim \\mu}[\\mid \\eta(x) - \\hat{\\eta}(x) \\mid]$.\n",
    "That is the bound might be vacuous even though the classifier is optimal."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "346f8b4cc9b15cec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Choosing from a class of decision rules\n",
    "\n",
    "An alternative to model the conditional probability $\\eta$ of $Y$ given $X$: Fix a class $\\mathcal{H}$ of functions from $X$ to $Y$ (decision rules), and use data choose $h_n$ from $\\mathcal{H}$.\n",
    "\n",
    "For examples, consider the class of linear threshold functions on $\\mathbb{R}^d$:\n",
    "$$\n",
    "\\mathcal{H} = x \\mapsto sign(\\theta^T x): \\theta \\in \\mathbb{R}^d\n",
    "$$\n",
    "where $sign(t) = \\begin{cases}\n",
    "1 & \\text{ if } t \\geq 0 \\\\\n",
    "-1 & \\text{ if } t < 0\n",
    "\\end{cases}$\n",
    "\n",
    "The decision boundaries are hyperplanes through the origin ($d - 1$ dimensional subspaces of $\\mathbb{R}^d$) and the decision regions are half-spaces through the origin.\n",
    "- Easy to understand and interpret\n",
    "- Easy to compute\n",
    "- Optimal if class conditional probabilities (conditional probabilities of $Y$ given $X$) are (linear functions of $X$) e.g. Gaussian with the same covariance matrix\n",
    "\n",
    "**Linear threshold functions**:\n",
    "For threshold linear functions, the decision boundaries are hyperplanes through the origin.\n",
    "For threshold affine functions, the decision boundaries are hyperplanes:\n",
    "$$\n",
    "\\mathcal{H} = \\begin{cases}\n",
    "x \\mapsto sign(\\theta^T x + c): \\theta \\in \\mathbb{R}^d, c \\in \\mathbb{R} \\\\\n",
    "x \\mapsto sign(\\theta^T \\tilde{x}): \\tilde{x} \\in \\mathbb{R}^{d+1}\n",
    "\\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2501bcc00855bb93"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Empirical risk minimization\n",
    "**Question**: How to choose $h_n$ from $\\mathcal{H}$?\n",
    "**Answer**: One approach is empirical risk minimization (ERM), choose $h_n$ from $\\mathcal{H}$ to minimize the empirical risk:\n",
    "$$\n",
    "\\hat{R}_n(h) = \\mathbb{E}_{(x, y) \\sim \\hat{P}_n}[l(y, h(x))] = \\frac{1}{n} \\sum_{i=1}^n l(y_i, h(x_i))\n",
    "$$\n",
    "where $\\hat{P}_n$ is the empirical distribution of the data:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6149a02a5f59ffb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Linear threshold functions**:\n",
    "Consider empirical risk minimization for the class of linear threshold functions on $\\mathbb{R}^d$:\n",
    "- **Approximation**: Very restricted class of decision rules, we can consider a much larger class and retain the attractive properties of linearly parametrized functions, by considering a non-linear transformation of the input space:\n",
    "$$\n",
    "\\Phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D \\quad \\text{for some } D \\gg d\n",
    "$$\n",
    "- **Estimation**: Small $\\frac{d}{n}$ is ok, but large can be ok if we regularize.\n",
    "- **Computation**: Easy if $\\hat{R}_n = 0$. In general, hard if not. Can simplify if we consider after native (convex) loss functions $l$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caa4d53a72f48f51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic regression:\n",
    "$$\n",
    "\\mathcal{H} = \\{x \\mapsto sign(\\theta^T \\Phi(x)): \\theta \\in \\mathbb{R}^D\\}\n",
    "$$\n",
    "where $\\Phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^D$ is a non-linear transformation of the input space.\n",
    "\n",
    "**Logistic loss**:\n",
    "$$\n",
    "l(y, h(x)) = \\log(1 + e^{-y \\times h(x)})\n",
    "$$\n",
    "\n",
    "Maximizing the likelihood of the data under the model:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\prod_{i=1}^n \\frac{1}{1 + e^{-y_i \\times \\theta^T \\Phi(x_i)}}\n",
    "\n",
    "\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^n \\log(1 + e^{-y_i \\times \\theta^T \\Phi(x_i)})\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a621bc6743215192"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perceptron algorithm\n",
    "\n",
    "Input $\\left(X_1, Y_1\\right), \\ldots,\\left(X_N, Y_N\\right) \\in \\mathbb{R}^d \\times\\{-1,1\\}$\n",
    "- $\\theta_0 \\leftarrow 0 \\in \\mathbb{R}^d$\n",
    "- $t \\leftarrow 0$\n",
    "while $y_i \\neq \\operatorname{sign}\\left(\\theta_t^T x_i\\right)$, i.e some $\\left(x_i, y_i\\right)$ are misclassfied do\n",
    "- $\\theta_{t+1} \\leftarrow \\theta_t+y_i x_i$\n",
    "- $t \\leftarrow t+1$\n",
    "end while\n",
    "return $\\theta_t$\n",
    "\n",
    "Reminder\n",
    "$$\n",
    "\\operatorname{sign}(\\alpha)= \\begin{cases}1, & \\text { if } \\alpha>0 \\\\ -1, & \\text { if } \\alpha<0 \\\\ 0, & \\text { if } \\alpha=0\\end{cases}\n",
    "$$\n",
    "\n",
    "**Convergence of Perceptron algorithm**:\n",
    "Given linearly seperable data (i.e. there is a $\\theta \\in \\mathbb{R}^d$ such that $\\forall i, y_i \\theta^T x_i>0$ ), for any choices made at the update step, it terminates (with empirical risk zero) after no more than $\\frac{r^2}{\\gamma^2}$ iterations, where $r = \\max_i \\Vert x_i \\Vert$ and $\\gamma = \\min_i \\frac{y_i \\theta^T x_i}{\\Vert \\theta \\Vert}$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a0c466f6f8a84b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Proof**:\n",
    "\n",
    "The idea is to use the inner product $\\theta_t^T \\theta$ as a measure of progress, and show that each mistake gives a big increase to the inner product (aligns $\\theta_t$ with $\\theta$ ), but gives only a small increase to $\\left\\|\\theta_t\\right\\|$.\n",
    "\n",
    "First,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_{t+1}^T \\theta & =\\left(\\theta_t+y_i x_i\\right)^T \\theta \\\\\n",
    "& \\geq \\theta_t^T \\theta+\\gamma\\|\\theta\\| .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But $\\theta_0=0$, so $\\theta_t^T \\theta \\geq t \\gamma\\|\\theta\\|$.\n",
    "\n",
    "On the other hand,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\|\\theta_{t+1}\\right\\|^2 & =\\left\\|\\theta_t+y_i x_i\\right\\|^2 \\\\\n",
    "& =\\left\\|\\theta_t\\right\\|^2+\\left\\|x_i\\right\\|^2+2 y_i \\theta_t^T x_i \\\\\n",
    "& \\leq\\left\\|\\theta_t\\right\\|^2+r^2 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But $\\theta_0=0$, so $\\left\\|\\theta_t\\right\\|^2 \\leq t r^2$.\n",
    "Combining (and using Cauchy-Shwarz):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t \\gamma\\|\\theta\\| & \\leq \\theta_t^T \\theta & \\leq \\|\\theta_t\\| \\|\\theta\\| \\leq \\sqrt{t} r\\|\\theta\\| \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d79a8e75a4fcad77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Proof of Linh**:\n",
    "\n",
    "The goal is to analyze the behavior of the vector $\\theta_t$ as it gets updated over iterations. We're particularly interested in two aspects: how well $\\theta_t$ aligns with a fixed vector $\\theta$, and the growth of the magnitude of $\\theta_t$. To do this, we will examine the inner product $\\theta_t^T \\theta$ and the norm $\\left\\|\\theta_t\\right\\|$.\n",
    "\n",
    "First, consider the update rule for $\\theta_t$:\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + y_i x_i\n",
    "\\end{equation}\n",
    "where $y_i$ and $x_i$ are data points.\n",
    "\n",
    "Now, let's analyze the inner product $\\theta_t^T \\theta$:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\theta_{t+1}^T \\theta & =\\left(\\theta_t+y_i x_i\\right)^T \\theta \\\\\n",
    "& \\geq \\theta_t^T \\theta+\\gamma\\|\\theta\\| .\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Here, $\\gamma$ is the minimum margin, and the inequality shows that each update increases the alignment of $\\theta_t$ with $\\theta$.\n",
    "\n",
    "Since $\\theta_0=0$, we have:\n",
    "\\begin{equation}\n",
    "\\theta_t^T \\theta \\geq t \\gamma\\|\\theta\\|.\n",
    "\\end{equation}\n",
    "This indicates that the alignment increases at least linearly with the number of updates.\n",
    "\n",
    "Next, we examine the growth of the norm of $\\theta_t$:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\left\\|\\theta_{t+1}\\right\\|^2 & =\\left\\|\\theta_t+y_i x_i\\right\\|^2 \\\\\n",
    "& =\\left\\|\\theta_t\\right\\|^2+\\left\\|x_i\\right\\|^2+2 y_i \\theta_t^T x_i \\\\\n",
    "& \\leq\\left\\|\\theta_t\\right\\|^2+r^2 .\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Here, $r$ is the maximum norm of $x_i$, and the inequality shows that the norm of $\\theta_t$ grows by at most $R^2$ per update.\n",
    "\n",
    "Since $\\theta_0=0$, we get:\n",
    "\\begin{equation}\n",
    "\\left\\|\\theta_t\\right\\|^2 \\leq t r^2.\n",
    "\\end{equation}\n",
    "This indicates that the norm's growth is bounded by $tr^2$.\n",
    "\n",
    "Finally, combining these results and using the Cauchy-Schwarz inequality:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "t \\gamma\\|\\theta\\| & \\leq \\theta_t^T \\theta & \\leq \\|\\theta_t\\| \\|\\theta\\| \\leq \\sqrt{t} r\\|\\theta\\| \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "This encapsulates the balance between alignment and growth: as $\\theta_t$ aligns more with $\\theta$, its norm does not grow too fast.\n",
    "So we can have $t \\gamma\\|\\theta\\| \\leq \\sqrt{t} r\\|\\theta\\|$ then $t \\leq \\frac{r^2}{\\gamma^2}$. So the algorithm terminates after no more than $\\frac{r^2}{\\gamma^2}$ iterations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43633bc15fd4030e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "New algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{w \\in \\mathbb{R}^d} t(w)\\\\\n",
    "s.t. \\quad & \\forall i, y_i w^T x_i \\geq 1 \\\\\n",
    "\n",
    "\\Leftrightarrow argmin_{w \\in \\mathbb{R}^d} \\frac{r^2}{\\gamma ^2(w)}\\\\\n",
    "s.t. \\quad & \\forall i, y_i w^T x_i \\geq 1 \\\\\n",
    "\n",
    "\\Leftrightarrow argmin_{w \\in \\mathbb{R}^d} \\frac{1}{\\gamma(w)}\\\\\n",
    "s.t. \\quad & \\forall i, y_i w^T x_i \\geq 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $r = \\max_i \\Vert x_i \\Vert$ and $\\gamma(w) = \\min_i \\frac{y_i w^T x_i}{\\Vert w \\Vert}$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f25fbc711c42b4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**A glimpse of kernel methods**\n",
    "\n",
    "Notes: We can write $w_t$ in term of data: $w_t = \\sum_{i=1}^n \\alpha_i x_i$ where $\\Vert \\alpha \\Vert = \\sum_{i=1}^n |\\alpha_i| = t$.\n",
    "We can replace inner $\\langle x, w \\rangle = x^T w$ by $K(x, w) = \\langle \\Phi(x), \\Phi(w) \\rangle$ where $\\Phi$ is a non-linear transformation of the input space.\n",
    "- Predict: $$\\hat{y}_i = sign(\\sum_{j=1}^n \\alpha_j \\underbrace{\\langle x_j, x_i \\rangle}_{K(x_j, x_i)})$$\n",
    "- Update: If $y_i \\neq \\hat{y}_i$ then $\\alpha_i^{t+1} = \\alpha_i^t + y_i$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "716fdbaaf66c24c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vector Machines\n",
    "$ L(w, w_0, \\alpha) = \\frac{1}{2} \\Vert w \\Vert^2 - \\sum_{i=1}^n \\alpha_i (y_i (w^T x_i + w_0) - 1)$\n",
    "\n",
    "Derivative of $L$ with respect to $w$ and $w_0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w} & = w - \\sum_{i=1}^n \\alpha_i y_i x_i = 0 \\Rightarrow w = \\sum_{i=1}^n \\alpha_i y_i x_i \\\\\n",
    "\\frac{\\partial L}{\\partial w_0} & = - \\sum_{i=1}^n \\alpha_i y_i = 0 \\Rightarrow \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Plug in $w$ and $w_0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(w, w_0, \\alpha) & = \\frac{1}{2} \\Vert w \\Vert^2 - \\sum_{i=1}^n \\alpha_i (y_i (w^T x_i + w_0) - 1) \\\\\n",
    "& = \\frac{1}{2} \\Vert \\sum_{i=1}^n \\alpha_i y_i x_i \\Vert^2 - \\sum_{i=1}^n \\alpha_i (y_i (\\sum_{j=1}^n \\alpha_j y_j \\langle x_j, x_i \\rangle + w_0) - 1) \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\langle x_i, x_j \\rangle - \\sum_{i=1}^n \\alpha_i y_i (\\sum_{j=1}^n \\alpha_j y_j \\langle x_j, x_i \\rangle + w_0) + \\sum_{i=1}^n \\alpha_i \\\\\n",
    "& = \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\langle x_i, x_j \\rangle - \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\langle x_j, x_i \\rangle - w_0 \\sum_{i=1}^n \\alpha_i y_i + \\sum_{i=1}^n \\alpha_i \\\\\n",
    "& = \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\langle x_i, x_j \\rangle\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Derivative of $L$ with respect to $\\alpha$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\alpha_i} & = 1 - \\frac{1}{2} \\sum_{j=1}^n \\alpha_j y_j y_i \\langle x_j, x_i \\rangle - \\frac{1}{2} \\sum_{j=1}^n \\alpha_i y_i y_j \\langle x_i, x_j \\rangle \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Because $\\frac{\\partial L}{\\partial \\alpha_i} = 0$ then $1 - \\frac{1}{2} \\sum_{j=1}^n \\alpha_j y_j y_i \\langle x_j, x_i \\rangle - \\frac{1}{2} \\sum_{j=1}^n \\alpha_i y_i y_j \\langle x_i, x_j \\rangle = 0$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Rightarrow 1 - \\frac{1}{2} \\sum_{j=1}^n \\alpha_j y_j y_i \\langle x_j, x_i \\rangle & = \\frac{1}{2} \\sum_{j=1}^n \\alpha_i y_i y_j \\langle x_i, x_j \\rangle \\\\\n",
    "\\Rightarrow 2 - \\sum_{j=1}^n \\alpha_j y_j y_i \\langle x_j, x_i \\rangle & = \\sum_{j=1}^n \\alpha_i y_i y_j \\langle x_i, x_j \\rangle \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So we have:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{j=1}^n \\alpha_i y_i y_j \\langle x_i, x_j \\rangle & = 2 - \\sum_{j=1}^n \\alpha_j y_j y_i \\langle x_j, x_i \\rangle \\\\\n",
    "& = 2 - \\sum_{j=1}^n \\alpha_j y_j y_i \\langle x_i, x_j \\rangle \\\\\n",
    "& = 2 - \\sum_{j=1}^n \\alpha_j y_j y_i K(x_i, x_j) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So $\\alpha_i = \\frac{1}{y^2_i \\Vert x_i \\Vert^2} (2 - \\sum_{j=1}^n \\alpha_j y_j y_i K(x_i, x_j))$.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37d2b312d22fdfda"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "397d99baf57f4525"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-14T09:03:56.917960Z",
     "start_time": "2023-12-14T09:03:56.915868Z"
    }
   },
   "id": "e3d2e442744cba73"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
