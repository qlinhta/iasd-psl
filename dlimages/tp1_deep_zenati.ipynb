{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP: Artfical neural networks\n",
    "# ===========================================================\n",
    "\n",
    "The objective of this practical session is to implement from scratch an artificial neuron network architecture using the numpy library, then to train this neuron network in order to learn the function (non-linear):\n",
    "\n",
    "$$\n",
    "f^*(x_1, x_2) = x_1^2 + x_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "from training data:\n",
    "\n",
    "$$\n",
    "\\{(x_n, y_n) \\in \\mathbb{R}^2 \\times \\mathbb{R}, n = 1, \\dots, N \\},\n",
    "$$\n",
    "\n",
    "for which:\n",
    "\n",
    "$$\n",
    "\\forall n \\in \\{1, \\dots, N \\}, y_n = f^*(x_n) + \\epsilon_n,\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\epsilon_n$ is Gaussian white noise with zero mean and variance $\\sigma^2$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generation of training and test data\n",
    "\n",
    "The function below is used to generate data from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(nsamples, sigma=0.):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a dataset with the specified number of samples\n",
    "    \n",
    "    :param nsamples: number of sample points\n",
    "    :type nsamples: int\n",
    "    :param sigma: standard deviation of the noise\n",
    "    :type sigma: float\n",
    "    \n",
    "    :return: Generated dataset {(x_n, y_n), n = 1, ..., nsamples}\n",
    "    :rtype: tuple of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.zeros((nsamples, 2))\n",
    "    x[:, 0] = np.random.uniform(-1, 1, nsamples)\n",
    "    x[:, 1] = np.random.uniform(-1, 1, nsamples)\n",
    "\n",
    "    eps = np.random.normal(loc=0, scale=sigma, size=nsamples)\n",
    "    \n",
    "    y = x[:, 0]**2 + x[:, 1]**2 + eps\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 50\n",
    "sigma = 0.\n",
    "xtrain, ytrain = generate_dataset(nsamples, sigma)\n",
    "xtest, ytest = generate_dataset(nsamples, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing a network layer\n",
    "\n",
    "We will rely on a class, Layer, whose attributes are:\n",
    "\n",
    "- the \"input_dim\" size of the signal at the input of the layer\n",
    "- the number \"output_dim\" of neurons in the layer\n",
    "- the activation function \"activation\" used by the layer ('RELU' or none)\n",
    "- the \"weights\" matrix of the weights of the neurons of the layer\n",
    "- the \"biases\" vector of the biases\n",
    "- the input signal \"input\"\n",
    "- the output signal \"output\"\n",
    "- the gradients \"input_grad\", \"output_grad\", \"weights_grad\", and \"biases_grad\"\n",
    "\n",
    "This class allows us to implement what takes place inside a layer of the neural network. \n",
    "\n",
    "Complete the implementation of the class:\n",
    "\n",
    "**Question 2.1.** In the constructor, initialize the values of the weight and bias matrix.\n",
    "\n",
    "**Question 2.2.** Implement the computation of the output signal y of the layer in the presence of an input x. We remind here that:\n",
    "\n",
    "$$\n",
    "y = \\sigma (Wx) + b,\n",
    "$$\n",
    "\n",
    "where $\\sigma $ is the activation function of the layer and $b$ the bias.\n",
    "\n",
    "**Question 2.3.** Implement the backpropagation algorithm of the gradient of the output signal \"output_grad\" in the layer. For a layer $i$, we have:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "t_i &=& b_i + W_i x \\\\\n",
    "y_i &=& g_i(t_i) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "It can really help you if you calculate the derivative $\\frac{\\partial t}{\\partial W}$. Think about how to write this mathematically prior of implementing it. \n",
    "\n",
    "\n",
    "\n",
    "**Question 2.4.** Implement a method to update the weight / bias values of the layer knowing the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    \"\"\"\n",
    "    Neural network layer implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation='RELU'):\n",
    "\n",
    "        \"\"\"\n",
    "        :param input_dim: dimension of the input vector\n",
    "        :type input_dim: integer\n",
    "\n",
    "        :param output_dim: dimension of the output vector \n",
    "         (i.e number of neurons in the layer)\n",
    "        :type output_dim: integer\n",
    "\n",
    "        :param activation: activation function\n",
    "        :type activation: string\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "\n",
    "        #TODO, help: define all the weight and bias matrix using the adaptated dimensions\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes a forward pass in the layer\n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "\n",
    "        :return: output of the layer\n",
    "        :rtype: numpy array of size output_dim\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        #TODO, help: implement the output signal with the activation function\n",
    "\n",
    "\n",
    "    def backward(self, output_grad):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes a backward pass in the layer\n",
    "\n",
    "        :param output_grad: gradient of the output of the layer w.r.t\n",
    "         the training loss\n",
    "        :type output_grad: numpy array of size output_dim\n",
    "\n",
    "        :return: gradient of the output of the layer w.r.t\n",
    "         the training loss\n",
    "        :rtype: numpy array of size input_dim\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO, help: write down mathematically the gradients, no matter the activation function used then implement\n",
    "        \n",
    "\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "\n",
    "        \"\"\"\n",
    "        Update the weights during the gradient descent\n",
    "\n",
    "        :param learning_rate: learning rate\n",
    "        :type learning_rate: float\n",
    "        \"\"\"\n",
    "    \n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear model \n",
    "\n",
    "The Linear class below is used to implement the linear model\n",
    "\n",
    "$$\n",
    "f^*(x_1, x_2) = w_1 x_1 + w_2 x_2\n",
    "$$\n",
    "\n",
    "\n",
    "associated with the cost function corresponding to the standard $L^2$.\n",
    "\n",
    "\n",
    "The implementation is based on the Layer class: the linear model is in fact nothing more than a network layer without an activation function.\n",
    "\n",
    "The attributes of the class are:\n",
    "- the \"input_dim\" and \"output_dim\" dimensions of the input and output signals\n",
    "- the instance of the Layer class used to describe the model\n",
    "- the output signal \"output\"\n",
    "- the \"target\" target used during training\n",
    "- the \"loss\" value of the cost function:\n",
    "$$\n",
    "loss = (output - target)^2\n",
    "$$\n",
    "- the \"loss_grad\" gradient of the cost function compared to the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    \"\"\"\n",
    "    Linear model implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = Layer(self.input_dim, self.output_dim, activation='None')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes a forward pass in the neural network\n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "\n",
    "        :return: output of the neural network\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        self.output = self.layer.forward(x)\n",
    "\n",
    "\n",
    "    def compute_loss(self, x, target):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes the loss \n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "\n",
    "        :param target: target value\n",
    "        :type target: float\n",
    "\n",
    "        :return: loss\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "\n",
    "        self.target = target\n",
    "        self.forward(x)\n",
    "        self.loss = (self.output - target)**2\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Backpropagation in the neural network \n",
    "        \"\"\"\n",
    "\n",
    "        self.loss_grad = 2*(self.output - self.target)\n",
    "       \n",
    "        #use the layer.backward method to compute the backprop\n",
    "        self.layer.backward(self.loss_grad)\n",
    "\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "\n",
    "        \"\"\"\n",
    "        Update the weights of the network during the gradient descent\n",
    "\n",
    "        :param learning_rate: learning rate\n",
    "        :type learning_rate: float\n",
    "        \"\"\"\n",
    "        ##use the layer.update method to compute the gradient descent\n",
    "        self.layer.update(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network\n",
    "\n",
    "**Question 4.1** Complete the implementation of the TwoLayersNetwork class below, inspiring of the linear class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayersNetwork:\n",
    "\n",
    "    \"\"\"\n",
    "    Linear model implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1):\n",
    "        \n",
    "        \"\"\"\n",
    "        Model initialization\n",
    "        \n",
    "        :param input_dim: Dimension of the input signal\n",
    "        :type input_dim: int\n",
    "        \n",
    "        :param hidden_dim: Dimension of the hidden layer\n",
    "        :type input_dim: int\n",
    "        \n",
    "        :param output_dim: Dimension of the output signal\n",
    "        :type output_dim: int\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO, help: Use the layer class to build your layers\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes a forward pass in the neural network\n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO\n",
    "\n",
    "\n",
    "    def compute_loss(self, x, target):\n",
    "\n",
    "        \"\"\"\n",
    "        Computes the loss \n",
    "\n",
    "        :param x: input signal\n",
    "        :type x: numpy array of size input_dim\n",
    "\n",
    "        :param target: target value\n",
    "        :type target: float\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Backpropagation in the neural network \n",
    "        \"\"\"\n",
    "\n",
    "        #TODO\n",
    "\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "\n",
    "        \"\"\"\n",
    "        Update the weights of the network during the gradient descent\n",
    "\n",
    "        :param learning_rate: learning rate\n",
    "        :type learning_rate: float\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear model training\n",
    "\n",
    "The code below is used to train the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8cbf6befced7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ce5f061329f3>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "    # 1: Linear model\n",
    "\n",
    "    input_dim = 2\n",
    "    output_dim = 1\n",
    "    nepochs = 100\n",
    "\n",
    "    # Initializes the model\n",
    "    linear = Linear(input_dim, output_dim)\n",
    "    \n",
    "    # Fix the learning rate\n",
    "    learning_rate = 1e-1\n",
    "\n",
    "    training_loss, test_loss = [], []\n",
    "    for epoch in range(nepochs):\n",
    "        \n",
    "        train_err = []\n",
    "        for n in range(nsamples):\n",
    "\n",
    "            linear.compute_loss(xtrain[n], ytrain[n])\n",
    "            linear.backward()\n",
    "            linear.update(learning_rate)\n",
    "            train_err.append(linear.loss)\n",
    "\n",
    "        test_err = []\n",
    "        for n in range(nsamples):\n",
    "\n",
    "            linear.compute_loss(xtest[n], ytest[n])\n",
    "            test_err.append(linear.loss)\n",
    "\n",
    "        training_loss.append(np.array(train_err).mean())\n",
    "        test_loss.append(np.array(test_err).mean())\n",
    "\n",
    "    plt.plot(np.array(training_loss), label='training loss')\n",
    "    plt.plot(np.array(test_loss), label='test loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Minimal training error: \" + str(min(training_loss)))\n",
    "    print(\"Minimal test error: \" + str(min(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural network training\n",
    "\n",
    "**Question 6.1.** Using the linear model as an inspiration, implement the training of the neural network on the data\n",
    "\n",
    "**Question6.2.** What is the influence of the number of neurons in the hidden layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Question 6.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
