{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOKJm3FM6mnmCrfuECvGYk0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Segmentation\n","\n","This notebook regarde semantic segmentation using the [PascalVOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) dataset and PyTorch library. The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented.\n","\n","Let's first install and load the library we will use throught the notebook."],"metadata":{"id":"dL9sS5u1x8MV"}},{"cell_type":"code","source":["!pip install torchmetrics\n","!pip install monai\n","import os\n","import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import time\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torchvision.transforms.functional import InterpolationMode\n","import torchmetrics\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","import torchvision\n","from torchvision.models.segmentation import FCN_ResNet50_Weights\n","import torch.nn.functional as F\n","import monai\n","import torchvision.transforms.functional as TF"],"metadata":{"id":"ZJMQjn9ix9FQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data\n","\n","In this notebook we will use the [PascalVOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) for semantic segmentation. 20 classes are defined (+ background) and an additional label (255) is used for the void category, i.e. border regions (5px) and mask for difficult objects. In the following we will consider these pixels as background and therefore we will have 21 classes in total.\n","\n","We define here some auxiliary functions to:\n","<ul>\n","    <li> Change the pixel vaues of void category to background</li>\n","    <li> Plot a group of images </li>\n","</ul>"],"metadata":{"id":"7tCnKr-UyAQ2"}},{"cell_type":"code","source":["def replace_tensor_value_(tensor, a, b):\n","    tensor[tensor == a] = b\n","    return tensor\n","\n","\n","def plot_images(images, num_per_row=8, title=None, vmax=None):\n","    num_rows = int(math.ceil(len(images) / num_per_row))\n","\n","    fig, axes = plt.subplots(num_rows, num_per_row, dpi=150)\n","    fig.subplots_adjust(wspace=0, hspace=0)\n","    for image, ax in zip(images, axes.flat):\n","        ax.imshow(image, vmax=vmax)\n","        ax.axis('off')\n","\n","    return fig"],"metadata":{"id":"8t9YMef1x-5T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we define the classes for PascalVOC segmantic segementation (from 0 to 20) and the color palette used for their representation in RGB. (These variables are not used in the following but can be useful if you want to further experiment with this dataset)"],"metadata":{"id":"gDUwDNqUyITP"}},{"cell_type":"code","source":["VOC_CLASSES = [\n","    \"background\",\n","    \"aeroplane\",\n","    \"bicycle\",\n","    \"bird\",\n","    \"boat\",\n","    \"bottle\",\n","    \"bus\",\n","    \"car\",\n","    \"cat\",\n","    \"chair\",\n","    \"cow\",\n","    \"diningtable\",\n","    \"dog\",\n","    \"horse\",\n","    \"motorbike\",\n","    \"person\",\n","    \"potted plant\",\n","    \"sheep\",\n","    \"sofa\",\n","    \"train\",\n","    \"tv/monitor\",\n","]\n","# Color palette for segmentation masks in RGB\n","PALETTE = np.array(\n","    [\n","        [0, 0, 0],\n","        [128, 0, 0],\n","        [0, 128, 0],\n","        [128, 128, 0],\n","        [0, 0, 128],\n","        [128, 0, 128],\n","        [0, 128, 128],\n","        [128, 128, 128],\n","        [64, 0, 0],\n","        [192, 0, 0],\n","        [64, 128, 0],\n","        [192, 128, 0],\n","        [64, 0, 128],\n","        [192, 0, 128],\n","        [64, 128, 128],\n","        [192, 128, 128],\n","        [0, 64, 0],\n","        [128, 64, 0],\n","        [0, 192, 0],\n","        [128, 192, 0],\n","        [0, 64, 128],\n","    ]\n","    + [[0, 0, 0] for i in range(256 - 22)] # Maybe not used \n","    + [[255, 255, 255]], # Maybe not used either\n","    dtype=np.uint8,\n",")"],"metadata":{"id":"Mb_pSbk4yK1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Download and visualize data\n","\n","In the following we first download the data using the datasets class VOCSegmentation provided by PyTorch in the torchvision module (https://pytorch.org/vision/stable/generated/torchvision.datasets.VOCSegmentation.html#torchvision.datasets.VOCSegmentation)"],"metadata":{"id":"sDn-H0Weya0g"}},{"cell_type":"code","source":["# Creating (and downloading) the dataset\n","train_dataset = datasets.VOCSegmentation(\n","    './datasets/',\n","    year='2012',\n","    download=True,\n","    image_set='train'\n",")\n","val_dataset = datasets.VOCSegmentation(\n","    './datasets/',\n","    year='2012',\n","    download=True,\n","    image_set='val'\n",")"],"metadata":{"id":"fTuTE4sqybJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Some info regarding the dataset\n","print(f\"Number of training images: {len(train_dataset)}\")\n","print(f\"Number of validation images: {len(val_dataset)}\")\n","# Plot some sample\n","inputs, ground_truths = map(list, zip(*[val_dataset[i] for i in range(32)]))\n","_ = plot_images(inputs)\n","_ = plot_images(ground_truths)"],"metadata":{"id":"tFrr78rcyhZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Transformation \n","\n","Pytorch provides several function to transform and augment images in the module [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html). Here, we will first resize the images to a fixed size and after that we will apply normalization using some standard values using in semantic segmentation. For label images we also replace the void label (255) with background (0). Finally, we load again the datasets providing the created segmentation for inputs and targets."],"metadata":{"id":"OxfeZDtaydUj"}},{"cell_type":"code","source":["image_size = 520 # We will resize to a standard size\n","image_mean = [0.485, 0.456, 0.406]  # These are some standard for image noramization \n","image_std = [0.229, 0.224, 0.225]  # in deep learning\n","input_transform = transforms.Compose(\n","    [\n","        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BILINEAR),\n","        transforms.ToTensor(),\n","        transforms.Normalize(image_mean, image_std),\n","    ]\n",")\n","target_transform = transforms.Compose(\n","    [\n","        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.NEAREST),\n","        transforms.PILToTensor(),\n","        transforms.Lambda(lambda x: replace_tensor_value_(x.squeeze(0).long(), 255, 0)),\n","    ]\n",")\n","# Creating (and downloading) the dataset\n","train_dataset = datasets.VOCSegmentation(\n","    './datasets/',\n","    year='2012',\n","    download=True,\n","    image_set='train',\n","    transform=input_transform,\n","    target_transform=target_transform,\n",")\n","val_dataset = datasets.VOCSegmentation(\n","    './datasets/',\n","    year='2012',\n","    download=True,\n","    image_set='val',\n","    transform=input_transform,\n","    target_transform=target_transform,\n",")"],"metadata":{"id":"UEYbuQy_ykgZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Let's try some pre-trained models\n","\n","Pytorch provides several pre-trained models in the module [torchvision.models](https://pytorch.org/vision/stable/models.html). In this notebook, we are interested in semantic segmentation models which can be found at https://pytorch.org/vision/stable/models.html#semantic-segmentation. Let's try the Fully-Convolutional Network model with a ResNet-50 backbone trained on another dataset with the 20 categories of PascalVOC (from the [Fully Convolutional Networks for Semantic Segmentation paper](https://arxiv.org/abs/1411.4038)). We first plot some results compared with the ground truth and then we use some metrics to evaluate the accuracy of the segmentation.\n","\n","#### Note: the cell below should print cuda:0, otherwise change the runtime on Google Colab"],"metadata":{"id":"uJxsd1GuypkM"}},{"cell_type":"code","source":["# Set device \n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('The current processor is ...', device)"],"metadata":{"id":"dGI0mQbDyqSu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataloader are iterative objects than automatically group data samples in batches \n","# and return them with the transformation specified in the dataset\n","batch_size = 32\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"],"metadata":{"id":"3C9a_zziysjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Exercise:\n","\n","Check the model description below and try to understand its components. Focus on the Bottleneck component(s), which of the two residual blocks represented in figure 5 of the [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf) is implemented?\n","\n","Note: In PyTorch a convolution with $I$ input channels and $O$ ouput channels is defined as Conv2D($I$, $O$)"],"metadata":{"id":"_biHgBR_yyJf"}},{"cell_type":"markdown","source":["# Answer\n","\n","Write here"],"metadata":{"id":"59wSwKtszWXd"}},{"cell_type":"code","source":["# Load pre-trained weights and create the model\n","weights = FCN_ResNet50_Weights.DEFAULT\n","model = torchvision.models.segmentation.fcn_resnet50(weights=weights)\n","print(model)"],"metadata":{"id":"fBTGHXDWyzik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we try to feed some samples and visualize the resulting segmentation."],"metadata":{"id":"7rBoXZMdy-Ds"}},{"cell_type":"code","source":["# Move model to device\n","model.to(device)\n","model.eval()\n","# In val, disactivate gradient computation to save memory\n","with torch.no_grad():\n","  for batch in valid_loader:\n","      data, label = batch\n","      # Move data to device\n","      data, label = data.to(device), label.to(device)\n","      output = model(data)\n","      # The second output is known as an auxiliary output and is contained \n","      # in the AuxLogits part of the network. We use just the 'out'.\n","      prediction = output['out'].argmax(dim=1).squeeze()\n","      print(\"Predictions\")\n","      _ = plot_images(prediction.cpu(), vmax=21, title='prediction')\n","      plt.show()\n","      print(\"Label\")\n","      _ = plot_images(label.cpu(), vmax=21, title='label')\n","      plt.show()\n","      break"],"metadata":{"id":"dqziorKcy9SF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dice accuracy\n","Let $T_P$, $F_N$ and $F_P$ be the true positives, false negatives and false positives, we define the Dice accuracy as:\n","\n","$$ Dice = \\frac{2 T_P}{2 T_P + F_N + F_P}$$\n","\n","We use the multi-class implementation of Dice accuracy provided by the torchmetrics library. Check the documentation here https://torchmetrics.readthedocs.io/en/stable/classification/dice.html."],"metadata":{"id":"O6Fa8Yt7zFRx"}},{"cell_type":"code","source":["# Start metric\n","dice = torchmetrics.Dice().to(device)\n","model.eval()\n","with torch.no_grad():\n","  for idx, batch in enumerate(valid_loader):\n","      start = time.time()\n","      data, label = batch\n","      data, label = data.to(device), label.to(device)\n","      output = model(data)\n","      logits = output['out']\n","      # Add this batch to metric computation\n","      dice.update(logits, label)\n","      print(f\"Batch {idx} inference in {time.time() - start} seconds\")\n","# Compute the total Dice on the whole validation set\n","print(f\"Average DICE accuracy on validation set: {dice.compute().item()*100} %\")\n","# Important to reset and free memory\n","dice.reset()"],"metadata":{"id":"O9HS0eqRzFqn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise:\n","\n","Based on the previous cells try some other pretrained model from the torchvision.models module. Check the avialble modlues and weghts here https://pytorch.org/vision/stable/models.html#semantic-segmentation. Are the model you tried better or worst than the Fully-Convolutional Network model with a ResNet-50 backbone? Do you see any difference in the predictions? Why?"],"metadata":{"id":"ibj9gg1wzKC0"}},{"cell_type":"code","source":["# TODO: write the code here to test another pre-trained model as previously done"],"metadata":{"id":"elKcIWFrzKWf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Let's train a custom module (UNet)\n","\n","In the following we manually implement the UNet model for image segmentation and we train it from scratch using the standard PyTorch pipeline.\n","\n","First we define the modules for each part of the UNet and after we define the complete model from https://github.com/milesial/Pytorch-UNet."],"metadata":{"id":"z6b1ckhozNla"}},{"cell_type":"code","source":["class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","\n","        # if bilinear, use the normal convolutions to reduce the number of channels\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","            self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","        # if you have padding issues, see\n","        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n","        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)"],"metadata":{"id":"7OrGp62nzOcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class UNet(nn.Module):\n","    def __init__(self, channel_in, n_classes, bilinear=False):\n","        super(UNet, self).__init__()\n","        self.channel_in = channel_in\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","\n","        self.inc = (DoubleConv(channel_in, 64))\n","        self.down1 = (Down(64, 128))\n","        self.down2 = (Down(128, 256))\n","        self.down3 = (Down(256, 512))\n","        factor = 2 if bilinear else 1\n","        self.down4 = (Down(512, 1024 // factor))\n","        self.up1 = (Up(1024, 512 // factor, bilinear))\n","        self.up2 = (Up(512, 256 // factor, bilinear))\n","        self.up3 = (Up(256, 128 // factor, bilinear))\n","        self.up4 = (Up(128, 64, bilinear))\n","        self.outc = (OutConv(64, n_classes))\n","\n","    def forward(self, x):\n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits"],"metadata":{"id":"FgHl_DygzcHR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we create and print our UNet model, the inputs channels are 3 (RGB) and the output calsses are 21 (20 objects + background)"],"metadata":{"id":"QaN5gaeszfNo"}},{"cell_type":"code","source":["# Create the model and print it\n","model = UNet(channel_in=3, n_classes=21, bilinear=True)\n","print(model)"],"metadata":{"id":"h5PN__0ezhh0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also load again the data with different resizing and batch size to don't fill the GPUs memory and have a 'CUDA out of memory' error."],"metadata":{"id":"6QfnQeFgzsG7"}},{"cell_type":"code","source":["image_size = 224 # We will resize to a standard size\n","image_mean = [0.485, 0.456, 0.406]  # These are some standard for image noramization \n","image_std = [0.229, 0.224, 0.225]  # in deep learning\n","input_transform = transforms.Compose(\n","    [\n","        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BILINEAR),\n","        transforms.ToTensor(),\n","        transforms.Normalize(image_mean, image_std),\n","    ]\n",")\n","target_transform = transforms.Compose(\n","    [\n","        transforms.Resize((image_size, image_size), interpolation=InterpolationMode.NEAREST),\n","        transforms.PILToTensor(),\n","        transforms.Lambda(lambda x: replace_tensor_value_(x.squeeze(0).long(), 255, 0)),\n","    ]\n",")\n","# Creating (and downloading) the dataset\n","train_dataset = datasets.VOCSegmentation(\n","    './datasets/',\n","    year='2012',\n","    download=True,\n","    image_set='train',\n","    transform=input_transform,\n","    target_transform=target_transform,\n",")\n","val_dataset = datasets.VOCSegmentation(\n","    './datasets/',\n","    year='2012',\n","    download=True,\n","    image_set='val',\n","    transform=input_transform,\n","    target_transform=target_transform,\n",")\n","batch_size = 16\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"],"metadata":{"id":"2p7ka7L6zsdF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We define in the following all the parameters that will be used in the training. Namely:\n","\n","<ul>\n","  <li> Learning rate </li>\n","  <li> Number of epochs </li>\n","  <li> Optimizer: Adam </li>\n","  <li> Loss function:  Cross Entropy (CE) loss.</li>\n","  <li> Function to compute the validation accuracy (DICE as before)</li>\n","</ul>\n"],"metadata":{"id":"DfZ_NY7Ez0oQ"}},{"cell_type":"code","source":["learning_rate = 1e-5\n","epochs = 20\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = torch.nn.CrossEntropyLoss()\n","dice = torchmetrics.Dice().to(device)"],"metadata":{"id":"0G54YbVKz3Wa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Exercise:\n","\n","We impmenet int he next cell, the classic PyTorch pipeline, structured as follow to perform the training of the UNet. Fill the missing code blocks (TODO). After that try to change the learning rate and the number of epochs, what is the impact on the final result ? If you want apply also other changes in the model or loss function to try to improve the results. It is not easy to find the good combination of parameters giving good results, but you will see the loss decreasing during training and the accuracy increasing anyway"],"metadata":{"id":"hwkt9eIBz6p6"}},{"cell_type":"code","source":["model = model.to(device)\n","train_losses = []\n","val_losses = []\n","val_accuracies = []\n","\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    # One train set iteration\n","    running_loss = 0.0\n","    last_train_loss = 0.0\n","    model.train()\n","\n","    for idx, batch in enumerate(train_loader):\n","\n","      # TODO: Fetch batch data as in the previous validation loops\n","\n","      labels = labels.unsqueeze(1) # We need this to have labels as [B, 1, W, H]\n","\n","      # zero the parameter gradients\n","      optimizer.zero_grad()\n","\n","      # forward\n","      # TODO: Feed data into the network and store the ouput\n","      # TODO: Compute loss using the criterion previously defined\n","\n","      # backward + optimize\n","      loss.backward()\n","      optimizer.step()\n","\n","      # print statistics\n","      running_loss += loss.item()\n","      if idx % 10 == 9:    # print every 10 mini-batches\n","          print(f'[{epoch + 1}, {idx + 1:5d}] loss: {running_loss / 10:.3f}')\n","          last_train_loss = running_loss / 10\n","          running_loss = 0.0\n","\n","    train_losses.append(last_train_loss)\n","    print(f\"Final test set loss for epoch {epoch + 1}: {last_train_loss:.3f}\")\n","\n","    # Validation loop\n","    model.eval()\n","    val_loss = 0.0\n","\n","    # TODO: Validation loop (see previous cells), accumulate the loss and the accuracy over the entire validation loader\n","    \n","    # AFter the validation iteration ends compute the total Dice on the whole validation set\n","    val_loss = val_loss / len(valid_loader)\n","    val_acc = dice.compute().item()*100\n","    val_losses.append(val_loss)\n","    val_accuracies.append(val_acc)\n","    print(f\"Loss on validation set for epoch {epoch + 1}: {val_loss:.3f}\")\n","    print(f\"Average DICE accuracy on validation set for epoch {epoch + 1}: {val_acc} %\")\n","    dice.reset()\n","\n","print('Finished Training')\n","\n","# Save model\n","torch.save(model.state_dict(), 'models/unet_v0.pt')\n","\n","# If you want load the trained model see https://pytorch.org/tutorials/beginner/saving_loading_models.html"],"metadata":{"id":"QkMDf6yTz5_W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plot the loss and accuracy graphs of the training"],"metadata":{"id":"lIAVvZdJ0KSO"}},{"cell_type":"code","source":["# Plot Train vs Val loss over epochs:\n","plt.figure()\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='train')\n","plt.plot(val_losses, label='val')\n","plt.ylabel(\"Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.legend()\n","plt.title(\"Loss vs Epochs\")\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(val_accuracies)\n","plt.ylabel(\"Accuracy [%]\")\n","plt.xlabel(\"Epochs\")\n","plt.title(\"Validation accuracy\")\n","plt.show()"],"metadata":{"id":"fZk3TkMY0JqG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Show segmentation of one validation batch"],"metadata":{"id":"5n7_beqh0lJ3"}},{"cell_type":"code","source":["model.eval()\n","# In val, disactivate gradient computation to save memory\n","with torch.no_grad():\n","  for batch in valid_loader:\n","      data, labels = batch\n","      # Move data to device\n","      data, labels = data.to(device), labels.to(device)\n","      output = model(data)\n","      # The second output is known as an auxiliary output and is contained \n","      # in the AuxLogits part of the network. We use just the 'out'.\n","      prediction = output.argmax(dim=1).squeeze()\n","      print(\"Predictions\")\n","      _ = plot_images(prediction.cpu(), vmax=21, title='prediction')\n","      plt.show()\n","      print(\"Label\")\n","      _ = plot_images(labels.cpu(), vmax=21, title='label')\n","      plt.show()\n","      break"],"metadata":{"id":"_P9n68jz0ngy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Additional Exercise:\n","\n","Add the parameter n_channels to the UNet initialization and define the convolutions input and output sizes proportional to this paramtetr. \n","\n","```\n","class UNet(nn.Module):\n","    def __init__(self, channel_in, n_classes, n_channels, bilinear=False):\n","        super(UNet, self).__init__()\n","        ...\n","```\n","\n","####Note: at each layer of UNet the features size, i.e. output channels, is doubled. Try to use the parameter n_channels as ouptput size of the first convolutions and make the followings proportional."],"metadata":{"id":"R6ZOOSCe0tRn"}},{"cell_type":"code","source":["# TODO: write the new model class here"],"metadata":{"id":"7n3wp8mU0uUP"},"execution_count":null,"outputs":[]}]}