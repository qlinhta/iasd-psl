{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YenwiDe3XnNz"
      },
      "source": [
        "# Tutorial on visualization of Neural Networks\n",
        "\n",
        "This exercise aims at exploring different ways of visualizing Neural Networks:\n",
        "- t-SNE of representations (CIFAR10)\n",
        "- grad-CAM (ImageNet)\n",
        "- activation maximization (ImageNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S1nu8O5edO0"
      },
      "source": [
        "First, some preliminaries that facilitate plotting and data access on Google drive ... just execute ! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LylQUzn0Ihl"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt \n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import importlib.util\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/dlia_course/practical_sessions/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQo2eR7qYp2Q"
      },
      "source": [
        "# basic imports\n",
        "import os\n",
        "import time\n",
        "\n",
        "# import keras and tensorflow classes\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# check the hardware settings\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "# use GPU as hardware acceleration.\n",
        "tf.device('/device:GPU:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aBuCvgrvFg5"
      },
      "source": [
        "# Visualization of the encodings by t-SNE\n",
        "\n",
        "First, we will visualize encodings of a network trained on the CIFAR data set. Here, we import and preprocess the data. We keep the original labels in `y_data` (to be used for visualization later on). For training, we need to transform them to one-hot-vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVt5biiB2fzO"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_data, y_data), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# preprocessing (normalization)\n",
        "x_data = x_data.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# encodings in one-hot-vectors\n",
        "y_train_cat = to_categorical(y_data)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "# train/val separation\n",
        "x_train = x_data[:40000]\n",
        "x_val = x_data[40000:]\n",
        "y_train = y_train_cat[:40000]\n",
        "y_val = y_train_cat[40000:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HbHQOcAwjRg"
      },
      "source": [
        "We will first visualize the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxNPROzN_LID"
      },
      "source": [
        "# helper function to plot a few images for each class\n",
        "def plot_array(fig, X, Y, classes_to_plot=None, samples_per_class=7):\n",
        "    if classes_to_plot is None:\n",
        "        classes_to_plot = np.unique(Y)\n",
        "    num_classes = len(classes_to_plot)\n",
        "\n",
        "    for k, y in enumerate(classes_to_plot):\n",
        "        idxs = np.flatnonzero(Y == y)\n",
        "        idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "        #print(y, idxs)\n",
        "\n",
        "        for i, idx in enumerate(idxs):\n",
        "            plt_idx = i * num_classes + k + 1\n",
        "            ax = fig.add_subplot(samples_per_class, num_classes, plt_idx)\n",
        "            ax.imshow(X[idx].astype(np.uint8))\n",
        "            ax.axis('off')\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "plot_array(fig, x_data*255, y_data, samples_per_class=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqxu5FE1vU0G"
      },
      "source": [
        "Next we define a neural network and train it on the data set. We output the model summary. The model summary gives you also the names of each of the layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wMHw5MgvOec"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "# compile model\n",
        "opt = RMSprop(learning_rate=0.001, rho=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUC_g0Vg-KmC"
      },
      "source": [
        "Now, we train the neural network, we observe training and validation error, and we try to get a solution that roughly obtains 75\\% or more on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4-q8B9Jw0mG"
      },
      "source": [
        "# model fit\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, \n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "model.evaluate(x=x_test, y=y_test_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk5Fnv4EaGxj"
      },
      "source": [
        "# We plot the learning curves (loss and accuracy)\n",
        "\n",
        "# plot loss\n",
        "plt.title('Cross Entropy Loss')\n",
        "plt.plot(history.history['loss'], color='blue', label='training loss')\n",
        "plt.plot(history.history['val_loss'], color='orange', label='validation loss')\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy\n",
        "plt.title('Classification Accuracy')\n",
        "plt.plot(history.history['accuracy'], color='blue', label='training acc')\n",
        "plt.plot(history.history['val_accuracy'], color='orange', label='validation acc')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_pbMHnr-lmC"
      },
      "source": [
        "Now, we need to define a few plots ... just execute !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPXn3E8d7oJD"
      },
      "source": [
        "class_definition = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', \n",
        "                    4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', \n",
        "                    9: 'truck'}\n",
        "\n",
        "from matplotlib.colors import to_hex\n",
        "\n",
        "# definition of the scatterplot\n",
        "def make_scatterplot(X, y, feature1=None, feature2=None, \n",
        "                     class_indices=None, class_definition=None):\n",
        "    if class_indices is None:\n",
        "        class_indices = np.unique(y)\n",
        "    if class_definition is None:\n",
        "        class_definition = dict(zip(class_indices, [str(i) for i in class_indices]))\n",
        "    if feature1 is None:\n",
        "      feature1 = 'Component 1'\n",
        "    if feature2 is None:\n",
        "      feature2 = 'Component 2'\n",
        "\n",
        "    # colors\n",
        "    colors = plt.cm.get_cmap('tab10', 10).colors[:,:3]\n",
        "\n",
        "    fig = plt.figure(figsize = (8,8))\n",
        "    ax = fig.add_subplot(1,1,1) \n",
        "    ax.set_xlabel(feature1)\n",
        "    ax.set_ylabel(feature2)\n",
        "    ax.set_title('Scatter plot: %s vs. %s' % (feature2, feature1))\n",
        "\n",
        "    comp1 = X[:,0]\n",
        "    comp2 = X[:,1]\n",
        "    for class_index in class_indices:\n",
        "        class_label = class_definition[class_index]\n",
        "        ax.scatter(comp1[y==class_index],\n",
        "                   comp2[y==class_index],\n",
        "                   c=to_hex(colors[class_index]),\n",
        "                   label=class_label,\n",
        "                   s=15)\n",
        "    ax.legend()\n",
        "    ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6qaIpFg-rMr"
      },
      "source": [
        "Now, we will extract the features of a layer and visualize the distribution of the encodings with t-SNE. \n",
        "\n",
        "First, we start with layer `flatten`. This is the last layer before the dense layers in the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldu8dEToy2W_"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# we limit ourselves to 1000 training samples. t-SNE does not scale \n",
        "# very well with the number of samples. \n",
        "x = x_data[:1000]\n",
        "y = y_data[:1000]\n",
        "\n",
        "layer_name = 'flatten'\n",
        "intermediate_layer_model = Model(inputs=model.input,\n",
        "                                 outputs=model.get_layer(layer_name).output)\n",
        "features = intermediate_layer_model.predict(x)\n",
        "\n",
        "print(features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMrbzc0U_8LN"
      },
      "source": [
        "Finally, we perform t-SNE. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iR1C7Hq71Ye"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "X_embedded = TSNE(n_components=2, perplexity=5).fit_transform(features)\n",
        "make_scatterplot(X_embedded, y.flatten(), class_definition=class_definition)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**: Explain what is a feature vector (hint: other terms in the literature are \"Encodings\", \"Embeddings\")"
      ],
      "metadata": {
        "id": "XIlwQ0dtwMQ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xlsOCfyAByB"
      },
      "source": [
        "**Assignment**: Try out several perplexities: 5, 10, 30. What do you observe? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTzUZtWvASm-"
      },
      "source": [
        "**Assignment**: Visualize now the scores at the two fully connected layers. Why are the representations so strikingly different? Imagine you would like to use the same representations in another project (same image size, but other classes). Which of the representations seems less useful? Why? "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**: Visualize the tSNE plot of the model before training, what do you observe? (flatten layer and last fully_connected layer)"
      ],
      "metadata": {
        "id": "OZmD-QFgzdqe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXhL6-54NNf9"
      },
      "source": [
        "# Classification activation maps (grad-CAM)\n",
        "\n",
        "Classification activation maps provide certainly the most popular visualization methods for network inspection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnBC0eTRp-Qh"
      },
      "source": [
        "!pip install --upgrade tf-keras-vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWDAj14wPbgj"
      },
      "source": [
        "from tf_keras_vis.gradcam import Gradcam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54oRqGkOQfj9"
      },
      "source": [
        "Next, we will load VGG16, pretrained on `ImageNet`. This is the network we are going to investigate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GREvDxCZPaAe"
      },
      "source": [
        "# Pretrained network: VGG16\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# We want to extract the entire network, including the finaly layer.\n",
        "model = VGG16(weights='imagenet',include_top=True)\n",
        "\n",
        "# We show the summary of model (to recall the dimensions)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwE27xqTRKU0"
      },
      "source": [
        "Now, we load an image from the `ImageNet` data base. We have downloaded these images: they are in the folder `imagenet`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UQjdr_AUxeG"
      },
      "source": [
        "import os\n",
        "filename = '418657219_3567961db1.jpg'\n",
        "\n",
        "# animals:\n",
        "# bird : filename = '418657219_3567961db1.jpg'\n",
        "# dog : filename = '425248370_b15374000e.jpg'\n",
        "# bird : filename = '485627874_8f4144223a.jpg'\n",
        "\n",
        "# bridges: \n",
        "# filename = 'bridge1.jpg'\n",
        "# filename = 'bridge2.jpg'\n",
        "# filename = 'bridge3.jpg'\n",
        "\n",
        "folder_name = '/content/gdrive/MyDrive/01_PROJECTS/practical_sessions/imagenet'\n",
        "# folder_name = '/content/gdrive/My Drive/Colab Notebooks/dlia_course/practical_sessions/imagenet'\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "image = load_img(os.path.join(folder_name, filename), target_size=(224, 224))\n",
        "plt.title('Bird 1')\n",
        "plt.imshow(image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGp_207hZwD9"
      },
      "source": [
        "Now, we will predict the label of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-jfZnSOZtin"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "\n",
        "# convert the image pixels to a numpy array\n",
        "img_prep = img_to_array(image)\n",
        "\n",
        "# reshape data (the model expects a batch of images.)\n",
        "img_prep = img_prep.reshape((1, img_prep.shape[0], img_prep.shape[1], img_prep.shape[2]))\n",
        "\n",
        "# prepare the image for the VGG model\n",
        "img_prep = preprocess_input(img_prep)\n",
        "\n",
        "# predict the probability across all output classes\n",
        "img_prediction = model.predict(img_prep)\n",
        "\n",
        "# the output is a 1000-dimensional vector of posterior probabilities.\n",
        "print('Shape of the output vector:', img_prediction.shape)\n",
        "\n",
        "# result\n",
        "print('Prediction result:', decode_predictions(img_prediction, top=3))\n",
        "max_index = np.argmax(img_prediction)\n",
        "print('Solution Index: ', max_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NzbKaRzCjkc"
      },
      "source": [
        "We need to define a few helper functions. First the `model_modifier` that replaces the `softmax` by a linear layer. The reason is that we cannot study the influence of neurons on the output $y_k$, if the output depends on all classes (which is the case when we use `softmax`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqmf6yMICiBN"
      },
      "source": [
        "# model modifier\n",
        "def model_modifier(m):\n",
        "    m.layers[-1].activation = tf.keras.activations.linear\n",
        "    return m\n",
        "\n",
        "def loss(img_prediction):\n",
        "    # the loss gives the score of the image for the correct class. \n",
        "    # if you want to test the importance for the prediction of another class\n",
        "    # you have to adapt the index accordingly.\n",
        "    correct_class_index = np.argmax(img_prediction)\n",
        "    return img_prediction[0][correct_class_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8OyjxAuum7A"
      },
      "source": [
        "from tf_keras_vis.utils import normalize\n",
        "from matplotlib import cm\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "\n",
        "# Create Gradcam object\n",
        "gradcam = Gradcam(model,\n",
        "                  model_modifier=model_modifier,\n",
        "                  clone=False)\n",
        "\n",
        "# Generate heatmap with GradCAM\n",
        "cam = gradcam(loss,\n",
        "              img_prep,\n",
        "              penultimate_layer=-1, # model.layers number\n",
        "             )\n",
        "cam = normalize(cam)\n",
        "\n",
        "plt.imshow(image)\n",
        "heatmap = np.uint8(cm.jet(cam[0]) * 255)\n",
        "plt.imshow(heatmap, cmap='jet', alpha=0.5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_xb8F6Z87D9"
      },
      "source": [
        "**Assignment:** Test the grad-CAM first on the three animal images, and verify that you obtain a reasonable result. Then test the algorithm on the three bridge images. Visualize the top-3 predictions.(Make a nice plot with the results)\n",
        "\n",
        "*   For `Bridge1.jpg` you obtain a wrong classification, but what can be said about the learned network in view of the visualization of the top-3 predictions? \n",
        "*   For `Bridge3.jpg` you get the right result, but what can you say about the \"understanding\" of the image in view of your visualization result? \n",
        "\n",
        "Note that the correspondence of class names and indices can be found at: \n",
        "[ImageNet](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFQ4kfpX2y5Z"
      },
      "source": [
        "# Activation Maximization\n",
        "\n",
        "So far, we visualized activations of images, i.e. we focused on visualizations of inner network representations for given image data.\n",
        "\n",
        "We can also visualize properties of the network itself. A popular method is the activation maximization, where we seek an image that would maximize a given neuron inside the network. \n",
        "\n",
        "For this, we solve the maximization problem: \n",
        "\\begin{equation}\n",
        "x^{\\ast} = {\\arg \\max}_{x} z(x)\n",
        "\\end{equation}\n",
        "where $z(x)$ is the value of an arbitrary neuron (or a set of neurons, e.g. the neurons in one feature map) in the network. \n",
        "\n",
        "Typically, $z(x)=S_c(x)$ is the value of the output layer for one particular class. We therefore seek the image that maximizes the output for a particular class (e.g. the output for `water_ouzel`, `index: 20`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at_Acz9EwGrB"
      },
      "source": [
        "# INDEX is the index of the class in the 1000-dimensional output vector\n",
        "INDEX = 20\n",
        "\n",
        "from tf_keras_vis.activation_maximization import ActivationMaximization\n",
        "\n",
        "activation_maximization = ActivationMaximization(model,\n",
        "                                                 model_modifier,\n",
        "                                                 clone=False)\n",
        "\n",
        "def loss(output):\n",
        "    return output[:, INDEX]\n",
        "\n",
        "from tf_keras_vis.utils.callbacks import Print\n",
        "\n",
        "activation = activation_maximization(loss, callbacks=[Print(interval=50)])\n",
        "generated_img = activation[0].astype(np.uint8)\n",
        "\n",
        "plt.imshow(generated_img)\n",
        "plt.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeO8ZqFmxl-M"
      },
      "source": [
        "**Assignment**: You might want to play with this, e.g. 385: Indian elephant, 2: great white shark, 555: fire truck\n",
        "\n",
        "Other classes can be found at: \n",
        "[ImageNet](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment:** Generate the activation maximization image the the same index, for a network initialized randomly. What do you observe? "
      ],
      "metadata": {
        "id": "_lz3rgUQxQC-"
      }
    }
  ]
}