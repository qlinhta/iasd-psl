{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimisation for Machine Learning\n",
    "\n",
    "October 04, 2023\n",
    "\n",
    "### Logistic\n",
    "Contact: [Clement Royer](mailto:clement.royer@lamsade.dauphine.fr)\n",
    "Lecture's web: [URL](https://www.lamsade.dauphine.fr/%7Ecroyer/teachOAA.html)\n",
    "Examen: 60% (2h), dated December 13, 2023 10:00 AM - 12:00 PM\n",
    "Project: 40%, during from October 6, 2023 to December 23, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94bdb770cc04ed35"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-04T07:30:53.758057Z",
     "start_time": "2023-10-04T07:30:53.743550Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Today**: More on Gradient Descent and Non-Convexity\n",
    "**Tomorrow**: Regularization and Proximal Gradient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2423b571ad5ab72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Problem**: $\\min_{w \\in \\mathbb{R}^d} f(w)$ where $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is convex and differentiable.\n",
    "\n",
    "Gradient descent iteration: $\\forall k \\geq 0, w^{k+1} = w^k - \\alpha_k \\nabla f(w^k)$ where $\\alpha_k > 0$ is the step size."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1abe7e56cb010aab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Choosing the step size $\\alpha_k$ (a.k.a learning rate in ML): \n",
    "\n",
    "**Recall**: When $f in \\mathcal{C}^{1, 1}_L$ (i.e. $\\mathcal{C}^{1} + \\nabla f$ is $L$-Lipschitz continuous), $\\alpha_k = \\frac{1}{L}$ is the good step size choice because it guarantees that $f(w^{k+1}) \\leq f(w^k) - \\frac{1}{2L} \\|\\nabla f(w^k)\\|^2$. (Any $\\alpha_k \\in (0, \\frac{2}{L})$ is also good.)\n",
    "\n",
    "In practice, the value of $L$ could be too expensive to compute/unknown/not exist.\n",
    "\n",
    "In this case, there are 3 main strategies to choose $\\alpha_k$:\n",
    "- **Constant step size**: $\\alpha_k = \\alpha$ for all $k \\geq 0$ (e.g. $\\alpha = 0.1, 0.01, 0.001$). For a sufficiently small this choice leads to convergence. For $\\mathcal{C}^{1, 1}_L$ functions, $\\alpha \\leq \\frac{2}{L}$ works. This is very popular in practice but difficult to calibrate in advance.\n",
    "- **Diminishing step size**: $\\alpha_k \\rightarrow 0$ as $k \\rightarrow \\infty$. For example, $\\alpha_k = \\frac{\\alpha}{k+1}$ or $\\alpha_k = \\frac{\\alpha}{\\sqrt{k+1}}$. For $k$ large enough, $\\alpha_k \\leq \\frac{2}{L}$ and convergence is guaranteed for $\\mathcal{C}^{1, 1}_L$ functions. Less popular than constant step size yet but useful in stochastic settings.\n",
    "- **Adaptive step size**: (Learning rate scheduling) Idea is that $\\alpha_k$ is chosen according to $f$ and $w^k$, $\\nabla f(w^k)$, $\\nabla f(w^{k-1})$, ... (e.g. $\\alpha_k = \\frac{\\alpha}{\\|\\nabla f(w^k)\\|}$). typically done via a line search. The GD step has the form $\\underbrace{w^k - \\alpha \\nabla f(w^k)}_{\\text{function of } \\alpha}$\n",
    ". We would like the best possible value of $\\alpha$ in term of function value $f(w_k - \\alpha \\nabla f(w^k))$ as small as possible.\n",
    "\n",
    "Exact line search: $\\alpha_k = \\arg\\min_{\\alpha \\geq 0} f(w^k - \\alpha \\nabla f(w^k))$. This is expensive to compute. In practice, we replace by an approximate line search."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52b851a8bf63fdff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Anmijo backtracking line search**\n",
    "Start with $\\alpha_k = \\overline{\\alpha}$ (e.g. $\\overline{\\alpha} = 1$). While $f(w^k - \\alpha_k \\nabla f(w^k)) > f(w^k) - C \\alpha_k \\|\\nabla f(w^k)\\|^2$ (e.g. $C = 10^{-4}$), update $\\alpha_k = \\beta \\alpha_k$ (e.g. $\\beta = 0.5$). Stop when $f(w^k - \\alpha_k \\nabla f(w^k)) \\leq f(w^k) - C \\alpha_k \\|\\nabla f(w^k)\\|^2$.\n",
    "Process based on a sufficient decrease condition. (See [Nocedal and Wright, Numerical Optimization, 2nd edition, 2006, p. 37](https://link.springer.com/book/10.1007/978-0-387-40065-5)). The condition will be violated $(f(w^k - \\alpha_k \\nabla f(w^k)) \\leq f(w^k) - C \\alpha_k \\|\\nabla f(w^k)\\|^2)$ when $\\alpha_k$ is small enough. The condition is satisfied when $\\alpha_k$ is large enough.\n",
    "\n",
    "If $f \\in \\mathcal{C}^{1, 1}_L$ and $C = \\frac{1}{2}$, then $\\alpha_k \\leq \\frac{2}{L}$ and convergence is guaranteed.\n",
    "$\\Longrightarrow$ for $\\alpha_k = \\frac{1}{L}$, we recover the guarantee of the constant step size $f(w_k - \\alpha_k \\nabla f(w_k)) \\leq f(w_k) - \\frac{1}{2L} \\|\\nabla f(w_k)\\|^2$.\n",
    "\n",
    "This strategy is more expensive than using constant step size or decreasing step size because it requires function evaluations $f(w^k) + f(w^k - \\alpha_k \\nabla f(w^k))$ for every $\\alpha_k$ used in the backtracking line search. In ML (mostly in deep learning) this is often considered as prohibitive."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20c5fd30e3173343"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c3cebc3b359de6e4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T07:30:53.774247Z",
     "start_time": "2023-10-04T07:30:53.760738Z"
    }
   },
   "id": "499fa4383f97590f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
