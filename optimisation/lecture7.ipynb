{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:14:38.193336Z",
     "start_time": "2023-11-15T09:14:37.707276Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='sans-serif')\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', titlesize=14)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=14)\n",
    "plt.rc('ytick', labelsize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('lines', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Large scale optimisation\n",
    "\n",
    "$\\min_{w \\in \\mathbb{R}^d} f(w)$ with $d \\gg 1$. We want to avoid computation $f(w)$ for all $w$.\n",
    "\n",
    "Approach: At each iteration, update a single coordinate $w_i$.\n",
    "\n",
    "#### Coordinate descent method\n",
    "\n",
    "We have $w_0 \\in \\mathbb{R}^d$\n",
    "At iteration $k$ we choose $j_k \\in \\{1, \\dots, d\\}$ and update $w_{k+1} = w_k + \\alpha_k \\nabla_{j_k} f(w_k) e_{j_k}$ where $e_{j_k}$ is the $j_k$-th canonical vector and $\\nabla_{j_k} f(w_k)$ is the gradient of $f$ with respect to the $j_k$-th coordinate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1b374ecc594c11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interesting when I can compute $\\nabla_{j_k} f(w_k)$ without having to access full $w_k$.\n",
    "\n",
    "Example:\n",
    "Sparse least squares with $l2$ regularisation:\n",
    "$$\n",
    "\\mathbf{X} \\in \\mathbb{R}^{n \\times d}, y \\in \\mathbb{R}^n, \\lambda > 0, \\min_{w \\in \\mathbb{R}^d} \\frac{1}{2n} \\| \\mathbf{X} w - y \\|_2^2 + \\lambda \\| w \\|_2^2 = f(w)\n",
    "$$\n",
    "\n",
    "Gradient:\n",
    "$$\n",
    "\\nabla f(w) = \\mathbf{X}^T (\\mathbf{X} w - y) + 2 \\lambda w\n",
    "$$\n",
    "cost of computing $\\nabla f(w)$ depends on the number of non-zero entries in $\\mathbf{X}$.\n",
    "\n",
    "$j = 1, \\dots, d$:\n",
    "$$\n",
    "\\nabla_j f(w) = \\frac{1}{n} \\sum_{i=1}^n x_{ij} (x_i^T w - y_i) + 2 \\lambda w_j\n",
    "$$\n",
    "cost of computing $\\nabla_j f(w)$ depends on the number of non-zero entries in the $j$-th column of $\\mathbf{X}$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e97999edc259c48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Variants of coordinate descent\n",
    "\n",
    "Question raised how we can choose $j_k$?\n",
    "$\\Longrightarrow$ Cyclic coordinate descent: $j_k = (1, 2, \\dots, d)$ we cycle through the coordinates. This might not converge (counter example by Powell). Cycle through a permutation of the coordinates $1, \\dots, d$ which is updated every $d$ iterations. So it is better guaranteed to converge than prior version.\n",
    "\n",
    "Randomised coordinate descent: $j_k$ is chosen uniformly at random from $\\{1, \\dots, d\\}$. At every iteration we choose a random coordinate. Good convergence guarantees.\n",
    "\n",
    "Block coordinate descent: We update a few cordinates at every iteration\n",
    "\n",
    "Example: Given matrix $\\mathbf{M} \\in \\mathbb{R}^{n \\times d}$, one wants to find a low rank approximation of $\\mathbf{M}$:\n",
    "$$\n",
    "\\mathbf{L} \\in \\mathbb{R}^{n \\times d}, \\mathbf{R} \\in \\mathbb{R}^{d \\times d}, \\mathbf{M} \\approx \\mathbf{L} \\mathbf{R}^T\n",
    "$$\n",
    "Find \\mathbf{L} and \\mathbf{R} such that $\\| \\mathbf{M} - \\mathbf{L} \\mathbf{R}^T \\|_F^2$ is minimised. This is non-convex and difficult to solve.\n",
    "\n",
    "Block the cordinate descents over \\mathbf{L} and \\mathbf{R}:\n",
    "Iteration k, update $L_{i_k, :}$ and $R_{:, j_k}$.\n",
    "$$\n",
    "\\begin{align}\n",
    "L_{i_k, :} &\\leftarrow \\arg \\min_{L_{i_k, :}} \\| \\mathbf{M} - \\mathbf{L} \\mathbf{R}^T \\|_F^2 \\\\\n",
    "R_{:, j_k} &\\leftarrow \\arg \\min_{R_{:, j_k}} \\| \\mathbf{M} - \\mathbf{L} \\mathbf{R}^T \\|_F^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Coordinates descent is designed for a distributed setting. We can distribute the data and compute the gradient in parallel. At every iteration we choose a random coordinate and update it in parallel. This is called parallel coordinate descent.\n",
    "\n",
    "Synchronized:\n",
    "$w_k \\rightarrow \\{\\text{every processor updates its own coordinate}\\} \\rightarrow w_{k+1}$\n",
    "Asynchronized:\n",
    "$w_k \\rightarrow \\{\\text{every processor updates its own coordinate without waiting for the others}\\} \\rightarrow w_{k+1}$ This works well in practice: Algorithm of Hogwild Recht et al. 2011. Hogwild combines the advantages of SGD and coordinate descent. It is a SGD algorithm where the gradient is computed in parallel. It is a coordinate descent algorithm where the coordinates are updated in parallel."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0ecea759631c464"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Decentralised learning\n",
    "\n",
    "$$ \\min_{w \\in \\mathbb{R}^d} \\sum_{i=1}^n f_i(w) $$\n",
    "\n",
    "The setting is that we have $n$ agents, each agent $i$ has access to $f_i$ and wants to minimise it. How can we minimise this in a decentralised way? With nobody having access to all of $f_i$.\n",
    "\n",
    "Each agent has its own variables $w_i \\in \\mathbb{R}^d$ and wants to minimise $\\min_{w_i \\in \\mathbb{R}^d} f_i(w_i)$. $z = w_i$ for all $i$.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9275ac1cadff57ea"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d11f8781ab710cca"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b772e50aa7a5685b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a385858dd4bf6a35"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
