{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Optimization for Machine Learning</span>\n",
    "\n",
    "### <span style=\"color:rgb(4,139,154)\">M2 MIAGE ID Apprentissage, 2023-2024</span>\n",
    "\n",
    "\n",
    "# <span style=\"color:rgb(4,139,154)\">Course project - Coordinate descent methods</span>\n",
    "\n",
    "### <span style=\"color:rgb(4,139,154)\">Current version: November 2, 2023</span>\n",
    "\n",
    "## <span style=\"color:rgb(4,139,154)\">Instructions</span>\n",
    "\n",
    "- This notebook must be filled out by each student **individually**. \n",
    "\n",
    "- The notebook contains a mix of theoretical and practical questions. **All answers are meant to be written directly into the notebook** (using Markdown and LaTeX commands as in the text blocks).\n",
    "\n",
    "- The coding part is minimal, yet it is necessary to fill out certain parts of the code for the algorithm to run. **Notebooks with no implementation or that raise an error upon execution will be penalized.**\n",
    "\n",
    "- Projects should be sent under the form of an individual file (.ipynb or .zip) **indicating your first and last name** to ***clement.royer@lamsade.dauphine.fr***.\n",
    "\n",
    "- The deadline for sending the project is **January 15, 2024**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(4,139,154)\">Useful links</span>\n",
    "\n",
    "- *The latest version of the project is available [here](https://www.lamsade.dauphine.fr/~croyer/ensdocs/OML/ProjOML.zip).*\n",
    "\n",
    "- *Lecture notes are available [here](https://www.lamsade.dauphine.fr/~croyer/ensdocs/OML/PolyOML.pdf).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(4,139,154)\">Introduction: Coordinate descent methods</span>\n",
    "\n",
    "Coordinate descent algorithms are among the most classical and historical optimization methods. Much like stochastic gradient techniques, they have experienced a regain of interest in recent years, thanks to their applicability to large-scale problems.\n",
    "\n",
    "The goal of this project is to investigate the theoretical foundations of such methods, as well as checking their use in sparse optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(4,139,154)\">Part 1: Basics of coordinate descent</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this project, we will be interested in optimization problems of the form\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w}),\n",
    "$$\n",
    "where the function $f$ is assumed to be convex and $\\mathcal{C}^1$.\n",
    " \n",
    " \n",
    "A coordinate descent algorithm starts from a given initial point $\\mathbf{w}_0 \\in \\mathbb{R}^d$. For any $k \\in \\mathbb{N}$, the method picks a coordinate index $j_k \\in \\{1,\\dots,d\\}$, a stepsize $\\alpha_k >0$, and computes\n",
    "$$\n",
    "    \\mathbf{w}_{k+1} := \\mathbf{w}_k - \\alpha_k \\nabla_{i_k} f(\\mathbf{w}_k) \\mathbf{e}_{i_k},\n",
    "$$\n",
    "where $\\nabla_{j_k}f(\\cdot)$ denotes the partial derivative with respect to the $j_k$th coordinate, (i.e. $\\nabla_{j_k} f(\\mathbf{w}) = [\\nabla f(\\mathbf{w})]_{j_k}$ for any $\\mathbf{w} \\in \\mathbb{R}^d$), while $\\mathbf{e}_{j_k}$ is the $j_k$th vector in the canonical basis of $\\mathbb{R}^d$. \n",
    "\n",
    "The primary goal of a coordinate descent iteration is to perform an optimization step with respect to *a single variable of the problem at a time*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(4,139,154)\">Part 1.1 First properties</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 1</span>\n",
    "\n",
    "*The coordinate descent iteration can be rewritten as $\\mathbf{w}_{k+1}:=\\mathbf{w}_k+c_k \\mathbf{e}_{i_k}$, \n",
    "where*\n",
    "$$\n",
    "    c_k = \\mathrm{argmin}_{c \\in \\mathbb{R}}\\ q_k(c):=\\nabla_{i_k} f(\\mathbf{w}_k) (c-[\\mathbf{w}_k]_{i_k}) \n",
    "    + \\frac{1}{2\\alpha_k}(c-[\\mathbf{w}_k]_{i_k})^2.\n",
    "$$\n",
    "\n",
    "*Justify that the problem $\\mathrm{minimize}_{c \\in \\mathbb{R}}\\ q_k(c)$ is strongly convex, and that it has a unique solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 1</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 2</span>\n",
    "\n",
    "A classical assumption in analyzing coordinate descent methods is that the gradient of $f$ is \n",
    "***coordinatewise Lipschitz continuous***, that is, it exists $d$ positive values $L_1,\\dots,L_d$ such that\n",
    "$$\n",
    "    \\forall j=1,\\dots,d,\\ \\forall (\\mathbf{w},\\mathbf{v}) \\in (\\mathbb{R}^d)^2, \n",
    "    \\quad \\left| \\nabla_j f(\\mathbf{w}) - \\nabla_j f(\\mathbf{v}) \\right| \n",
    "    \\le L_j \\|\\mathbf{w}-\\mathbf{v}\\|.\n",
    "$$\n",
    "\n",
    "*By analogy with gradient descent, how would you justify the choice $\\alpha_k = \\frac{1}{L_{j_k}}$ in the $k$th iteration of a coordinate descent method?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 2</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 3</span>\n",
    "\n",
    "If the gradient of $f$ is coordinatewise Lipschitz continuous, it is also Lipschitz continuous in the usual sense with\n",
    "$$\n",
    "    L:= \\sqrt{\\sum_{j=1}^d L_j^2}.\n",
    "$$\n",
    "\n",
    "*What can be the interest of using $\\alpha_k = \\frac{1}{L_{j_k}}$ instead of $\\alpha_k = \\frac{1}{L}$ in the $k$th iteration of a coordinate descent method?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 3</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(4,139,154)\">Part 1.2 Randomized variants</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One natural question that arises when implementing coordinate descent techniques is the way the coordinate $j_k$ is chosen at every iteration. A common practical choice consists in drawing this coordinate index at random, in which case the method is called **randomized coordinate descent**. The $k$th iteration of such a method is then written as\n",
    "$$\n",
    "    \\mathbf{w}_{k+1} := \\mathbf{w}_k - \\alpha_k\\nabla_{i_k} f(\\mathbf{w}_k) \\, \\mathbf{e}_{i_k},\n",
    "$$\n",
    "where $\\alpha_k > 0$. For simplicity, we will set $\\alpha_k = \\frac{1}{L_{j_k}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 4</span>\n",
    "\n",
    "Recall that $f$ is assumed to be convex. Letting $f^*=\\min_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w})$, it is possible to show that after $K \\ge 1$ of randomized coordinate descent, one has\n",
    "$$\n",
    "    \\mathbb{E}\\left[f(\\mathbf{w}_K)\\right] - f^* \\le \\mathcal{O}\\left(\\frac{d}{K}\\right),\n",
    "$$\n",
    "while we recall that gradient descent on the same problem gives\n",
    "$$\n",
    "    f(\\mathbf{w}_K)-f^* \\le \\mathcal{O}\\left(\\frac{1}{K}\\right).\n",
    "$$\n",
    "\n",
    "a) *Explain why these results suggest that gradient descent is better than coordinate descent.*\n",
    "\n",
    "b) *Can you find a metric (similarly to the notion of epochs in stochastic gradient) such that the results would be in favor of coordinate descent in large dimensions?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 4</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 5</span>\n",
    "\n",
    "It is possible to accelerate a randomized coordinate descent algorithm, by combining it with Nesterov's accelerated gradeint method. Starting from $\\mathbf{w}_0 \\in \\mathbb{R}^d$ and $\\mathbf{v}_0=\\mathbf{w}_0$, the algorithm draws (uniformly at random) an index $j_k \\in \\{1,\\dots,d\\}$, then computes:\n",
    "$$\n",
    "    \\left\\{ \n",
    "        \\begin{array}{lll}\n",
    "            \\mathbf{u}_k &:= &\\beta_k \\mathbf{v}_k + (1-\\beta_k) \\mathbf{w}_k \\\\\n",
    "            \\mathbf{w}_{k+1} &:= &\\mathbf{u}_k - \\alpha_k \\nabla_{j_k} f(\\mathbf{u}_k) \\mathbf{e}_{j_k} \\\\\n",
    "            \\mathbf{v}_{k+1} &:= &\\mu_k \\mathbf{v}_k + (1-\\mu_k) \\mathbf{u}_k - \\alpha_k\\gamma_k\\nabla_{j_k} f(\\mathbf{u}_k) \\mathbf{e}_{j_k},\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "where $\\alpha_k$ is typically chosen as $\\frac{1}{L_{j_k}}$ and $\\{\\beta_k,\\mu_k,\\gamma_k\\}$ are carefully chosen parameter sequences. With the appropriate choice for those quantities, one can improve the convergence rates of randomized coordinate descent.\n",
    "\n",
    "*What can be the practical drawback of such a method compared to a pure coordinate descent strategy?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 5</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 6</span>\n",
    "\n",
    "The randomized coordinate descent approach is similar in spirit to that of stochastic gradient, even though the former applies to coordinates while the latter applies to terms in the objective (in machine learning terms, the former uses randomization in the parameter space while the latter uses randomization in the data space). There is actually a deep connection between those two algorithms. \n",
    "\n",
    "To illustrate this connection, we consider the linear least-squares problem\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{v} \\in \\mathbb{R}^d} \n",
    "    \\frac{1}{d}\\left\\|\\mathbf{Q} \\mathbf{v} -\\mathbf{y} \\right\\|^2 \n",
    "    = \\frac{1}{d} \\sum_{i=1}^d (\\mathbf{q}_i^T \\mathbf{v} - [\\mathbf{y}]_i)^2,\n",
    "$$\n",
    "where $\\mathbf{Q} = \\left[ \\begin{array}{c} \\mathbf{q}_1^T \\\\ \\vdots \\\\ \\mathbf{q}_d^T \\end{array} \\right]$ is an orthogonal matrix, i.e. $\\mathbf{Q}^T = \\mathbf{Q}^{-1}$. It can be shown that this problem is equivalent to\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} \n",
    "    \\frac{1}{d} \\left\\|\\mathbf{w} -\\mathbf{Q}^T\\mathbf{y} \\right\\|^2 = \\frac{1}{d} \\sum_{i=1}^d ([\\mathbf{w}]_i - [\\mathbf{Q}^T\\mathbf{y}]_i)^2.\n",
    "$$\n",
    "\n",
    "*Show that stochastic gradient applied to the second problem is equivalent to randomized coordinate descent applied to the first problem, in the sense that if $\\mathbf{v}_k = \\mathbf{Q}\\mathbf{w}_k$ and both algorithms are applied using the same stepsize and same random index, then $\\mathbf{v}_{k+1} = \\mathbf{Q} \\mathbf{w}_{k+1}$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 6</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(4,139,154)\">Part 2: Coordinate descent and sparse optimization</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we wish to apply coordinate descent methods to problems of the form\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w})+\\lambda\\Omega(\\mathbf{w}),\n",
    "$$\n",
    "where $f$ is assumed to be of class $\\mathcal{C}^1$, $\\lambda>0$ and $\\Omega$ is a regularizing term. We further assume that $\\Omega$ possesses a **separable** structure, i.e. there exists $h:[0,\\infty) \\rightarrow [0,\\infty)$ such that\n",
    "$$\n",
    "    \\forall \\mathbf{w} \\in \\mathbb{R}^d, \\quad \\Omega(\\mathbf{w}) = \n",
    "    \\sum_{i=1}^d h([\\mathbf{w}]_i).\n",
    "$$\n",
    "A classical example of such a situation is when $h(t)=|t|$, in which case \n",
    "$\\Omega(\\mathbf{w})=\\|\\mathbf{w}\\|_1=\\sum_{i=1}^n |[\\mathbf{w}]_i|$. We will focus on this particular regularization technique in this project, however we point out that the key ideas extend to other regularizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(4,139,154)\">Part 2.1 One-dimensional case</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin our study by considering the problem\n",
    "$$\n",
    "    \\mathrm{minimize}_{w \\in \\mathbb{R}}\\ f_1(w):= a(w-u) + \\frac{L}{2} (w-u)^2 + \\lambda |w|,\n",
    "$$\n",
    "where $a,u \\in \\mathbb{R}$, $L>0$, and $\\lambda \\ge 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 7</span>\n",
    "\n",
    "The function $f_1$ is continuous and strongly convex. However, due to the presence of the $\\ell_1$ norm, this function is nonsmooth. Its *subdifferential* is given by\n",
    "$$\n",
    "    \\partial f_1(w) := \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            a + L(w-u) + \\lambda &\\mathrm{if}\\ w>0\\\\\n",
    "            a + L(w-u) -\\lambda &\\mathrm{if}\\ w<0\\\\\n",
    "            [a + L(w-u)-\\lambda,a + L(w-u)+\\lambda] &\\mathrm{if}\\ w=0.\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "*Show that the function $f_1$ has a unique global minimum given by*\n",
    "$$\n",
    "    w^* = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            u-\\tfrac{a}{L} - \\tfrac{\\lambda}{L} &\\mathrm{if}\\ u-\\tfrac{a}{L} > \\tfrac{\\lambda}{L} \\\\\n",
    "            u-\\tfrac{a}{L} + \\tfrac{\\lambda}{L} &\\mathrm{if}\\ u-\\tfrac{a}{L} < -\\tfrac{\\lambda}{L} \\\\\n",
    "            0 &\\mathrm{if}\\ u-\\tfrac{a}{L} \\in [-\\tfrac{\\lambda}{L},\\tfrac{\\lambda}{L}].\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 7</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 8</span>\n",
    "\n",
    "*Why is the result from the previous question more likely to yield  $\\ 0$ as a solution than in absence of regularization?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 8</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(4,139,154)\">Part 2.2 Block coordinate descent</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now go back to the multi-dimensional setting, and consider a *basis pursuit* problem. Given a dataset formed by a matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and a vector $\\mathbf{y} \\in \\mathbb{R}^d$, our goal is to compute the best possible linear model of the data according to\n",
    "$$\n",
    "    \\mathrm{minimize}_{\\mathbf{w} \\in \\mathbb{R}^d} f(\\mathbf{w})+ \\lambda \\|\\mathbf{w}_1\\|, \\qquad \n",
    "    f(\\mathbf{w}):= \\frac{1}{2 n} \\|\\mathbf{y} - \\mathbf{X} \\mathbf{w} \\|^2.\n",
    "$$\n",
    "This problem fits the general regularized framework of the beginning of this section. To tackle this problem, we will employ a **randomized block coordinate descent** method, which can be viewed as the counterpart of batch stochastic gradient for coordinate descent.\n",
    "\n",
    "At every iteration, a randomized block coordinate descent method chooses a (random) subset of coordinate indices $\\mathcal{B}_k \\subset \\{1,\\dots,d\\}$, and computes the new iterate as\n",
    "$$\n",
    "    \\mathbf{w}_{k+1} \\in \\mathrm{argmin}_{\\mathbf{w} \\in \\mathbb{R}} f_{\\mathcal{B}_k}(\\mathbf{w}):= f(\\mathbf{w}_k) + \\sum_{i \\in \\mathcal{B}_k} \\left\\{ \n",
    "    \\left([\\mathbf{w}]_j-[\\mathbf{w}_k]_j\\right) \\nabla_j f(\\mathbf{w}_k)+ \\frac{L_j}{2} \\left([\\mathbf{w}]_j-[\\mathbf{w}_k]_j\\right)^2 \n",
    "    + \\lambda \\left| [\\mathbf{w}_j] \\right| \\right\\},\n",
    "$$\n",
    "where $L_j$ represents the Lipschitz constant corresponding to the $j$th coordinate, which we suppose known as in Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 9</span>\n",
    "\n",
    "*Using the results from Part 2.1, justify that*\n",
    "$$\n",
    "    \\forall j \\in \\{1,\\dots,d\\}, \\quad \n",
    "    [\\mathbf{w}_{k+1}]_j = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        [\\mathbf{w}_k]_j &\\mathrm{if}\\ j \\notin \\mathcal{B}_k \\\\\n",
    "        [\\mathbf{w}_k]_j - \\tfrac{1}{L_j} \\nabla_j f(\\mathbf{w}_k) - \\tfrac{\\lambda}{L_j} &\\mathrm{if}\\ i \\in \\mathcal{B}_k\\ \\mathrm{and}\\  \n",
    "        [\\mathbf{w}_k]_j - \\tfrac{1}{L_j} \\nabla_j f(\\mathbf{w}_k) > \\tfrac{\\lambda}{L_j} \\\\\n",
    "        [\\mathbf{w}_k]_j - \\tfrac{1}{L_j} \\nabla_j f(\\mathbf{w}_k) + \\tfrac{\\lambda}{L_j} &\\mathrm{if}\\ j \\in \\mathcal{B}_k\\ \\mathrm{and}\\  \n",
    "        [\\mathbf{w}_k]_j - \\tfrac{1}{L_j} \\nabla_j f(\\mathbf{w}_k) < - \\tfrac{\\lambda}{L_j} \\\\\n",
    "        0 &\\mathrm{otherwise.}\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "*When all $L_j$s are equal and $\\mathcal{B}_k=\\{1,\\dots,d\\}$, what algorithm does the method correspond to?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 9</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 10</span>\n",
    "\n",
    "*Fill out the missing blocks in the code below to implement a block randomized coordinate descent method on our problem of interest. Run the next block to plot results for the method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized block coordinate descent for basis pursuit\n",
    "def rbcd(w0,X,y,lbda,nblocs=1,nits=500): \n",
    "    \"\"\"\n",
    "        Randomized block coordinate descent for basis pursuit problems.\n",
    "        \n",
    "        Inputs:\n",
    "            w0: Initial point\n",
    "            X: Data matrix (inputs/features)\n",
    "            y: Data vector (outputs/labels)\n",
    "            lbda: Regularization parameter\n",
    "            nblocs: Block size (constant over all iterations)\n",
    "            nits: Maximum iteration number (used as stopping condition)\n",
    "            \n",
    "        Outputs:\n",
    "            w_output: Last iterate computed by the algorithm\n",
    "            objvals: History of function values (Numpy array of length nits)\n",
    "            nnzvals: History of iterate sparsity (Numpy array of length nits)\n",
    "    \"\"\"\n",
    "    \n",
    "    ############\n",
    "    # Initialization\n",
    "    objvals = []\n",
    "    nnzvals = []\n",
    "    \n",
    "    # Initial iterate value\n",
    "    w = w0.copy()\n",
    "\n",
    "    # Iteration index\n",
    "    k=0    \n",
    "    \n",
    "    # Dimensions\n",
    "    n,d = X.shape\n",
    "    \n",
    "    # Lipschitz constants\n",
    "    ell = norm(np.matmul(X.T,X),axis=0)\n",
    "    \n",
    "    # Objective at the initial point\n",
    "    obj = norm(X.dot(w) - y) ** 2 / (2. * n)+ lbda * norm(w,1)\n",
    "    objvals.append(obj)\n",
    "    # Gradient for the smooth part\n",
    "    g = X.T.dot(X.dot(w) - y) / n\n",
    "    # Number of nonzero coefficients in the iterate \n",
    "    nnzvals.append(np.count_nonzero(w))\n",
    "\n",
    "    #########################\n",
    "    # Main loop\n",
    "    while (k < nits):\n",
    "        \n",
    "        #########################\n",
    "        # BEGINNING SECTION TO FILL OUT\n",
    "        \n",
    "        # Draw a random (subset of) coordinate(s)\n",
    "        \n",
    "        # Compute the new iterate (to be stored in w)\n",
    "        \n",
    "        \n",
    "        # Compute the objective and the gradient at the new point\n",
    "        # obj must contain the objective value\n",
    "        # g must contain the gradient of the smooth part of the objective\n",
    "        obj = \n",
    "        g = \n",
    "        \n",
    "        # END SECTION TO FILL OUT\n",
    "        ##########################\n",
    "        \n",
    "        # Update history\n",
    "        objvals.append(obj)\n",
    "        nnzvals.append(np.count_nonzero(w))\n",
    "        \n",
    "        k += 1  \n",
    "        \n",
    "    # End main loop \n",
    "    #######################\n",
    "    \n",
    "    w_output = w.copy()\n",
    "          \n",
    "    return w_output, np.array(objvals), np.array(nnzvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to validate your implementation of the method\n",
    "\n",
    "################# Preliminary imports\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # NumPy \n",
    "from scipy.linalg import norm # Euclidean norm\n",
    "import matplotlib.pyplot as plt # Plots\n",
    "\n",
    "############### Step 1 - Generate a sparse ground truth vector\n",
    "#\n",
    "# The coefficents of X are drawn following a normal distribution N(0,1/n)\n",
    "# The vector y is defined as y = X*w+eps,\n",
    "# where w is a sparse vector (90% of the coefficients equal to 0)\n",
    "from numpy.random import multivariate_normal, randn\n",
    "\n",
    "d = 200\n",
    "n = 200\n",
    "s = round(0.9*min(d,n))\n",
    "X = multivariate_normal(np.zeros(d), (1/n)*np.identity(d), size=n)\n",
    "idx = np.arange(d)\n",
    "\n",
    "# Ground truth coefficients\n",
    "wtrue = (-1)**idx * np.exp(-idx / 10.)\n",
    "ip = np.random.permutation(d)\n",
    "wtrue[ip[0:s]]=0\n",
    "\n",
    "Xw = X.dot(wtrue)\n",
    "std = (0.01/n)*(norm(Xw)**2)\n",
    "noise = std * randn(n)\n",
    "y = Xw + noise\n",
    "\n",
    "w0 = np.ones(d)\n",
    "lbda = 1/ (n**0.5)\n",
    "\n",
    "\n",
    "################# Step 2 - Test several variants of block coordinate descent\n",
    "\n",
    "nb=1\n",
    "nits=2000\n",
    "w1,obj1,nnz1 = rbcd(w0,X,y,lbda,nb,nits)\n",
    "print('Block size:',nb)\n",
    "print('Last objective value:',obj1[-1])\n",
    "print('Sparsity of the last iterate:',(d-nnz1[-1])*100/d)\n",
    "nb=int(d/100)\n",
    "w2,obj2,nnz2 = rbcd(w0,X,y,lbda,nb,nits)\n",
    "print('Block size:',nb)\n",
    "print('Last objective value:',obj2[-1])\n",
    "print('Sparsity of the last iterate:',(d-nnz2[-1])*100/d)\n",
    "nb=int(d/20)\n",
    "w3,obj3,nnz3 = rbcd(w0,X,y,lbda,nb,nits)\n",
    "print('Block size:',nb)\n",
    "print('Last objective value:',obj3[-1])\n",
    "print('Sparsity of the last iterate:',(d-nnz3[-1])*100/d)\n",
    "nb=d\n",
    "w4,obj4,nnz4 = rbcd(w0,X,y,lbda,nb,nits)\n",
    "print('Block size:',nb)\n",
    "print('Last objective value:',obj4[-1])\n",
    "print('Sparsity of the last iterate:',(d-nnz4[-1])*100/d)\n",
    "\n",
    "################# Step 3 - Plotting the results\n",
    "\n",
    "# In terms of the objective function\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.semilogy(obj1, label=\"nb=1\", color='indianred',lw=2)\n",
    "plt.semilogy(obj2, label=\"nb=d/100\", color='brown', lw=2)\n",
    "plt.semilogy(obj3, label=\"nb=d/20\", color='red', lw=2)\n",
    "plt.semilogy(obj4, label=\"nb=d\", color='darkred', lw=2)\n",
    "plt.title(\"Convergence\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Objective (log)\", fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "# In terms of sparsity\n",
    "plt.figure()\n",
    "itnum = np.arange(nits+1)\n",
    "print(itnum.shape)\n",
    "print(nnz4.shape)\n",
    "plt.scatter(itnum,nnz1,color='m',marker='o',label='nb=1')\n",
    "plt.scatter(itnum,nnz2,color='indigo',marker='d',label='nb=d/100')\n",
    "plt.scatter(itnum,nnz3,color='b',marker='*',label='nb=d/20')\n",
    "plt.scatter(itnum,nnz4,color='dodgerblue',marker='x',label='nb=d')\n",
    "plt.title(\"Iterate sparsity\", fontsize=16)\n",
    "plt.xlabel(\"#Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Nonzero coefficients\", fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 11</span>\n",
    "\n",
    "*From the results above, what variant seems the most efficient one? Is that expected?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 11</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(4,139,154)\">Question 12</span>\n",
    "\n",
    "*Without changing the algorithm, what other comparison could you provide that could help in assessing the interest of coordinate descent methods? Justify your answer either by implementing this comparison and providing the results, or by explaining why our suggestion is mathematically and algorithmically sound.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:rgb(4,139,154)\">Answer to question 12 (theoretical)</span>\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Answer to question 12 (numerical)\n",
    "\n",
    "#### ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
