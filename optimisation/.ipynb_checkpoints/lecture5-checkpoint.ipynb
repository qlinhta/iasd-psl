{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimisation for Machine Learning\n",
    "\n",
    "November 08, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "725c23c91785c73b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-09T13:40:55.512492Z",
     "start_time": "2023-11-09T13:40:55.018639Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='sans-serif')\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', titlesize=14)\n",
    "plt.rc('axes', labelsize=14)\n",
    "plt.rc('xtick', labelsize=14)\n",
    "plt.rc('ytick', labelsize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('lines', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finite sum problems\n",
    "\n",
    "$$\n",
    "\\min_{w \\in \\mathbb{R}^d} f(w) = \\sum_{i=1}^n f_i(w)\n",
    "$$\n",
    "\n",
    "where $f_i$ is convex and $L$-smooth for all $i$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93b09c22e663dcf3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Example:** Linear least squares\n",
    "$$\n",
    "\\mathbb{X} = \\begin{bmatrix} x_1^T \\\\ \\vdots \\\\ x_n^T \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}, \\quad y = \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix} \\in \\mathbb{R}^n\n",
    "$$\n",
    "$$\n",
    "f(w) = \\frac{1}{2n} \\| \\mathbb{X} w - y \\|_2^2 = \\frac{1}{2n} \\sum_{i=1}^n \\underbrace{\\left( x_i^T w - y_i \\right)^2}_{f_i(w)}\n",
    "$$\n",
    "\n",
    "Typical ML setup is $n \\gg 1$.\n",
    "\n",
    "Main cost of algortihm is to access the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97215b486b48cb9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assumptions:\n",
    "- $f_i$ is convex and $L$-smooth for all $i$, $f_i \\in \\mathcal{C}^1$ for all $i$\n",
    "\n",
    "Gradient descent: $\\nabla f(w) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w)$\n",
    "Iteration $k$: $w_{k+1} = w_k - \\alpha_k \\nabla f(w_k)$ = $w_k - \\alpha_k \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w_k)$\n",
    "Where $\\alpha_k$ is the step size with $\\alpha_k > 0$.\n",
    "\n",
    "One iteration of GD requires to access aall the data points. This is expensive for large $n$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ae6849d56d382f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GOAL**: Find a method that can minimise $f(w)$ that has iteration less expensive than GD.\n",
    "\n",
    "**Note**: $f(w) = \\frac{1}{n} \\sum_{i=1}^n f_i(w)$ known as **empirical risk** minimisation. Finite amount of data points.\n",
    "Most of what we study also applies to $\\min_{w \\in \\mathbb{R}^d} f(w) = \\mathbb{E}_{x \\sim \\mathcal{D}} \\left[ f(x, w) \\right]$ where $x$ is a random variable and $\\mathcal{D}$ is a probability distribution."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71de4fc9898e9e0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic gradient descent (SGD)\n",
    "\n",
    "$$\n",
    "f(w) = \\frac{1}{n} \\sum_{i=1}^n f_i(w) \\quad \\text{where } f_i \\in \\mathcal{C}^1 \\text{ and } L_i \\text{-smooth} \\\\\n",
    "\\text{Gradient descent: } w_{k+1} = w_k - \\alpha_k \\sum_{i=1}^n \\nabla f_i(w_k) \\\\\n",
    "\\text{Stochastic gradient descent: } w_{k+1} = w_k - \\alpha_k \\nabla f_{i_k}(w_k) \\quad \\text{where } i_k \\sim \\{1, \\dots, n\\} \\text{ uniformly at random}\n",
    "$$\n",
    "\n",
    "One iteration of SGD requires to access only one data point $f_{i_k}$ while GD requires to access all the data points.\n",
    "\n",
    "**Note**: Does SGD decrease the objective function $f(w)$? Not always!\n",
    "\n",
    "**Example**: $f_1(w) = 2w^2$, $f_2(w) = -w^2$, $f(w) = \\frac{1}{2} \\left( f_1(w) + f_2(w) \\right) = \\frac{1}{2} w^2$.\n",
    "For $i = 2$, the gradient is $\\nabla f_2(w) = -2w$ so $w_{k+1} = w_k + 2 \\alpha_k w_k$ and $w_{k+1} = \\left( 1 + 2 \\alpha_k \\right) w_k$. As long as $\\alpha_k > -\\frac{1}{2}$, $w_k \\to \\infty$.\n",
    "\n",
    "**Note**: SGD is not a descent method. The function value might increase because of the random and inexactly gradient.In practice, this works well for ML applications with many data points. In those cases, SGD tends to converge faster than GD."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e2c7e166a2ecdd9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iteration of SGD: $w_{k+1} = w_k - \\alpha_k \\nabla f_{i_k}(w_k) \\quad \\text{where } \\alpha > 0 \\text{ and } i_k \\sim \\{1, \\dots, n\\} \\text{ uniformly at random}$\n",
    "\n",
    "### **Question**: How to choose $\\alpha_k$ and $i_k$?\n",
    "\n",
    "##### Step size $\\alpha_k$\n",
    "- Constant step size: $\\alpha_k = \\alpha$ for all $k \\geq 0$\n",
    "- Decreasing step size: $\\alpha_k = \\frac{\\alpha}{k}$ for all $k \\geq 0$ This is a good choice for convex problems because $\\sum_{k=0}^\\infty \\alpha_k = \\infty$ and $\\sum_{k=0}^\\infty \\alpha_k^2 < \\infty$, so $\\lim_{k \\to \\infty} \\alpha_k = 0$ and $\\sum_{k=0}^\\infty \\alpha_k = \\infty$. Example: $\\alpha_k = \\frac{1}{k}$, $\\alpha_k = \\frac{1}{\\sqrt{k}}$, $\\alpha_k = \\frac{1}{k^\\beta}$ where $\\beta \\in (0, 1)$\n",
    "- Hybrid step size: Start with a fixed step size $\\alpha_0$ and then switch to a decreasing step size $\\alpha_k = \\frac{\\alpha_0}{k}$ for all $k \\geq 1$ while we are not making any progress.\n",
    "- Bold driver: $\\alpha_{k+1} = \\begin{cases} \\alpha_k \\text{ if } f(w_{k+1}) < f(w_k) \\\\ \\beta \\alpha_k \\text{ if } f(w_{k+1}) \\geq f(w_k) \\end{cases}$ where $\\beta > 1$ is a constant\n",
    "- Backtracking line search: $\\alpha_k = \\beta^j \\alpha_0$ where $\\beta \\in (0, 1)$ and $j = \\min \\{ j \\in \\mathbb{N} \\mid f(w_k - \\beta^j \\alpha_0 \\nabla f_{i_k}(w_k)) \\leq f(w_k) - \\frac{\\beta^j}{2} \\alpha_0 \\| \\nabla f_{i_k}(w_k) \\|_2^2 \\}$ LineSearches do not work well for SGD: checking the function decrease is expensive because we need to access all the data points."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33ff720476053b70"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convergence Analysis\n",
    "**Gradient Descent and Lipschitz Continuity**: The convergence analysis of gradient descent significantly relies on the Lipschitz continuity of the gradient. This mathematical condition provides a way to control the behavior of the function being optimized, especially its gradient. The Lipschitz condition for a gradient is given by:\n",
    "\n",
    "$$\n",
    "|| \\nabla f(w) - \\nabla f(v) ||_2 \\leq L \\| w - v \\|_2\n",
    "$$\n",
    "\n",
    "for all $w, v \\in \\mathbb{R}^d$. Here, $\\nabla f(w)$ is the gradient of the function at point $w$, and $L$ is the Lipschitz constant. The equation above essentially states that the change in the gradient of the function between any two points $w$ and $v$ in the domain is bounded by $L$ times the Euclidean distance between those two points. This condition implies that the function $f$ does not have any sudden changes or \"spikes\" in its gradient, making it more predictable and smoother for optimization purposes.\n",
    "\n",
    "Now, let's break down the convergence proof:\n",
    "\n",
    "1. **Lipschitz Condition Implication**:\n",
    "\n",
    "   Given the $L$-Lipschitz continuity of $\\nabla f$, we derive the following inequality:\n",
    "\n",
    "   $$\n",
    "   f(w) \\leq f(v) + \\nabla f(v)^T (w - v) + \\frac{L}{2} \\| w - v \\|_2^2\n",
    "   $$\n",
    "\n",
    "   for all $w, v \\in \\mathbb{R}^d$. This inequality is crucial because it provides a way to upper bound the function value at any point $w$ in terms of the function value and gradient at another point $v$. The term $\\frac{L}{2} \\| w - v \\|_2^2$ acts as a sort of penalty for the distance between $w$ and $v$.\n",
    "\n",
    "2. **Applying the Update Rule**:\n",
    "\n",
    "   In gradient descent, we update the variable $w_k$ to $w_{k+1}$ using the rule $w_{k+1} = w_k - \\alpha_k \\nabla f(w_k)$. Applying this update rule and the inequality derived from Lipschitz continuity, we get:\n",
    "\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   f(w_{k+1}) &\\leq f(w_k) + \\nabla f(w_k)^T (w_{k+1} - w_k) + \\frac{L}{2} \\| w_{k+1} - w_k \\|_2^2 \\\\\n",
    "   &= f(w_k) - \\alpha_k \\| \\nabla f(w_k) \\|_2^2 + \\frac{L \\alpha_k^2}{2} \\| \\nabla f(w_k) \\|_2^2 \\\\\n",
    "   &= f(w_k) + \\| \\nabla f(w_k) \\|_2^2 \\left( \\frac{L \\alpha_k^2}{2} - \\alpha_k \\right)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "   Here, we first substitute the update rule into the Lipschitz condition derived inequality. The first line applies the Lipschitz inequality, the second incorporates the update rule and rearranges terms, and the third line simplifies the expression, showing how the function value changes with each update.\n",
    "\n",
    "3. **Optimal Step Size**:\n",
    "\n",
    "   To ensure convergence, we need to find an optimal step size $\\alpha_k$. We define a function $\\phi(\\alpha)$ and find its minimum:\n",
    "\n",
    "   $$\n",
    "   \\phi(\\alpha) = \\frac{L \\alpha^2}{2} - \\alpha \\\\\n",
    "   \\phi'(\\alpha) = L \\alpha - 1 = 0 \\rightarrow \\alpha = \\frac{1}{L} \\\\\n",
    "   \\phi(\\frac{1}{L}) = \\frac{1}{2L} - \\frac{1}{L} = -\\frac{1}{2L} < 0\n",
    "   $$\n",
    "\n",
    "   We take the derivative of $\\phi(\\alpha)$, set it to zero to find the minimum, and find that $\\alpha = \\frac{1}{L}$ minimizes $\\phi(\\alpha)$. This step size guarantees a decrease in the function value at each iteration.\n",
    "\n",
    "4. **Cumulative Decrease in Function Value**:\n",
    "\n",
    "   Finally, we demonstrate that the function value decreases over iterations. We express this decrease as:\n",
    "\n",
    "   $$\n",
    "   f(w_k) - f(w_{k+1}) \\geq \\frac{1}{2L} \\| \\nabla f(w_k) \\|_2^2\n",
    "   $$\n",
    "\n",
    "   Summing up this decrease over all iterations gives us a lower bound on the overall decrease in the function value:\n",
    "\n",
    "   $$\n",
    "   \\begin{align*}\n",
    "   f(w_0) - f(w_{k}) = \\sum_{i=0}^{k-1} \\left( f(w_i) - f(w_{i+1}) \\right) &\\geq \\frac{1}{2L} \\sum_{i=0}^{k-1} \\| \\nabla f(w_i) \\|_2^2 \\\\\n",
    "   &\\geq \\frac{1}{2L} \\sum_{i=0}^{k-1} \\left( \\min_{w \\in \\mathbb{R}^d} \\| \\nabla f(w) \\|_2^2 \\right) \\\\\n",
    "   &= \\frac{k}{2L} \\left( \\min_{w \\in \\mathbb{R}^d} \\| \\nabla f(w) \\|_2^2 \\right)\n",
    "   \\end{align*}\n",
    "   $$\n",
    "\n",
    "   This lower bound implies that, as the number of iterations $k$ increases, the function value $f(w_k)$ decreases steadily, and the gradient norm $\\| \\nabla f(w_k) \\|_2^2$ converges to zero.\n",
    "\n",
    "5. **Convergence Conclusion**:\n",
    "\n",
    "   The analysis culminates in the conclusion that the sum of the squared gradients is finite, and the gradient norm converges to zero as the number of iterations tends to infinity:\n",
    "\n",
    "   $$\n",
    "   \\Rightarrow \\sum_{k=0}^\\infty \\| \\nabla f(w_k) \\|_2^2 < \\infty \\text{ and } \\lim_{k \\to \\infty} \\| \\nabla f(w_k) \\|_2 = 0\n",
    "   $$\n",
    "\n",
    "   This indicates the convergence of the gradient descent algorithm, a fundamental result for ensuring that the algorithm is effective for optimization tasks in machine learning and other applications."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e03023984af5f51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Rate of Convergence of Gradient Descent in Non-Convex Settings\n",
    "\n",
    "When dealing with non-convex functions, the analysis of gradient descent becomes more nuanced. The focus shifts from convergence to a global minimum to convergence to a stationary point, which is a point where the gradient is zero. The key inequality for understanding the rate of convergence in this setting is as follows:\n",
    "\n",
    "$$\n",
    "f(w_0) - f^* \\geq \\frac{1}{2L} \\sum_{i=0}^{k-1} \\| \\nabla f(w_i) \\|_2^2 \\geq \\frac{k}{2L} \\left( \\min_{w \\in \\mathbb{R}^d} \\| \\nabla f(w) \\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "This inequality is derived from the earlier discussion about the Lipschitz continuity of the gradient and the cumulative decrease in the function value.\n",
    "\n",
    "1. **Understanding the Inequality**:\n",
    "\n",
    "   The left part of the inequality, $f(w_0) - f^*$, represents the decrease in the function value from the initial point to the optimal value (not necessarily the global minimum in non-convex cases). \n",
    "\n",
    "   The middle term, $\\frac{1}{2L} \\sum_{i=0}^{k-1} \\| \\nabla f(w_i) \\|_2^2$, is a summation of the squared norms of the gradients over all iterations. This term provides an aggregate measure of how much the gradients have decreased.\n",
    "\n",
    "   The right part, $\\frac{k}{2L} \\left( \\min_{w \\in \\mathbb{R}^d} \\| \\nabla f(w) \\|_2^2 \\right)$, essentially scales the minimum squared norm of the gradient over the domain by the number of iterations and a constant factor related to the Lipschitz constant.\n",
    "\n",
    "2. **Implication for the Rate of Convergence**:\n",
    "\n",
    "   By rearranging the inequality, we get:\n",
    "\n",
    "   $$\n",
    "   \\min_{i=0, \\dots, k-1} \\| \\nabla f(w_i) \\|_2^2 \\leq \\sqrt{\\frac{2L \\left( f(w_0) - f^* \\right)}{k}} = \\mathcal{O} \\left( \\frac{1}{\\sqrt{k}} \\right)\n",
    "   $$\n",
    "\n",
    "   This crucial result implies that the minimum gradient norm encountered during the iterations is bounded above by a term that decreases at a rate proportional to $\\frac{1}{\\sqrt{k}}$, where $k$ is the number of iterations. In other words, as we run more iterations of gradient descent, the norm of the gradient at the point with the smallest gradient norm (among all iterates) decreases, indicating movement towards a stationary point.\n",
    "\n",
    "3. **Interpretation and Practical Relevance**:\n",
    "\n",
    "   This rate of convergence is particularly relevant in the context of non-convex optimization, where reaching the global minimum cannot be guaranteed. Instead, the goal is often to find a stationary point where the gradient is zero, indicating no local increase or decrease in the function value. The rate of $\\mathcal{O} \\left( \\frac{1}{\\sqrt{k}} \\right)$ is a theoretical guarantee of progress towards such points.\n",
    "\n",
    "   In practical applications, especially in machine learning where non-convex functions are common (e.g., in deep learning), this result assures that continued application of gradient descent will yield gradients with diminishing norms, a sign of approaching stationary points.\n",
    "\n",
    "In summary, the rate of convergence to a stationary point in non-convex optimization using gradient descent is given by $\\mathcal{O} \\left( \\frac{1}{\\sqrt{k}} \\right)$. This demonstrates that while we may not always reach the global minimum, gradient descent ensures a steady progression towards points where no immediate local improvements are possible."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d25e496c80a970f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Stochastic gradient descent**: \n",
    "$f \\in \\mathcal{C}^1$ and $L$-smooth, $\\nabla f$ is $L$-Lipschitz continuous. We have also $\\nabla f(w) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w)$, $w_{k+1} = w_k - \\alpha_k \\nabla f_{i_k}(w_k)$ where $i_k \\sim \\{1, \\dots, n\\}$ uniformly at random.\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(w_{k+1}) &\\leq f(w_k) + \\nabla f(w_k)^T (w_{k+1} - w_k) + \\frac{L}{2} \\| w_{k+1} - w_k \\|_2^2 \\\\\n",
    "&= f(w_k) + \\nabla f(w_k)^T \\left( -\\alpha_k \\nabla f_{i_k}(w_k) \\right) + \\frac{L \\alpha_k^2}{2} \\| \\nabla f_{i_k}(w_k) \\|_2^2 \\quad (*) \\quad \\text{because of the randomness in } i_k \\text{we cannot find the stepsize which ensures } f(w_{k+1}) \\leq f(w_k) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "At this juncture, denoted as $(*)$, we encounter a key challenge specific to SGD. Unlike in Gradient Descent, where we have deterministic steps, SGD involves a random component in selecting $i_k$. This randomness introduces uncertainty in directly assessing how much $f(w_{k+1})$ decreases compared to $f(w_k)$ at each step. \n",
    "\n",
    "We can show a decrease on average by taking the expected value with respect to $i_k$ in the last inequality, also think of the gradient $\\underbrace{\\nabla f_{i_k}(w_k)}_{\\text{g(w_k, i_k) = estimate of } \\nabla f(w_k)}$ as a random variable.\n",
    "\n",
    "Assume that at every iteration $k$, $i_k$ is chosen uniformly at random and independently from $i_0, \\dots, i_{k-1}$ and satisfies:\n",
    "- $\\mathbb{E}_{i_k} \\left[ \\nabla f_{i_k}(w_k) \\right] = \\nabla f(w_k) \\quad \\text{unbiased estimate of } \\nabla f(w_k)$\n",
    "- $\\mathbb{E}_{i_k} \\left[ \\| \\nabla f_{i_k}(w_k) \\|_2^2 \\right] \\leq \\| \\nabla f(w_k) \\|_2^2 + \\sigma^2 \\quad \\text{variance of } \\nabla f_{i_k}(w_k) \\leftrightarrow Var_{i_k} \\left[ \\| \\nabla f_{i_k}(w_k) \\|_2^2 \\right] \\leq \\sigma^2$\n",
    "\n",
    "To handle this, we analyze the expected decrease of the function value over iterations. We make two assumptions for this purpose:\n",
    "1. **Unbiased Gradient Estimates**: $\\mathbb{E}_{i_k}[\\nabla f_{i_k}(w_k)] = \\nabla f(w_k)$.\n",
    "2. **Bounded Gradient Variance**: $\\mathbb{E}_{i_k}[\\| \\nabla f_{i_k}(w_k) \\|_2^2] \\leq \\| \\nabla f(w_k) \\|_2^2 + \\sigma^2$.\n",
    "\n",
    "**Example**:\n",
    "Drawing $i_k$ uniformly at random from $\\{1, \\dots, n\\}$:\n",
    "$$\n",
    "\\mathbb{E}_{i_k} \\left[ \\nabla f_{i_k}(w_k) \\right] = \\sum_{i=1}^n \\underbrace{\\mathbb{P}(i = i_k)}_{\\frac{1}{n}} \\nabla f_i(w_k) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f_i(w_k) = \\nabla f(w_k)\n",
    "$$\n",
    "Continue convergence analysis from $(*)$ for SGD:\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(w_{k+1}) &\\leq f(w_k) - \\alpha_k \\nabla f(w_k)^T \\nabla f_{i_k}(w_k) + \\frac{L \\alpha_k^2}{2} \\left( \\| \\nabla f(w_k) \\|_2^2 + \\right) \\quad (*) \\\\\n",
    "\\text{Take expectation with respect to } i_k \\text{ on both sides: } \\\\\n",
    "\\mathbb{E}_{i_k} \\left[ f(w_{k+1}) \\right] &\\leq \\underbrace{\\mathbb{E}_{i_k} \\left[ \\nabla f_{i_k}(w_k) \\right]}_{\\nabla f(w_k)} - \\alpha_k \\nabla f(w_k)^T \\underbrace{\\mathbb{E}_{i_k} \\left[ \\nabla f_{i_k}(w_k) \\right]}_{\\nabla f(w_k)} + \\frac{L \\alpha_k^2}{2} \\underbrace{\\mathbb{E}_{i_k} \\left[ \\| \\nabla f_{i_k}(w_k) \\|_2^2 \\right]}_{\\leq \\| \\nabla f(w_k) \\|_2^2 + \\sigma^2} \\\\\n",
    "\\Leftrightarrow \\mathbb{E}_{i_k} \\left[ f(w_k) \\right] \\leq f(w_k) - \\alpha_k \\| \\nabla f(w_k) \\|_2^2 + \\frac{L \\alpha_k^2}{2} \\left( \\| \\nabla f(w_k) \\|_2^2 \\right) + \\underbrace{\\frac{L \\alpha_k^2}{2} \\sigma^2}_{\\text{noise/variance term}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "This allows to find step size $\\alpha_k$ such that $\\mathbb{E}_{i_k} \\left[ f(w_{k+1}) \\right] \\leq f(w_k)$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c6256221bd9df83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convergence result for $f, \\mu$-strongly convex and $L$-smooth:\n",
    "$$\n",
    "f(v) \\geq f(w) + \\nabla f(w)^T (v - w) + \\frac{\\mu}{2} \\| v - w \\|_2^2 \\quad \\text{for all } v, w \\in \\mathbb{R}^d \\\\\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "103fe94f4e24e33a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Theorem: Constant step size\n",
    "Let $f$ strongly convex, Lipschitz smooth. $f(w^*) = \\min f$\n",
    "Suppose that we do K iterations of SGD with constant step size $\\alpha_k = \\alpha \\leq \\frac{1}{L}$.\n",
    "Distribution of $i_k$ satisfies assumptions above.\n",
    "Then:\n",
    "$$\n",
    "\\mathbb{E} \\left[ f(w_K) - f(w^*) \\right] \\leq \\underbrace{\\left( 1 - \\alpha \\mu \\right)^K}_{\\text{convergence rate}} \\underbrace{\\left( f(w_0) - f(w^*) - \\frac{L \\alpha}{2} \\sigma^2 \\right)}_{\\text{initial error}} + \\underbrace{\\frac{L \\alpha}{2} \\sigma^2}_{\\text{noise term}}\n",
    "$$\n",
    "\n",
    "**Recall on Gradient Descent**\n",
    "$\\alpha_k = \\alpha \\text{fixed}$, $f$ strongly convex:\n",
    "$$\n",
    "f(w_k) - f(w^*) \\leq \\frac{L}{2} \\| w_k - w^* \\|_2^2 \\leq (1 - \\mu \\alpha)^k \\left( f(w_0) - f(w^*) \\right)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee607a474ebbec87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reducing stepsize leads to smaller noise but worst convergence rate. $(1 - \\mu\\alpha) \\to 1$ as $\\alpha \\to 0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30209c451670d97c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient descent gives $f(w_k) \\to f(w^*)$ deterministically; linear convergence rate.\n",
    "SGD gives in expectation that $\\mathbb{E} \\left[ f(w_k) \\right] \\to [f(w^*), f(w^*) + \\frac{L \\alpha}{2} \\sigma^2]$; this corresponds to practical behaviour."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f75d700b6f44aa75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Definition**: An EPOCH is a unit that corresponds to accessing n points. In each epoch, we access all the data points once.\n",
    "1 iteration of GD access all the data points once.\n",
    "1 iteration of SGD access 1 data point among n data points so it would be 1/n of an epoch.\n",
    "1 epoch = 1 iteration of GD = n iterations of SGD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d06323d5595c7215"
  },
  {
   "cell_type": "markdown",
   "source": [
    "SG with decreasing step size:\n",
    "**Theorem**: $f$ is strongly convex, run SGD with stepsize $\\alpha_k = \\mathcal{O} \\left( \\frac{1}{k + 1} \\right)$ for $k \\geq 1$.\n",
    "Then:\n",
    "$$\n",
    "\\mathbb{E} \\left[ f(w_k) - f(w^*) \\right] \\leq \\mathcal{O} \\left( \\frac{1}{k} \\right)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2180803940761408"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE: GD on $f$ strongly convex, $L$-smooth with $\\alpha_k = \\mathcal{O} \\left( \\frac{1}{k+1} \\right)$:\n",
    "$$\n",
    "f(w_k) - f(w^*) \\leq \\mathcal{O} \\left( \\frac{1}{k^2} \\right) \\\\\n",
    "\\mathcal{O} \\left( 1 - \\frac{\\mu}{L} \\right)^k \\to 0 \\text{ as } k \\to \\infty \\quad \\text{much faster than 1/k}\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d286a346b14ac5e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Budget of M epochs: $Mn$ iterations of SGD = $M$ iterations of GD\n",
    "We can expect $\\frac{1}{Mn} \\leq \\left(1 - \\frac{\\mu}{L} \\right)^M$ if m is large enough."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1a10cff9d5e1c47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Batch Stochastic Gradient\n",
    "\n",
    "SG: $w_{k+1} = w_k - \\alpha_k \\nabla f_{i_k}(w_k)$ where $i_k \\sim \\{1, \\dots, n\\}$ uniformly at random.\n",
    "GD: $w_{k+1} = w_k - \\alpha_k \\nabla f(w_k)$\n",
    "BatchSG: $w_{k+1} = w_k - \\frac{\\alpha_k}{|S_k|} \\sum_{i \\in S_k} \\nabla f_i(w_k)$ where $S_k \\subset \\{1, \\dots, n\\}$ is a subset of the data points of size $|S_k| = m$.\n",
    "\n",
    "$|S_k| = m$ is a hyperparameter. $m = 1$ is SGD, $m = n$ is GD.\n",
    "\n",
    "We have 2 regimes:\n",
    "- Large batch regime: $1 \\ll |S_k| \\ll n$: $|S_k|$ is large enough to approximate the gradient well but small enough to be faster than GD. Remove most of the variance in the algorithm less noise so it converges in a good neighbourhood of the optimum. But it is more expensive for each iteration, behaviour is closer to GD.\n",
    "- Small batch regime: $1 \\leq |S_k| \\ll n$: Per-iteration cost is much cheaper than GD, a little bit more expensive than SGD. Unlike SGD, possible to evaluate the gradient $\\nabla f(w_k)$ in parallel. More noise, but faster convergence than SGD.\n",
    "\n",
    "$\\Rightarrow$ The right batch size in practice depends on the architecture/hardware. How many gradient we can compute in parallel?\n",
    "\n",
    "**Convergence rate for BatchSG**:\n",
    "Assumptions:\n",
    "1. $\\mathbb{E}_{S_k} \\left[ \\nabla f_{S_k}(w_k) \\right] = \\nabla f(w_k)$\n",
    "2. $\\mathbb{E}_{S_k} \\left[ \\| \\nabla f_{S_k}(w_k) \\|_2^2 \\right] \\leq \\| \\nabla f(w_k) \\|_2^2 + \\sigma^2$\n",
    "\n",
    "Under that assumption:\n",
    "$$\n",
    "1. \\mathbb{E}_{i_k} \\left[ \\frac{1}{|S_k|} \\sum_{i \\in S_k} \\nabla f_i(w_k) \\right] = \\nabla f(w_k) \\\\\n",
    "2. \\mathbb{E}_{i_k} \\left[ \\left\\| \\frac{1}{|S_k|} \\sum_{i \\in S_k} \\nabla f_i(w_k) \\right\\|_2^2 \\right] \\leq \\frac{1}{|S_k|^2} \\sum_{i \\in S_k} \\mathbb{E}_{i_k} \\left[ \\| \\nabla f_i(w_k) \\|_2^2 \\right] \\leq \\| \\nabla f(w_k) \\|_2^2 + \\frac{\\sigma^2}{|S_k|}\n",
    "$$\n",
    "\n",
    "So if $f$ is $\\mu$-strongly convex and $L$-smooth, then:\n",
    "With $m_b = |S_k| = 1$:\n",
    "$$\n",
    "\\mathbb{E}_{i_k}[f(w_{k+1})] \\to \\[f(w^*), f(w^*) + \\frac{L \\alpha}{2 \\mu} \\sigma^2 \\]\n",
    "$$\n",
    "BatchSG with $m_b = |S_k| = m$:\n",
    "$$\n",
    "\\mathbb{E}_{i_k}[f(w_{k+1})] \\to \\[f(w^*), f(w^*) + \\frac{L \\alpha}{2 \\mu} \\frac{\\sigma^2}{m} \\]\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84c57f2b6c6d0533"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T13:40:55.513093Z",
     "start_time": "2023-11-09T13:40:55.511563Z"
    }
   },
   "id": "726fb150595d764f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
