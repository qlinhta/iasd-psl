{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimisation for Machine Learning\n",
    "\n",
    "September 27, 2023\n",
    "\n",
    "### Logistic\n",
    "Contact: [Clement Royer](mailto:clement.royer@lamsade.dauphine.fr)\n",
    "Lecture's web: [URL](https://www.lamsade.dauphine.fr/%7Ecroyer/teachOAA.html)\n",
    "Examen: 60% (2h), dated December 13, 2023 10:00 AM - 12:00 PM\n",
    "Project: 40%, during from October 6, 2023 to December 23, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3544c55eed147464"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-27T06:36:39.095522Z",
     "start_time": "2023-09-27T06:36:38.674739Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GRADIENT METHODS AND CONVEX OPTIMIZATION\n",
    "\n",
    "**Problem of interest**: $\\min_{w \\in \\mathbb{R}^d} f(w)$ and $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is convex and differentiable.\n",
    "\n",
    "When it is convex we don't have to worry about local minima, because the local minima is the global minima.\n",
    "\n",
    "**Assumption**: $f$ belongs to $C^{1,1}_{L}$ functions, i.e. $f$ is convex and its gradient is $L$-Lipschitz continuous.\n",
    "- $f$ is $C^1$ (continuously differentiable) at every point $w \\in \\mathbb{R}^d$, $\\exists \\nabla f(w) \\in \\mathbb{R}^d$ (gradient of $f$ at $w$) that represents how the function $f$ varies locally around $w$.\n",
    "- The gradient mapping $\\nabla f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d, w \\mapsto \\nabla f(w)$ is $L$-Lipschitz continuous where $L > 0$ is the Lipschitz constant of $\\nabla f$. This means that $\\forall w, w' \\in \\mathbb{R}^d, \\|\\nabla f(w) - \\nabla f(w')\\| \\leq L \\|w - w'\\|$. (At $L = 0$, $\\nabla f$ is constant, i.e. $\\nabla f(w) = \\nabla f(w')$ for all $w, w' \\in \\mathbb{R}^d$.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4c9f1317ab6d87a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Examples of $C^{1,1}_{L}$ functions**:\n",
    "- Linear least squares: $f(w) = \\frac{1}{2} \\|Xw - y\\|^2$ where $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^n$.\n",
    "- Logistic regression objective function: $f(w) = \\sum_{i=1}^n \\log(1 + \\exp(-y_i x_i^T w))$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, 1\\}$.\n",
    "- Quadratic function: $f(w) = \\frac{1}{2} w^T A w - b^T w$ where $A \\in \\mathbb{R}^{d \\times d}$ and $b \\in \\mathbb{R}^d$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f49dce74634d412"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Remark**: $C^{1,1}_{L}$ assumption is a simplifying assumption that:\n",
    "- Functions are not always $C^{1}$\n",
    "- Functions/gradients are not always Lipschitz continuous on the whole space $\\mathbb{R}^d$. $\\rightarrow$ Possible to use local Lipschitz constants instead of global Lipschitz constants.\n",
    "\n",
    "Sometimes $C^{1,1}_{L}$ are called $L$-smooth functions. $L$ is called the smoothness constant."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e90c46456345909"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Properties of $C^{1,1}_{L}$ functions**: If $f$ is $C^{1,1}_{L}$ convex then \n",
    "\n",
    "- $\\forall w, w' \\in \\mathbb{R}^d f(w') \\leq f(w) + \\nabla f(w)^T (w' - w) + \\frac{L}{2} \\|w' - w\\|^2$ $\\rightarrow$ Upper bound on $f(w')$ with a quadratic function of $w'$.\n",
    "- $\\forall w, w' \\in \\mathbb{R}^d f(w') \\geq f(w) + \\nabla f(w)^T (w' - w)$ $\\rightarrow$ Lower bound on $f(w')$ with a linear function of $w'$. (This is actually a characterization of convexity for $C^1$ functions.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f05ebea7d9a67aef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Toward gradient descent:\n",
    "1. Suppose that we are at $w \\in \\mathbb{R}^d$ and we know $f(w)$ and $\\nabla f(w)$.\n",
    "2. If $\\|\\nabla f(w)\\| = 0$ then $w$ is a global minimum of $f$ (because $f$ is convex).\n",
    "3. When $\\|\\nabla f(w)\\| \\neq 0$, using (1) we can find $v$ such that: $f(w) + \\nabla f(w)^T v + \\frac{L}{2} \\|v\\|^2 < f(w)$ implying that $f(v) \\leq f(w) + \\nabla f(w)^T v + \\frac{L}{2} \\|v\\|^2 < f(w)$.\n",
    "4. We can then replace $w$ by $v$ and repeat the process until $\\|\\nabla f(w)\\| = 0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd42fca2e2008210"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f1097b8d5c230c19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
