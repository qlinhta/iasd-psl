{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimisation for Machine Learning\n",
    "\n",
    "September 27, 2023\n",
    "\n",
    "### Logistic\n",
    "Contact: [Clement Royer](mailto:clement.royer@lamsade.dauphine.fr)\n",
    "Lecture's web: [URL](https://www.lamsade.dauphine.fr/%7Ecroyer/teachOAA.html)\n",
    "Examen: 60% (2h), dated December 13, 2023 10:00 AM - 12:00 PM\n",
    "Project: 40%, during from October 6, 2023 to December 23, 2023"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3544c55eed147464"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-04T06:27:48.976890Z",
     "start_time": "2023-10-04T06:27:48.949405Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('axes', titlesize=18)\n",
    "plt.rc('axes', labelsize=18)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('legend', fontsize=18)\n",
    "plt.rc('lines', markersize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GRADIENT METHODS AND CONVEX OPTIMIZATION\n",
    "\n",
    "**Problem of interest**: $\\min_{w \\in \\mathbb{R}^d} f(w)$ and $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is convex and differentiable.\n",
    "\n",
    "When it is convex we don't have to worry about local minima, because the local minima is the global minima.\n",
    "\n",
    "**Assumption**: $f$ belongs to $C^{1,1}_{L}$ functions, i.e. $f$ is convex and its gradient is $L$-Lipschitz continuous.\n",
    "- $f$ is $C^1$ (continuously differentiable) at every point $w \\in \\mathbb{R}^d$, $\\exists \\nabla f(w) \\in \\mathbb{R}^d$ (gradient of $f$ at $w$) that represents how the function $f$ varies locally around $w$.\n",
    "- The gradient mapping $\\nabla f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^d, w \\mapsto \\nabla f(w)$ is $L$-Lipschitz continuous where $L > 0$ is the Lipschitz constant of $\\nabla f$. This means that $\\forall w, w' \\in \\mathbb{R}^d, \\|\\nabla f(w) - \\nabla f(w')\\| \\leq L \\|w - w'\\|$. (At $L = 0$, $\\nabla f$ is constant, i.e. $\\nabla f(w) = \\nabla f(w')$ for all $w, w' \\in \\mathbb{R}^d$.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4c9f1317ab6d87a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Examples of $C^{1,1}_{L}$ functions**:\n",
    "- Linear least squares: $f(w) = \\frac{1}{2} \\|Xw - y\\|^2$ where $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\mathbb{R}^n$.\n",
    "- Logistic regression objective function: $f(w) = \\sum_{i=1}^n \\log(1 + \\exp(-y_i x_i^T w))$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, 1\\}$.\n",
    "- Quadratic function: $f(w) = \\frac{1}{2} w^T A w - b^T w$ where $A \\in \\mathbb{R}^{d \\times d}$ and $b \\in \\mathbb{R}^d$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f49dce74634d412"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Remark**: $C^{1,1}_{L}$ assumption is a simplifying assumption that:\n",
    "- Functions are not always $C^{1}$\n",
    "- Functions/gradients are not always Lipschitz continuous on the whole space $\\mathbb{R}^d$. $\\rightarrow$ Possible to use local Lipschitz constants instead of global Lipschitz constants.\n",
    "\n",
    "Sometimes $C^{1,1}_{L}$ are called $L$-smooth functions. $L$ is called the smoothness constant."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e90c46456345909"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Properties of $C^{1,1}_{L}$ functions**: If $f$ is $C^{1,1}_{L}$ convex then \n",
    "\n",
    "- [1] $\\forall w, w' \\in \\mathbb{R}^d f(w') \\leq f(w) + \\nabla f(w)^T (w' - w) + \\frac{L}{2} \\|w' - w\\|^2$ $\\rightarrow$ Upper bound on $f(w')$ with a quadratic function of $w'$.\n",
    "- [2] $\\forall w, w' \\in \\mathbb{R}^d f(w') \\geq f(w) + \\nabla f(w)^T (w' - w)$ $\\rightarrow$ Lower bound on $f(w')$ with a linear function of $w'$. (This is actually a characterization of convexity for $C^1$ functions.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f05ebea7d9a67aef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Toward gradient descent:\n",
    "1. Suppose that we are at $w \\in \\mathbb{R}^d$ and we know $f(w)$ and $\\nabla f(w)$.\n",
    "2. If $\\|\\nabla f(w)\\| = 0$ then $w$ is a global minimum of $f$ (because $f$ is convex).\n",
    "3. When $\\|\\nabla f(w)\\| \\neq 0$, using (1) we can find $v$ such that: $f(w) + \\nabla f(w)^T v + \\frac{L}{2} \\|v\\|^2 < f(w)$ implying that $f(v) \\leq f(w) + \\nabla f(w)^T v + \\frac{L}{2} \\|v\\|^2 < f(w)$.\n",
    "4. We can then replace $w$ by $v$ and repeat the process until $\\|\\nabla f(w)\\| = 0$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd42fca2e2008210"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Descent\n",
    "\n",
    "**Gradient descent algorithm**: $f$ is $C^{1,1}_{L}$ convex and differentiable.\n",
    "1. Initialize $w_0 \\in \\mathbb{R}^d$.\n",
    "2. For $k = 0, 1, \\dots$:\n",
    "    - Evaluate $f(w_k)$ and $\\nabla f(w_k)$. If $\\|\\nabla f(w_k)\\| = 0$ then stop. $\\Longrightarrow$ Output $w_k$.\n",
    "    - Choose a step size $\\alpha_k > 0$.\n",
    "    - Set $w_{k+1} = w_k - \\alpha_k \\nabla f(w_k)$.\n",
    "    End For.\n",
    "\n",
    "$w_{k+1} = w_k - \\alpha_k \\nabla f(w_k)$ is called the **gradient descent iteration**.\n",
    "\n",
    "**Remark**: $f$ is $C^{1,1}_{L}$ convex and differentiable $\\Longrightarrow$ $f$ is $C^1$ and convex $\\Longrightarrow$ $f$ is convex $\\Longrightarrow$ $f$ is $C^1$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "522f5168b7a54473"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**At step 1**: In an implementation, the for loop is replaced by a while loop involving a convergence criterion and a budget of iterations.\n",
    "- Convergence criterion: $\\|\\nabla f(w_k)\\| < \\epsilon$ for some $\\epsilon > 0$.\n",
    "- Budget of iterations: $k < k_{\\max}$ for some $k_{\\max} \\in \\mathbb{N}$.\n",
    "$\\Longrightarrow$ The algorithm stops when $\\|\\nabla f(w_k)\\| < \\epsilon$ or $k = k_{\\max}$.\n",
    "Budget of iterations is useful when the convergence criterion is not satisfied. It could be number of iterations, number of function evaluations, CPU time, etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "655119c355331cd9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Visualisation of gradient descent\n",
    "def plot_gradient_descent(f, grad_f, alpha, w0, max_iter=1000, eps=1e-6):\n",
    "    w = w0\n",
    "    w_list = [w]\n",
    "    while np.linalg.norm(grad_f(w)) > eps and len(w_list) < max_iter:\n",
    "        w = w - alpha * grad_f(w)\n",
    "        w_list.append(w)\n",
    "    w_list = np.array(w_list)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(w_list[:, 0], w_list[:, 1], 'o-', markersize=5, color='cyan')\n",
    "    plt.xlabel('$w_1$')\n",
    "    plt.ylabel('$w_2$')\n",
    "    plt.grid()\n",
    "    plt.title('Gradient descent')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T06:27:49.050293Z",
     "start_time": "2023-10-04T06:27:48.962829Z"
    }
   },
   "id": "6204700d7eedeabe"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 500x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 100\n",
    "d = 10\n",
    "X = np.random.randn(n, d)\n",
    "y = np.random.randn(n)\n",
    "def f(w):\n",
    "    return 0.5 * np.linalg.norm(X @ w - y) ** 2\n",
    "def grad_f(w):\n",
    "    return X.T @ (X @ w - y)\n",
    "alpha = 0.001\n",
    "w0 = np.random.randn(d)\n",
    "plot_gradient_descent(f, grad_f, alpha, w0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T06:27:49.199856Z",
     "start_time": "2023-10-04T06:27:48.967850Z"
    }
   },
   "id": "779faa11e85348f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Other convergence criteria:\n",
    "- $\\| \\nabla f(w_k) \\| < \\epsilon \\| \\nabla f(w_0) \\|$ where $\\epsilon$ is small tolerance/precision parameter. (e.g. $\\epsilon = 10^{-6}$)\n",
    "- $f(w_k) - \\min_{w \\in \\mathbb{R}^d} f(w) < \\epsilon$ where $\\epsilon$ is small tolerance/precision parameter. Closed to the best possible objective. Most of the time the optimal value is unknown. But for the convex functions, we can provide theorical bounds on the cost of satisfying this condition.\n",
    "- $\\|w_k - w^*\\| < \\epsilon$ where $\\epsilon$ is small tolerance/precision parameter, $w^* in \\arg\\min_{w \\in \\mathbb{R}^d} f(w)$ is the optimal solution.\n",
    "\n",
    "**Remark**: In general the optimal solution is unknown. But for certain convex functions, we can provide theorical bounds on the cost of satisfying $\\|w_k - w^*\\| < \\epsilon$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51ef91f2a1270414"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**At second step** How we can choose $\\alpha_k$?\n",
    "\n",
    "One choice that works for $C^{1,1}_{L}$ functions is $\\alpha_k = \\frac{1}{L}$.\n",
    "Proposition: Suppose that we are at iteration $k$ of gradient descent and $\\| \\nabla f(w_k) \\| \\neq 0$. Then, if $\\alpha_k = \\frac{1}{L}$, we have:\n",
    "$$\n",
    "f(w_k) - \\alpha_k \\|\\nabla f(w_k)\\|^2 \\leq f(w_k) - \\frac{1}{2L} \\|\\nabla f(w_k)\\|^2 < f(w_{k})\n",
    "$$\n",
    "\n",
    "Proof **(PLEASE REDO THIS!)**:\n",
    "Apply (1) with $v = w_k - \\alpha \\nabla f(w_k)$ and $w = w_k$:\n",
    "$$\n",
    "f(w_k) + \\nabla f(w_k) \\leq f(w_k) + \\nabla f(w_k)^T (-\\frac{1}{L} \\nabla f(w_k)) + \\frac{L}{2} \\|-\\frac{1}{L} \\nabla f(w_k)\\|^2 \\\\\n",
    "= f(w_k) + \\nabla f(w_k)^T(- \\alpha \\nabla f(w_k)) + \\frac{L}{2}\\|- \\alpha_k \\nabla f(w_k)\\|^2 \\\\\n",
    "= f(w_k) - \\underbrace {\\alpha_k \\nabla f(w_k)^T \\nabla f(w_k)}_{\\|\\nabla f(w_k)\\|^2} + \\frac{L}{2} \\alpha_k^2 \\|\\nabla f(w_k)\\|^2 \\\\\n",
    "= f(w_k) - \\alpha_k \\|\\nabla f(w_k)\\|^2 + \\frac{L}{2} \\alpha_k^2 \\|\\nabla f(w_k)\\|^2 \\\\\n",
    "= f(w_k) - \\frac{1}{L} \\|\\nabla f(w_k)\\|^2 + \\frac{L}{2} \\times \\frac{1}{L^2} \\|\\nabla f(w_k)\\|^2 \\\\\n",
    "= f(w_k) + [\\frac{1}{L} - \\frac{1}{2L}] \\|\\nabla f(w_k)\\|^2 \\\\\n",
    "= f(w_k) - \\frac{1}{2L} \\|\\nabla f(w_k)\\|^2 < f(w_k)\n",
    "$$\n",
    "\n",
    "More general result: For any $\\alpha_k \\in (0, \\frac{2}{L})$, we have:\n",
    "$$\n",
    "f(w_k) - \\alpha_k \\nabla f(w_k) < f(w_k)\n",
    "$$\n",
    "\n",
    "**Remark**:\n",
    "- This suggests a problem dependent choice of $\\alpha_k$.\n",
    "- Requires knowledge of Lipschitz constant $L$.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7729a842349590f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "QUESTION: Under our assumptions, what can we prove about the algorithm?\n",
    "\n",
    "**Theorem**: Suppose that $f$ is $C^{1,1}_{L}$ convex and differentiable. Then, for any $\\alpha_k = \\frac{1}{L}$, we have:\n",
    "1. Convergence rate: For any $k \\geq 1$, after $k$ iterations of gradient descent, we have: $$f(w_k) - \\min_{w \\in \\mathbb{R}^d} f(w) \\leq \\mathcal{O}(\\frac{1}{k})$$\n",
    "2. Complexity bound: For any $\\epsilon > 0$, GD reaches an iterate $w_k$ such that $f(w_k) - \\min_{w \\in \\mathbb{R}^d} f(w) \\leq \\epsilon$ in at most $\\mathcal{O}(\\frac{1}{\\epsilon})$ iterations. The cost of satisfying $f(w_k) - \\min_{w \\in \\mathbb{R}^d} f(w) \\leq \\epsilon$ is $\\mathcal{O}(\\frac{1}{\\epsilon})$ iterations.\n",
    "\n",
    "$\\mathcal{O}$ constant is dependent on $w_0$ and $L$, distance between $w_0$ and $w^*$.\n",
    "$\\leq$ signify that it is a worst case "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeb0a6dd65f53a11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convergence rates and complexity bounds are used as guidance or indicators of the performance of an algorithm. They can be closed to practical performance or simple examples and these rates/bounds are attained in some (pathological) cases.\n",
    "\n",
    "Two ways to get better rates/bounds:\n",
    "- Add more assumptions on $f$.\n",
    "- Change the algorithm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c35ce4f83f4e4579"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Strongly Convex Functions\n",
    "\n",
    "**Definition**: $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is $\\mu$-strongly convex if $\\forall v, w \\in \\mathbb{R}^d, f(v) \\geq f(w) + \\nabla f(w)^T (v - w) + \\frac{\\mu}{2} \\|v - w\\|^2$ where $\\mu > 0$ is the strong convexity constant of $f$.\n",
    "\n",
    "The part $\\frac{\\mu}{2} \\|v - w\\|^2$ is a quadratic term compared to the linear term $\\nabla f(w)^T (v - w)$.\n",
    "$f(v) \\geq f(w) + \\nabla f(w)^T (v - w)$ is corresponding to the linear approximation of $f$ at $w$ ([2] in $C^{1,1}_{L}$ functions)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336ba9c76445cfe1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**NB**: $f$ is $\\mu$-strongly convex $\\Longrightarrow$ $f$ is convex\n",
    "\n",
    "**Property**:\n",
    "If $f$ is $\\mu$-strongly convex then it has a unique global minimum, which is the unique solution of $\\nabla f(w) = 0$.\n",
    "\n",
    "**Consequence of Gradient Descent**: Suppose that $f \\in F^{1,1}_{L, \\mu}$, i.e. $f$ is $C^{1,1}_{L}$ convex and differentiable and $\\mu$-strongly convex $\\Longrightarrow$ $L \\geq \\mu$. Then running gradient descent with $\\alpha_k = \\frac{1}{L}$ give the following guarantee:\n",
    "1. Convergence rate: For any $k \\geq 1$, after $k$ iterations of gradient descent, we have: $$\\underbrace{\\|w_k - w^*\\|^2}_{\\text{distance to } w^*} \\leq \\underbrace{(1 - \\frac{\\mu}{L})^k}_{\\text{convergence rate}} \\underbrace{\\|w_0 - w^*\\|^2}_{\\text{distance from } w_0 \\text{ to } w^*}$$\n",
    "\n",
    "This is called a linear rate of convergence or an exponential rate of convergence. $1 - \\frac{\\mu}{L}$ is called the convergence factor.\n",
    "Better than the results for convex functions.\n",
    "- Stronger criterion: $\\|w_k - w^*\\| < \\epsilon$ instead of $f(w_k) - \\min_{w \\in \\mathbb{R}^d} f(w) \\leq \\epsilon$.\n",
    "- Better rate $(1 - \\frac{\\mu}{L})^k$ goes to $0$ faster than $\\frac{1}{k}$.\n",
    "In practice, GD is faster on strongly convex functions than on convex functions. For certain strongly convex functions, we can define a better step size than $\\alpha_k = \\frac{1}{L}$. Example for quadratic functions: $\\alpha_k = \\frac{2}{\\mu + L}$.\n",
    "\n",
    "2. Complexity bound: $\\|w_k - w^*\\| < \\epsilon$ holds after at most $\\mathcal{O}(\\frac{L}{\\mu} \\log(\\frac{1}{\\epsilon}))$ iterations. $\\log(\\frac{1}{\\epsilon}) < \\frac{1}{\\epsilon}$ for $\\epsilon$ small enough. $\\mathcal{O}(\\frac{L}{\\mu} \\log(\\frac{1}{\\epsilon}))$ is better than $\\mathcal{O}(\\frac{1}{\\epsilon})$ for convex functions.\n",
    "3. Acceleration:\n",
    "Is there an algorithm that has better guarantees than GD while doing the same amount of works per iteration? (i.e same number of function evaluations and gradient evaluations per iteration)\n",
    "**Recall:** GD iteration: $\\forall k \\geq 0, w_{k+1} = w_k - \\alpha_k \\nabla f(w_k)$ where $\\alpha_k = \\frac{1}{L}$.\n",
    "To define the step size $w_{k+1} = w_k - \\alpha_k \\nabla f(w_k)$, we use the information related to $w_k$ and ignore the information related to $w_{k-1}$, $w_{k-2}$, etc. $\\Longrightarrow$ We can do better by using more information."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c907f12e59ce5fdd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Polyak's Heavy Ball Method 1964\n",
    "Heavy Ball Method/Gradient Descend with Momentum\n",
    "$$\n",
    "\\forall k \\geq 0, w_{k+1} = w_k - \\alpha_k \\nabla f(w_k) + \\underbrace{\\beta_k (\\underbrace{w_k - w_{k-1}}_{\\text{previous step}})}_{\\text{momentum term}}\n",
    "$$\n",
    "with $\\beta_0 = 0$ and $\\beta_k \\geq 0$ for $k \\geq 1$.\n",
    "\n",
    "- This is optimal or strongly convex quadratic functions (better than Gb and cannot do better with only 1 gradient evaluation per iteration).\n",
    "Convergence rate $(1 - \\sqrt{\\frac{\\mu}{L}})^k \\leq \\underbrace{(1 - \\frac{\\mu}{L})^k}_{\\text{GD}}$.\n",
    "- But Heavy Ball Method can fail on strongly convex functions that are not quadratic."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3490bf71bca10440"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Nesterov's Accelerated Gradient Method 1983\n",
    "Accelerated Gradient/Nesterov's Method:\n",
    "$$\n",
    "\\forall k \\geq 0, w_{k+1} = w_k - \\alpha_k \\nabla f(w_k) + \\beta_k (w_k - w_{k-1}) + \\beta_k (w_k - w_{k-1})\n",
    "$$\n",
    "The difference with Heavy Ball Method is that:\n",
    "- First compute $w_k - \\beta_k (w_k - w_{k-1})$.\n",
    "- Then do a gradient step from $w_k - \\beta_k (w_k - w_{k-1})$.\n",
    "\n",
    "**Guarantees for Nesterov's Method**:\n",
    "1. If $f$ is $F^{1,1}_{L,\\mu}$, then for any $\\alpha_k = \\frac{1}{L}$ and $\\beta_k = \\frac{\\sqrt{L} - \\sqrt{\\mu}}{\\sqrt{L} + \\sqrt{\\mu}}$, we have: $${\\|w_k - w^*\\|}_{\\forall k \\geq 1} \\leq \\underbrace{(1 - \\frac{\\mu}{L})^k}_{\\text{convergence rate}} \\underbrace{\\|w_0 - w^*\\|}_{\\text{distance from } w_0 \\text{ to } w^*}$$ Complexity bound: $\\|w_k - w^*\\| < \\epsilon$ holds after at most $\\mathcal{O}(\\sqrt{\\frac{L}{\\mu}} \\log(\\frac{1}{\\epsilon}))$ iterations.\n",
    "2. If $f \\in F^{1,1}_{L,\\mu}$ and convex: Set $\\alpha_k = \\frac{1}{L}$ and $\\underbrace{\\beta_k = \\frac{t_{k-1}}{t_{k+1}}}_{\\text{Independent of problem}} where t_0 = 0$ and $t_1 = 1$ and $t_{k+1} = \\frac{1}{2} (1 + \\sqrt{1 + 4 t_k^2})$ for $k \\geq 1$."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e52650a6b6817ab7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T06:27:49.201191Z",
     "start_time": "2023-10-04T06:27:49.171869Z"
    }
   },
   "id": "72b215424a59590d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
